
@misc{_figueiredo_2021,
  title = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}: {{Collective Memories}} and {{Present}}-{{Day Intergroup Relations}}: {{Introduction}} to the {{Special Thematic Section}}},
  shorttitle = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}},
  year = {2021},
  month = jun,
  doi = {10.5964/jspp.v5i2.895},
  howpublished = {https://jspp.psychopen.eu/index.php/jspp/article/download/4995/4995.html?inline=1}
}

@misc{_impact_2021,
  title = {The Impact of a Multicultural Exchange between Indigenous and Non-Indigenous History Teachers for Students' Attitudes: Preliminary Evidence from a Pilot Study in {{Chile}}: {{Multicultural Education Review}}: {{Vol}} 12, {{No}} 3},
  year = {2021},
  month = jun,
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/2005615X.2020.1808927}
}

@misc{_retraction_,
  title = {Retraction {{Watch}}},
  journal = {Retraction Watch},
  abstract = {Tracking retractions as a window into the scientific process},
  howpublished = {https://retractionwatch.com/},
  language = {en-US}
}

@book{abrilruiz_manzanas_2019,
  title = {{Manzanas podridas: Malas pr\'acticas de investigaci\'on y ciencia descuidada}},
  shorttitle = {{Manzanas podridas}},
  author = {Abril Ruiz, Angel},
  year = {2019},
  isbn = {978-1-07-075536-6},
  language = {Spanish},
  annotation = {OCLC: 1120499121}
}

@article{aczel_consensusbased_2020,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and {de la Guardia}, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jan,
  journal = {Nat Hum Behav},
  volume = {4},
  number = {1},
  pages = {4--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  language = {en},
  keywords = {forrt,herramienta},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-134}{}}

\begin{itemize}

\item This manuscript provides a checklist of research transparency practices researchers can use to evaluate the transparency in their study, or to help improve transparency at each stage of the research process.

\item Transparency~is required to evaluate and reproduce findings, and also for research synthesis and meta analysis from the raw data.

\item There~is a~lack of transparency in the literature, but we should not assume an intention~to be deceptive or misleading. Rather, human reasoning is prone to biases (e.g. confirmation bias and motivated reasoning)~and few journals ask about statistical and methodological practices and transparency.

\item Journals can support open practices by offering badges, using the transparency and openness promotion guidelines, promote the availability of all research items, including data, materials and codes.

\item The consensus-based transparency checklist can be submitted with the manuscript to provide critical information about the process to evaluate the robustness of a finding.

\item The checklist can be modified by deleting, adding and rewording items with a high~level of acceptability and consensus with no strong counter argument for single items.

\item Researchers can explain the choices at the end of each 36 section. There is a shortened 12-item version to reduce demands on the researchers' time and facilitate broader adoption that fosters transparency and asks authors to complete a 36-item list.

\end{itemize}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-134}{}}

\par
\textbf{We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.}}
}

@article{allen_open_2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  language = {en},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data}
}

@article{allison_reproducibility_2018,
  title = {Reproducibility of Research: {{Issues}} and Proposed Remedies},
  shorttitle = {Reproducibility of Research},
  author = {Allison, David B. and Shiffrin, Richard M. and Stodden, Victoria},
  year = {2018},
  month = mar,
  journal = {PNAS},
  volume = {115},
  number = {11},
  pages = {2561--2562}
}

@article{an_crisis_2018,
  title = {The {{Crisis}} of {{Reproducibility}}, the {{Denominator Problem}} and the {{Scientific Role}} of {{Multi}}-Scale {{Modeling}}},
  author = {An, Gary},
  year = {2018},
  month = dec,
  journal = {Bull Math Biol},
  volume = {80},
  number = {12},
  pages = {3071--3080},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0497-0},
  abstract = {The ``Crisis of Reproducibility'' has received considerable attention both within the scientific community and without. While factors associated with scientific culture and practical practice are most often invoked, I propose that the Crisis of Reproducibility is ultimately a failure of generalization with a fundamental scientific basis in the methods used for biomedical research. The Denominator Problem describes how limitations intrinsic to the two primary approaches of biomedical research, clinical studies and preclinical experimental biology, lead to an inability to effectively characterize the full extent of biological heterogeneity, which compromises the task of generalizing acquired knowledge. Drawing on the example of the unifying role of theory in the physical sciences, I propose that multi-scale mathematical and dynamic computational models, when mapped to the modular structure of biological systems, can serve a unifying role as formal representations of what is conserved and similar from one biological context to another. This ability to explicitly describe the generation of heterogeneity from similarity addresses the Denominator Problem and provides a scientific response to the Crisis of Reproducibility.},
  language = {en},
  keywords = {crisis}
}

@article{andrea_why_2018,
  title = {Why Science's Crisis Should Not Become a Political Battling Ground},
  author = {Andrea, Saltelli},
  year = {2018},
  month = dec,
  journal = {Futures},
  volume = {104},
  pages = {85--90},
  issn = {0016-3287},
  doi = {10.1016/j.futures.2018.07.006},
  abstract = {A science war is in full swing which has taken science's reproducibility crisis as a battleground. While conservatives and corporate interests use the crisis to weaken regulations, their opponent deny the existence of a science's crisis altogether. Thus, for the conservative National Association of Scholars NAS the crisis is real and due to the progressive assault on higher education with ideologies such as ``neo-Marxism, radical feminism, historicism, post-colonialism, deconstructionism, post-modernism, liberation theology''. In the opposite field, some commentators claim that there is no crisis in science and that saying the opposite is irresponsible. These positions are to be seen in the context of the ongoing battle against regulation, of which the new rules proposed at the US Environmental Protection Agency (EPA) are but the last chapter. In this optic, Naomi Oreskes writes on Nature that what constitutes the crisis is the conservatives' attack on science. This evident right-left divide in the reading of the crisis is unhelpful and dangerous to the survival of science itself. An alternative reading ignored by the contendents would suggest that structural contradictions have emerged in modern science, and that addressing these should be the focus of our attention.},
  language = {en},
  keywords = {crisis,Evidence-based policy,History and philosophy of science,Post-normal science,Science and technology studies,Science’s crisis,Science’s reproducibility,Science’s war,Scientism}
}

@article{angell_publish_1986,
  title = {Publish or {{Perish}}: {{A Proposal}}},
  shorttitle = {Publish or {{Perish}}},
  author = {Angell, Marcia},
  year = {1986},
  month = feb,
  journal = {Ann Intern Med},
  volume = {104},
  number = {2},
  pages = {261--262},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-104-2-261},
  keywords = {institutional}
}

@misc{anid_consulta_2020,
  title = {Consulta {{P\'ublica}}: {{Pol\'itica Acceso Abierto}} a {{Informaci\'on Cient\'ifica}}},
  author = {ANID},
  year = {1 de Junio, 2020}
}

@misc{anid_propuesta_2020,
  title = {Propuesta de {{Pol\'itica}} de Acceso Abierto a La Informaci\'on Cient\'ifica y Adatos de Investigaci\'onfinanciados Con Fondos P\'ublicos de La {{ANID}}},
  author = {ANID},
  year = {26 de mayo, 2020},
  publisher = {{ANID}}
}

@article{anvari_replicability_2018,
  title = {The Replicability Crisis and Public Trust in Psychological Science},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2018},
  month = sep,
  journal = {Comprehensive Results in Social Psychology},
  volume = {3},
  number = {3},
  pages = {266--286},
  publisher = {{Routledge}},
  issn = {2374-3603},
  doi = {10.1080/23743603.2019.1684822},
  abstract = {Replication failures of past findings in several scientific disciplines, including psychology, medicine, and experimental economics, have created a ``crisis of confidence'' among scientists. Psychological science has been at the forefront of tackling these issues, with discussions about replication failures and scientific self-criticisms of questionable research practices (QRPs) increasingly taking place in public forums. How this replicability crisis impacts the public's trust is a question yet to be answered by research. Whereas some researchers believe that the public's trust will be positively impacted or maintained, others believe trust will be diminished. Because it is our field of expertise, we focus on trust in psychological science. We performed a study testing how public trust in past and future psychological research would be impacted by being informed about (i) replication failures (replications group), (ii) replication failures and criticisms of QRPs (QRPs group), and (iii) replication failures, criticisms of QRPs, and proposed reforms (reforms group). Results from a mostly European sample (N = 1129) showed that, compared to a control group, people in the replications, QRPs, and reforms groups self-reported less trust in past research. Regarding trust in future research, the replications and QRPs groups did not significantly differ from the control group. Surprisingly, the reforms group had less trust in future research than the control group. Nevertheless, people in the replications, QRPs, and reforms groups did not significantly differ from the control group in how much they believed future research in psychological science should be supported by public funding. Potential explanations are discussed.},
  keywords = {crisis of confidence,open science,Replicability crisis,reproducibility crisis,trust in science},
  annotation = {\_eprint: https://doi.org/10.1080/23743603.2019.1684822}
}

@article{armeni_widescale_2021,
  title = {Towards Wide-Scale Adoption of Open Science Practices: {{The}} Role of Open Science Communities},
  shorttitle = {Towards Wide-Scale Adoption of Open Science Practices},
  author = {Armeni, Kristijan and Brinkman, Loek and Carlsson, Rickard and Eerland, Anita and Fijten, Rianne and Fondberg, Robin and Heininga, Vera E and Heunis, Stephan and Koh, Wei Qi and Masselink, Maurits and Moran, Niall and Baoill, Andrew {\'O} and Sarafoglou, Alexandra and Schettino, Antonio and Schwamm, Hardy and Sjoerds, Zsuzsika and Teperek, Marta and {van den Akker}, Olmo R and {van't Veer}, Anna and {Zurita-Milla}, Raul},
  year = {2021},
  month = jul,
  journal = {Science and Public Policy},
  number = {scab039},
  issn = {0302-3427},
  doi = {10.1093/scipol/scab039},
  abstract = {Despite the increasing availability of Open Science (OS) infrastructure and the rise in policies to change behaviour, OS practices are not yet the norm. While pioneering researchers are developing OS practices, the majority sticks to status quo. To transition to common practice, we must engage a critical proportion of the academic community. In this transition, OS Communities (OSCs) play a key role. OSCs are bottom-up learning groups of scholars that discuss OS within and across disciplines. They make OS knowledge more accessible and facilitate communication among scholars and policymakers. Over the past two years, eleven OSCs were founded at several Dutch university cities. In other countries, similar OSCs are starting up. In this article, we discuss the pivotal role OSCs play in the large-scale transition to OS. We emphasize that, despite the grassroot character of OSCs, support from universities is critical for OSCs to be viable, effective, and sustainable.}
}

@article{baker_500_2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Nature Publishing Group},
  language = {en},
  keywords = {crisis}
}

@misc{bakker_ensuring_2018,
  title = {Ensuring the Quality and Specificity of Preregistrations},
  author = {Bakker, Marjan and Veldkamp, Coosje Lisabet Sterre and van Assen, Marcel A. L. M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David Thomas and Wicherts, Jelte},
  year = {2018},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdgyh},
  abstract = {Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of `researcher degrees of freedom' aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration (now called ``OSF Preregistration'', http://osf.io/prereg/). The Prereg Challenge format was a structured workflow with detailed instructions, and an independent review to confirm completeness; the ``Standard'' format was unstructured with minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that the structured format restricted the opportunistic use of researcher degrees of freedom better (Cliff's Delta = 0.49) than the unstructured format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
  keywords = {Meta-science,preregistration,Quantitative Methods,Questionable research practices,researcher degrees of freedom,Social and Behavioral Sciences,Statistical Methods}
}

@article{barba_terminologies_2018,
  title = {Terminologies for {{Reproducible Research}}},
  author = {Barba, Lorena A.},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.03311 [cs]},
  eprint = {1802.03311},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries}
}

@article{becerrilgarcia_end_2019,
  title = {The {{End}} of a {{Centralized Open Access Project}} and the {{Beginning}} of a {{Community}}-{{Based Sustainable Infrastructure}} for {{Latin America}}},
  author = {Becerril Garc{\'i}a, Arianna and Aguado L{\'o}pez, Eduardo},
  year = {2019},
  journal = {OpenEdition Press},
  pages = {41--55},
  doi = {10.4000/books.oep. 9003.},
  abstract = {The Latin American region has an ecosystem where the nature of publication is conceived as the act of making public, of sharing, not as the publishing industry. International, national and institutional contexts have led to redefine a project\textemdash Redalyc.org\textemdash that began in 2003 and that has already fulfilled its original mission: give visibility to knowledge coming from Latin America and promote qualitative scientific journals. Nevertheless, it has to be transformed from a Latin American platform based in Mexico into a community- based regional infrastructure that continues assessing journals' quality and providing access to full-text, thus allowing visibility for journals and free access to knowledge. It is a framework that generates technology in favor of the empowerment and professionalization of journal editors, making sustainable the editorial task in open access so that Redalyc may sustain itself collectively. This work describes Redalyc's first model, presents the problematic in process and the new business model Redalyc is designing and adopting to operate.}
}

@article{benjamin-chung_internal_2020,
  title = {Internal Replication of Computational Workflows in Scientific Research},
  author = {{Benjamin-Chung}, Jade and Colford, Jr., John M. and Mertens, Andrew and Hubbard, Alan E. and Arnold, Benjamin F.},
  year = {2020},
  month = jun,
  journal = {Gates Open Res},
  volume = {4},
  pages = {17},
  issn = {2572-4754},
  doi = {10.12688/gatesopenres.13108.2},
  abstract = {Failures to reproduce research findings across scientific disciplines from psychology to physics have garnered increasing attention in recent years. External replication of published findings by outside investigators has emerged as a method to detect errors and bias in the published literature. However, some studies influence policy and practice before external replication efforts can confirm or challenge the original contributions. Uncovering and resolving errors before publication would increase the efficiency of the scientific process by increasing the accuracy of published evidence. Here we summarize the rationale and best practices for internal replication, a process in which multiple independent data analysts replicate an analysis and correct errors prior to publication. We explain how internal replication should reduce errors and bias that arise during data analyses and argue that it will be most effective when coupled with pre-specified hypotheses and analysis plans and performed with data analysts masked to experimental group assignments. By improving the reproducibility of published evidence, internal replication should contribute to more rapid scientific advances.},
  language = {en}
}

@article{bergh_there_2017,
  title = {Is There a Credibility Crisis in Strategic Management Research? {{Evidence}} on the Reproducibility of Study Findings},
  shorttitle = {Is There a Credibility Crisis in Strategic Management Research?},
  author = {Bergh, Donald D and Sharp, Barton M and Aguinis, Herman and Li, Ming},
  year = {2017},
  month = aug,
  journal = {Strategic Organization},
  volume = {15},
  number = {3},
  pages = {423--436},
  publisher = {{SAGE Publications}},
  issn = {1476-1270},
  doi = {10.1177/1476127017701076},
  abstract = {Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70\% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.},
  language = {en},
  keywords = {crisis,knowledge credibility,replication,reproducibility}
}

@article{berlin_declaracion_2003,
  title = {La {{Declaraci\'on}} de {{Berl\'in}} Sobre Acceso Abierto},
  author = {Berl{\'i}n},
  year = {2003},
  series = {Sociedad {{Max Planck}}},
  volume = {1},
  number = {2},
  pages = {152--154}
}

@misc{bethesda_declaracion_2003,
  title = {Declaraci\'on de {{Bethesda}} Sobre Publicaci\'on de Acceso Abierto},
  author = {Bethesda},
  year = {2003}
}

@article{bishop_rein_2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, Dorothy},
  year = {2019},
  month = apr,
  journal = {Nature},
  volume = {568},
  number = {7753},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  language = {en},
  keywords = {forrt},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-123}{}}

\begin{itemize}

\item Publication bias, low statistical power, p-hacking, and HARKing (hypothesising after the results are known) are threats to research reproducibility that make it difficult to find meaningful results.

\item Publication bias harms patients. The tendency to not~publish negative results misleads readers and biases meta-analyses.

\item Low statistical power also misleads readers- when the sample size is small, there is a low probability that one will detect an effect even if it exists.

\item Time and resources are~wasted on such underpowered studies.

\item P-hacking occurs when researchers conduct many analyses, but report only those that are significant.

\item HARKing is so wide-spread, that researchers may come to accept it as a good practice. Authors should be free to do exploratory analyses, but not when p-values are used outside of the context that was used to calculate them.

\item These four problems are older than most of the junior researchers working on them. New developments may help combat these issues:

\end{itemize}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-123}{}}

\par
Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.}
}

@book{bjork_developing_2014,
  title = {Developing an {{Effective}}  {{Market}} for {{Open Access}}  {{Article Processing Charges}}},
  author = {Bj{\"o}rk, Bo-Christer and Solomon, David},
  year = {2014},
  publisher = {{Weolcome Trust}}
}

@misc{boai_diez_2012,
  title = {{Diez a\~nos desde la Budapest Open Access Initiative: hacia lo abierto por defecto}},
  author = {BOAI, Budapest Open Access Initiative},
  year = {2012},
  collaborator = {Melero, Remedios and Babini, Dominique},
  language = {Traducido}
}

@misc{boai_iniciativa_2002,
  title = {{Iniciativa de Budapest para el Acceso Abierto}},
  author = {BOAI, Budapest Open Access Initiative},
  year = {2002},
  language = {Traducido}
}

@article{bowers_how_2016,
  title = {How to Improve Your Relationship with Your Future Self},
  author = {Bowers, Jake and Voors, Maarten},
  year = {2016},
  month = dec,
  journal = {Revista de ciencia pol\'itica (Santiago)},
  volume = {36},
  number = {3},
  pages = {829--848},
  issn = {0718-090X},
  doi = {10.4067/S0718-090X2016000300011}
}

@article{breznau_does_2021,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  month = mar,
  journal = {Societies},
  volume = {11},
  number = {1},
  pages = {9},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/soc11010009},
  abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {crisis of science,Habermas,Merton,open science,p-hacking,publication bias,replication,research ethics,revisado,science community,sociology legitimation,transparency,Weber},
  note = {\textbf{Brief description/principal idea}
\par
Este art\'iculo analiza por qu\'e la sociolog\'ia tambi\'en deber\'ia cambiar en lo que respecta a transparencia y ciencia abierta. Revisa la sociolog\'ia como disciplina a trav\'es de la lente de las pr\'acticas actuales, las definiciones de la sociolog\'ia, las posiciones de las asociaciones sociol\'ogicas y una breve consideraci\'on de los argumentos de tres soci\'ologos altamente influyentes pero epistemol\'ogicamente diversos: Weber, Merton y Habermas. Es una descripci\'on general para que los estudiantes y soci\'ologos se familiaricen r\'apidamente con el estado de la sociolog\'ia o exploren la idea de ciencia abierta y su relevancia para su disciplina. (extracto abstract)
\par
\textbf{Cites}\textbf{}
\par
\#\# Important cite
\par
La transparencia en la ciencia abierta es una forma de que todos los investigadores revelen el proceso, las ideas y materiales en los que se basa un argumento o teor\'ia, y para apoyar una comunidad cient\'ifica m\'as \'etica, incluso si esto solo es posible antes de la realizaci\'on de un estudio\textbf{}
\par
\#\# Important cite
\par
Las ideas de ciencia abierta no son nuevas. Los acad\'emicos advirtieron hace mucho tiempo sobre los problemas con la competencia del estatus acad\'emico, los incentivos institucionales perversos, los problemas de intersubjetividad y la falta de transparencia.
\par
\#\# Cite
\par
Adem\'as, hay pruebas s\'olidas de que el p\'ublico y los responsables de la formulaci\'on de pol\'iticas tienen poca confianza y respeto por la ciencia, en particular las ciencias sociales. Por lo tanto, mi posici\'on es que la sociolog\'ia necesita ciencia abierta.
\par
\#\# Cite
\par
El investigador es el recipiente que convierte la observaci\'on en evidencia independientemente del origen de esa evidencia. El fraude intencional puede ser m\'as un caso at\'ipico, pero las pr\'acticas de investigaci\'on cuestionables realizadas sin el conocimiento de los investigadores de las consecuencias o la intenci\'on de causar alg\'un da\~no probablemente sean mucho m\'as frecuentes. Por ejemplo, un entrevistador cualitativo que selecciona a los entrevistados que probablemente apoyen una hip\'otesis determinada, o un investigador cuantitativo que ejecuta modelos hasta encontrar el efecto deseado.
\par
\#\# Cite
\par
Si los investigadores consideran cientos o miles de formas de analizar o interpretar sus datos, esencialmente est\'an probando "todas" las teor\'ias posibles. Cuando eligieron solo una versi\'on para informar de los miles, pero fingieron que ten\'ian una teor\'ia que predice este resultado \'unico todo el tiempo, parece que encontraron evidencia a favor de esa teor\'ia. La realidad es que han seleccionado una teor\'ia que se ajusta a ciertos hallazgos o han seleccionado ciertos hallazgos que se ajustan a una teor\'ia, ninguna de las cuales es evidencia a favor de ninguna teor\'ia sobre otra. En otras palabras, conduce a resultados poco fiables e in\'utiles que se promocionan como "ciencia".
\par
\#\# Cite
\par
La investigaci\'on de Lindsay, Boghossian y Pluckrose es un recordatorio de que el revisor y los sistemas de edici\'on de revistas son parte de las causas del problema de confiabilidad. No porque los revisores y editores no hagan un buen trabajo, sino porque es una carga demasiado pesada hacer que tres revisores que est\'en disponibles al azar y con limitaciones de tiempo sean responsables de garantizar que un art\'iculo sea confiable.
\par
\#\# Cite
\par
*En sociolog\'ia*
\par
El valor p se convirti\'o en el est\'andar de oro, y los resultados de los estudios ahora a menudo se juzgan completamente en funci\'on de si tienen un valor p por debajo de cierto umbral o no; el valor p aqu\'i es sin\'onimo de una prueba t y sus equivalentes; un valor t de 1,96 o un valor p de 0,05 tiene el mismo significado de mantenimiento de puerta. Es problem\'atico juzgar los estudios de esta manera, porque el valor p solo es \'util si el modelo de generaci\'on de datos es correcto.
\par
\#\# Cite
\par
En sociolog\'ia, un estudio anterior de Wilson, Smoke y Martin [32] encontr\'o que el 80\% de los estudios publicados en las tres principales revistas de sociolog\'ia de la \'epoca rechazaron la hip\'otesis nula, es decir, ten\'ian valores p por debajo de un umbral. Esto sugiere sesgo de publicaci\'on, si no p-hacking.
\par
\#\# Cite
\par
*Las ideas de ciencia abierta aparecen en las clausulas de \'etica de las asociaciones acad\'emicas*
\par
American Sociological Association 'Code of Ethics' 2019, 12.4.d. \textemdash{}
\par
De acuerdo con el esp\'iritu de divulgaci\'on completa de m\'etodos y an\'alisis, una vez que los hallazgos se difunden p\'ublicamente, los soci\'ologos permiten su evaluaci\'on y verificaci\'on abiertas por parte de otros investigadores responsables, con las salvaguardas adecuadas para proteger la confidencialidad de los participantes de la investigaci\'on.
\par
Sociedad Alemana de Sociolog\'ia `Ethik-Kodex' 2019 I.1.2\textemdash{}
\par
[Al presentar o publicar hallazgos sociol\'ogicos, deben describirse sin omitir resultados importantes; es decir, eso falsificar\'ia los hallazgos. Los detalles de las teor\'ias, m\'etodos y dise\~nos de investigaci\'on que son importantes para la evaluaci\'on de los resultados de la investigaci\'on y los l\'imites de su validez se dan seg\'un el mejor conocimiento de cada uno.]
\par
Sociedad Sociol\'ogica Japonesa, 'C\'odigo de \'Etica' 2019, Art\'iculo 9\textemdash{}
\par
[ Los miembros deben mantener actitudes y comportamientos abiertos para garantizar un lugar para la cr\'itica y la verificaci\'on mutuas.
\par
\#\# Cite
\par
Los investigadores a menudo se preguntan por qu\'e es tan importante compartir datos. La respuesta se relaciona con sesgo y pirater\'ia. Los investigadores que utilizan m\'etodos cualitativos no pueden simplemente piratear, sino que pueden sacar frases y palabras fuera de contexto y construir o interpretar cosas a su gusto. Un estudio de Riemann (2003) publicado en Forum: Qualitative Social Research (``Qualitative Sozialforschung'') pidi\'o a varios investigadores que analizaran cualitativamente los mismos datos de autobiograf\'ia narrativa. Los resultados fueron en diferentes direcciones con poco en com\'un (ver: http://www.qualitative-research.net/index.php/fqs/issue/view/17).}
}

@article{breznau_does_2021a,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  volume = {11}
}

@misc{breznau_observing_2021,
  title = {Observing {{Many Researchers Using}} the {{Same Data}} and {{Hypothesis Reveals}} a {{Hidden Universe}} of {{Uncertainty}}},
  author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Adem, Muna and Adriaans, Jule and {Alvarez-Benjumea}, Amalia and Andersen, Henrik Kenneth and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin and Castillo, Juan Carlos and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Am{\'e}lie and Gr{\"o}mping, Max and Gro{\ss}, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and H{\"o}vermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignacz, Zsofia and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Ko{\l}czy{\'n}ska, Marta and Kuk, John Seungmin and Kuni{\ss}en, Katharina and Sinatra, Dafina Kurti and Greinert, Alexander and Lersch, Philipp M. and L{\"o}bel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar Jose and McManus, Patricia and Wagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan J. B. and Moya, Crist{\'o}bal and Neunhoeffer, Marcel and N{\"u}st, Daniel and Nyg{\aa}rd, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Katja M. and Schmidt, Regine and {Schmidt-Catran}, Alexander and Schmiedeberg, Claudia and Schneider, J{\"u}rgen and Schoonvelde, Martijn and {Schulte-Cloos}, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, J{\"u}rgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian B. and Vagni, Giacomo and Assche, Jasper Van and van der Linden, Meta and van der Noll, Jolanda and Hootegem, Arno Van and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and {\.Z}{\'o}{\l}tak, Tomasz and Nguyen, Hung H. V.},
  year = {2021},
  month = mar,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/cd5j9},
  abstract = {How does noise generated by researcher decisions undermine the credibility of science? We test this by observing all decisions made among 73 research teams as they independently conduct studies on the same hypothesis with identical starting data. We find excessive variation of outcomes. When combined, the 107 observed research decisions taken across teams explained at most 2.6\% of the total variance in effect sizes and 10\% of the deviance in subjective conclusions. Expertise, prior beliefs and attitudes of the researchers explain even less. Each model deployed to test the hypothesis was unique, which highlights a vast universe of research design variability that is normally hidden from view and suggests humility when presenting and interpreting scientific findings.},
  keywords = {Analytical Flexibility,Crowdsourced Replication Initiative,Crowdsourcing,Economics,Garden of Forking Paths,Immigration,Many Analysts,Meta-Science,Noise,Other Social and Behavioral Sciences,Political Science,Psychology,Researcher Degrees of Freedom,Researcher Variability,Social and Behavioral Sciences,Social Policy,Sociology}
}

@misc{breznau_open_,
  type = {Billet},
  title = {Open Science in Sociology. {{What}}, Why and Now.},
  author = {Breznau, Nate},
  journal = {Crowdid},
  abstract = {WHAT By now you've heard the term ``open science''. Although it has no global definition, its advocates tend toward certain agreements. Most definitions focus on the practical aspects of accessibility. ``\ldots the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely \ldots{} Continue reading Open science in sociology. What, why and now.},
  language = {en-US}
}

@techreport{brodeur_methods_2018,
  type = {{{IZA Discussion Paper}}},
  title = {Methods {{Matter}}: {{P}}-{{Hacking}} and {{Causal Inference}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2018},
  month = aug,
  number = {11796},
  institution = {{Institute of Labor Economics (IZA)}},
  abstract = {The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25\% of claims of marginally significant results in IV papers are misleading.},
  keywords = {causal inference,p-curves,p-hacking,practices,publication bias,research methods}
}

@article{brodeur_star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = {2016},
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  language = {en},
  keywords = {Market for Economists; Estimation: General,Role of Economics,Role of Economists}
}

@misc{budapestopenaccessinitiative_diez_2012,
  title = {{Diez a\~nos desde la Budapest Open Access Initiative: hacia lo abierto por defecto}},
  author = {, Budapest Open Access Initiative},
  year = {12 de Septiembre, 2012},
  language = {Traducido}
}

@misc{budapestopenaccessinitiative_iniciativa_2002,
  title = {{Iniciativa de Budapest para el Aceso Abierto}},
  author = {, Budapest Open Access Initiative},
  year = {14 de Febrero, 2002},
  journal = {Budapest Open Access Initiative},
  language = {Traducido}
}

@article{burlig_improving_2018,
  title = {Improving Transparency in Observational Social Science Research: {{A}} Pre-Analysis Plan Approach},
  shorttitle = {Improving Transparency in Observational Social Science Research},
  author = {Burlig, Fiona},
  year = {2018},
  month = jul,
  journal = {Economics Letters},
  volume = {168},
  pages = {56--60},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2018.03.036},
  abstract = {Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I highlight three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data.},
  language = {en},
  keywords = {Confidential data,Observational research,Pre-registration,Transparency}
}

@misc{businessmanagementink_protecting_2016,
  title = {Protecting {{Students}}' {{Intellectual Property}}},
  author = {Business \& Management INK},
  year = {2016},
  journal = {social science space}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nat Rev Neurosci},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en},
  keywords = {practices}
}

@article{byington_solutions_2017,
  title = {Solutions to the {{Credibility Crisis}} in {{Management Science}}},
  author = {Byington, Eliza and Felps, Will},
  year = {2017},
  month = mar,
  journal = {Academy of Management Learning and Education, The},
  volume = {16},
  pages = {142--162},
  doi = {10.5465/amle.2015.0035},
  abstract = {This article argues much academic misconduct can be explained as the result of social dilemmas occurring at two levels of Management science. First, the career benefits associated with engaging in Noncredible Research Practices (NCRPs) (e.g. data manipulation, fabricating results, data hoarding, undisclosed HARKing) result in many academics choosing self-interest over collective welfare. These perverse incentives derive from journal gatekeepers who are pressed into a similar social dilemma. Namely, an individual journal's status (i.e. its ``impact factor'') is likely to suffer from unilaterally implementing practices that help ensure the credibility of Management science claims (e.g. dedicating journal space to strict replications, crowd-sourcing replications, data submission requirements, in-house analysis checks, registered reports, Open Practice badges). Fortunately, research on social dilemmas and collective action offers solutions. For example, journal editors could pledge to publish a certain number of credibility boosting articles contingent on a proportion of their ``peer'' journals doing the same. Details for successful implementation of conditional pledges, other social dilemma solutions \textendash{} including actions for Management academics who support changes in journal practices (e.g. reviewer boycotts / buycotts), and insights on credibility supportive journal practices from other fields are provided.},
  keywords = {crisis}
}

@article{caldwell_moving_2020,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  shorttitle = {Moving {{Sport}} and {{Exercise Science Forward}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, R{\'e}mi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary and Lohse, Keith R. and Nunan, David and {Consortium for Transparency in Exercise Science (COTES) Collaborators}},
  year = {2020},
  month = mar,
  journal = {Sports Med},
  volume = {50},
  number = {3},
  pages = {449--459},
  issn = {1179-2035},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR\$\$\textbackslash chi \$\$iv: https://osf.io/preprints/sportrxiv/fxe7a/.},
  language = {en},
  keywords = {transparency}
}

@article{camerer_evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nat Hum Behav},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  language = {en},
  keywords = {crisis}
}

@article{campbell_enhancing_2014,
  title = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings: {{A}} Guide for Relationship Researchers},
  shorttitle = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings},
  author = {Campbell, Lorne and Loving, Timothy J. and Lebel, Etienne P.},
  year = {2014},
  journal = {Personal Relationships},
  volume = {21},
  number = {4},
  pages = {531--545},
  issn = {1475-6811},
  doi = {10.1111/pere.12053},
  abstract = {The purpose of this paper is to extend to the field of relationship science, recent discussions and suggested changes in open research practises. We demonstrate different ways that greater transparency of the research process in our field will accelerate scientific progress by increasing accuracy of reported research findings. Importantly, we make concrete recommendations for how relationship researchers can transition to greater disclosure of research practices in a manner that is sensitive to the unique design features of methodologies employed by relationship scientists. We discuss how to implement these recommendations for four different research designs regularly used in relationship research and practical limitations regarding implementing our recommendations and provide potential solutions to these problems.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/pere.12053}
}

@article{card_role_2011,
  title = {The {{Role}} of {{Theory}} in {{Field Experiments}}},
  author = {Card, David and DellaVigna, Stefano and Malmendier, Ulrike},
  year = {2011},
  month = sep,
  journal = {Journal of Economic Perspectives},
  volume = {25},
  number = {3},
  pages = {39--62},
  issn = {0895-3309},
  doi = {10.1257/jep.25.3.39},
  abstract = {studies that estimate structural parameters in a completely specified model. We also classify laboratory experiments published in these journals over the same period and find that economic theory has played a more central role in the laboratory than in the field. Finally, we discuss in detail three sets of field experiments\textemdash on gift exchange, on charitable giving, and on negative income tax\textemdash that illustrate both the benefits and the potential costs of a tighter link between experimental design and theoretical underpinnings.},
  language = {en},
  keywords = {Field Experiments}
}

@article{carey_fraud_2011,
  title = {Fraud {{Case Seen}} as a {{Red Flag}} for {{Psychology Research}}},
  author = {Carey, Benedict},
  year = {2011},
  month = nov,
  journal = {The New York Times},
  issn = {0362-4331},
  abstract = {A Dutch scholar was found to have falsified findings in dozens of papers, in a field that critics say is vulnerable to such abuses.},
  chapter = {Health},
  language = {en-US},
  keywords = {Falsification of Data,Frauds and Swindling,Psychology and Psychologists,Research,Stapel; Diederik}
}

@article{carrier_facing_2017,
  title = {Facing the {{Credibility Crisis}} of {{Science}}: {{On}} the {{Ambivalent Role}} of {{Pluralism}} in {{Establishing Relevance}} and {{Reliability}}},
  shorttitle = {Facing the {{Credibility Crisis}} of {{Science}}},
  author = {Carrier, Martin},
  year = {2017},
  month = may,
  journal = {Perspectives on Science},
  volume = {25},
  number = {4},
  pages = {439--464},
  issn = {1063-6145},
  doi = {10.1162/POSC_a_00249},
  abstract = {Science at the interface with society is regarded with mistrust among parts of the public. Scientific judgments on matters of practical concern are not infrequently suspected of being incompetent and biased. I discuss two proposals for remedying this deficiency. The first aims at strengthening the independence of science and suggests increasing the distance to political and economic powers. The drawback is that this runs the risk of locking science in an academic ivory tower. The second proposal favors ``counter-politicization'' in that research is strongly focused on projects ``in the public interest,'' that is, on projects whose expected results will benefit all those concerned by these results. The disadvantage is that the future use of research findings cannot be delineated reliably in advance. I argue that the underlying problem is the perceived lack of relevance and reliability and that pluralism is an important step toward its solution. Pluralism serves to stimulate a more inclusive research agenda and strengthens the well-testedness of scientific approaches. However, pluralism also prevents the emergence of clear-cut practical suggestions. Accordingly, pluralism is part of the solution to the credibility crisis of science, but also part of the problem. In order for science to be suitable as a guide for practice, the leeway of scientific options needs to be narrowed \textendash{} in spite of uncertainty in epistemic respect. This reduction can be achieved by appeal to criteria that do not focus on the epistemic credentials of the suggestions but on their appropriateness in practical respect.},
  keywords = {crisis}
}

@article{chambers_registered_2013,
  title = {Registered {{Reports}}: {{A}} New Publishing Initiative at~{{Cortex}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2013},
  month = mar,
  journal = {Cortex},
  volume = {49},
  number = {3},
  pages = {609--610},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.12.016},
  language = {en},
  keywords = {forrt,reports},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-174}{}}

\begin{itemize}

\item We value novel and eye-catching findings over genuine findings, thus increasing questionable research practices.

\item Editorial decisions are one cause of questionable research practices, as they make decisions based on results.

\item Science undergraduates are taught about data analysis and hypothesis generation before the data is collected, ensuring the observer is independent of observation.

\item Cortex provides registered reports to allow null results and encourage replication.

\item Registered reports are manuscripts submitted before the experiment begins. This includes the introduction, hypotheses, procedures, analysis pipeline, power analysis, and pilot data, if possible.

\item Following peer review, the article is rejected or accepted in principle for publication, irrespective of the obtained results.

\item Authors have to submit a finalised manuscript for re-review, share raw data, and laboratory logs.

\item Pending quality checks and a sensible interpretation of findings, the manuscript is, in essence, accepted.

\item Registered reports are immune to publication bias and need authors to adhere to pre-approved methodology and analysis pipeline to prevent questionable research practices from being used.

\item A priori power analysis is required and the criteria for a registered report is seen as providing the highest truth value.

\item Registered reports do~not exclude exploratory analyses but must be distinguished from the planned analyses.

\item Not all modes of scientific investigation fit registered reports but most will.

\end{itemize}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-174}{}}

\par
\textbf{This is an editorial by Chris Chambers who encouraged Registered Reports in Cortex as a viable initiative to reduce questionable research practices, its benefits, limitations and what information to include in a registered report.}}
}

@misc{chambers_registered_2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  author = {Chambers, Christopher D.},
  year = {2014},
  journal = {Reviewers' Update},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  howpublished = {https://www.elsevier.com/connect/reviewers-update/registered-reports-a-step-change-in-scientific-publishing},
  language = {en},
  keywords = {forrt,reports},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-185}{}}

\begin{itemize}

\item Registered reports foster clarity and replication before the experiment~is conducted.

\item Study protocols are reviewed before the experiments are conducted.

\item Readers feel more confident that work is replicable with initial study predictions and analysis plans that were independently reviewed.

\item Registered reports are a departure from traditional peer review.

\item Low power, high rate of cherry picking, post-hoc hypothesising, lack of data sharing, journal culture marked by publication bias, and few replication studies, have contributed~to the reproducibility~crisis.

\item Allows us to publish positive, negative, or null findings, thus producing a true picture of the literature.

\item We will not suffer from publication bias, when a~manuscript is worthy of publication, editors and reviewers are driven by the quality of the methods, as opposed to results.

\item Registered reports are not an innovation but closer to restoration-reinvention of publication and peer review mechanisms.

\item Registered reports allow creativity, flexibility and reporting of unexpected findings.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote-106}{}}

\begin{quotation}
``Ultimately, it is up to all of us to determine the future of any reform, and if the community continues to support Registered Reports then that future looks promising. Each field that adopts this initiative will be helping to create a scientific literature that is free from publication bias, that celebrates transparency, that welcomes replication as well as novelty, and in which the reported science will be more reproducible.'' (p. 3)

\end{quotation}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-185}{}}

\par
\textbf{Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias.}}
}

@article{chambers_registered_2015,
  title = {Registered Reports: Realigning Incentives in Scientific Publishing},
  shorttitle = {Registered Reports},
  author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
  year = {2015},
  month = may,
  journal = {Cortex},
  volume = {66},
  pages = {A1-2},
  issn = {1973-8102},
  doi = {10.1016/j.cortex.2015.03.022},
  language = {eng},
  pmid = {25892410},
  keywords = {Biomedical Research,Editorial Policies,forrt,Humans,Motivation,Peer Review; Research,Publication Bias,Publishing,reports,Reproducibility of Results},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-163}{}}

\begin{itemize}

\item Registered Reports allows peer reviews to focus on the quality and rigour of the experimental design instead of ground-breaking results. This should reduce questionable research practices such as selective reporting, post-hoc hypothesising, and low statistical power.

\item Registered reports are reviewed and revised prior to data collection.

\item A cortex editorial sub-team triages submissions within one week: to reject manuscripts; to~invite for revision to meet the necessary standards; or to send out for Stage 1 in-depth review.

\item It takes approximately 8-10 weeks for a Stage 1 Registered Report to move from ``initial review''~to ``in-principle acceptance''. This also includes 1-3 rounds of peer reviews.

\item Once the study is completed, it takes 4 weeks for a paper to move from Stage 2 review to final editorial decision.

\item Registered reports are not~a one-shot cure for reproducibility problems in science and pose~no threat to exploratory analyses.

\end{itemize}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-163}{}}

\par
\textbf{This is a view on registered reports in Cortex by Professor Chris Chambers and colleagues. It contains information on Registered Reports and the length of duration for submission and review. They discuss the editorial process and that a registered report is not a threat to exploratory research and is not a panacea to cure reproducibility problems.}
\par
\textbf{FORRT Sum}}
}

@article{christensen_transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = {2018},
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  language = {en},
  keywords = {Market for Economists; Methodological Issues: General; Higher Education,Research Institutions,Role of Economics,Role of Economists}
}

@book{christensen_transparent_2019,
  title = {Transparent and Reproducible Social Science Research: How to Do Open Science},
  shorttitle = {Transparent and Reproducible Social Science Research},
  author = {Christensen, Garret S. and Freese, Jeremy and Miguel, Edward},
  year = {2019},
  publisher = {{University of California Press}},
  address = {{Oakland, California}},
  abstract = {"Social science practitioners have recently witnessed numerous episodes of influential research that fell apart upon close scrutiny. These instances have spurred suspicions that other published results may contain errors or may at least be less robust than they appear. In response, an influential movement has emerged across the social sciences for greater research transparency, openness, and reproducibility. Transparent and Reproducible Social Science Research crystallizes the new insights, practices, and methods of this rising interdisciplinary field"--Provided by publisher},
  isbn = {978-0-520-96923-0},
  lccn = {Q180.55.S7},
  keywords = {Reproducible research,Research,Social sciences,transparency},
  note = {Introduction -- What is ethical research? -- Publication bias -- Specification searching -- Using all evidence : registration and meta-analysis -- Pre-analysis plans -- Sensitivity analysis and other approaches -- Reporting standards -- Replication -- Data sharing -- Reproducible workflow -- Conclusion}
}

@article{chubin_open_1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  journal = {Science, Technology, \& Human Values},
  volume = {10},
  number = {2},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  language = {en}
}

@misc{conicyt_chile_2017,
  title = {Chile y {{Argentina}} Son Destacados Como Ejemplos de Pol\'iticas de Acceso Abierto a La Informaci\'on},
  author = {CONICYT},
  year = {9 de Enero, 2017}
}

@misc{creativecommons_sobre_2017,
  title = {Sobre Las Licencias: {{Lo}} Que Nuestras Licencias Hacen},
  author = {Creative Commons},
  year = {7 de Noviembre, 2017}
}

@misc{cruwell_easy_2018,
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2018},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  keywords = {Meta-science,Meta-Science,Open Access,Open Science,other,Psychology,Reproducibility,revisado,Social and Behavioral Sciences,Transparency},
  note = {\textbf{Brief description/principal idea:}
\par
El art\'iculo provee una revisi\'on de literatura sobre t\'opicos relacionados a la ciencia abierta, con tal de formar estudiantes e investigadores. Se revisan los principales art\'iculos que han contribuido a la promoci\'on de ideas sobre ciencia abierta y se divide en siete t\'opicos:
\par
1. Ciencia abierta
\par
Definen la ciencia abierta como un conjunto "de pr\'acticas de investigaci\'on que se manifiestan de manera diversa en diferentes contextos de investigaci\'on (Corker, 2018)".~ Estas pr\'acticas son transparentes y ayudan a mejorar la calidad de la ciencia.
\par
Otra def: ``el proceso de hacer que el contenido y el proceso de producci\'on de evidencia y afirmaciones sean transparentes y accesibles para otros'' (p. 5).
\par
2. Acceso abierto
\par
El acceso abierto (OA) se refiere a la disponibilidad p\'ublica sin restricciones de productos de investigaci\'on. Tiene como objetivo eliminar las barreras para acceder y distribuir la investigaci\'on y sus productos. (cita)
\par
3. Datos, materiales y c\'odigo abierto
\par
Los datos abiertos, los materiales y el c\'odigo ayudan a aumentar la credibilidad del proceso de investigaci\'on y aumentan la eficiencia del descubrimiento cient\'ifico y la verificabilidad. Dos argumentos:
\par
- la ciencia se basa en la verificabilidad, no en la confianza: uno quiere estar informado sobre cada detalle en un an\'alisis en lugar de depender \'unicamente de la palabra del autor,
\par
- la reproducibilidad anal\'itica solo se puede lograr cuando los datos est\'an disponibles abiertamente; la repetici\'on de an\'alisis para identificar errores es un ingrediente clave de un ciclo de investigaci\'on saludable. (esto es cita)
\par
4. An\'alisis reproducible
\par
El punto de partida es que los datos, materiales y c\'odigos sean abiertos para ser reproducibles, sin embargo, un an\'alisis reproducible implica una estructura de carpetas y de orden de archivos para ser efectivo. Tambi\'en, una gu\'ia detallada para que cada parte del an\'alisis pueda ser reproducida, ya sea en c\'odigo (ideal) o un punteo detallado de los pasos en SPSS.
\par
5. Preregistro
\par
La preinscripci\'on es una pr\'actica cient\'ifica abierta que protege a los investigadores de algunas de las influencias de incentivos desalineados, lo que les permite ser m\'as transparentes en su toma de decisiones anal\'iticas (cita). Los preregistos permiten separar el an\'alisis confirmatorio del an\'alisis exploratorio.
\par
6. Replicaci\'on
\par
Replicar una investigaci\'on consiste en efectuar el mismo plan de an\'alisos con distintos datos. Producir trabajo replicable y evaluar la replicabilidad de los hallazgos juega un papel central en cualquier dominio cient\'ifico cre\'ible y se alinea fundamentalmente con los valores de apertura en la ciencia (cite).
\par
7. Ense\~nar ciencia abierta
\par
Ense\~nar ciencia abierta y los t\'opicos relacionados a la crisis de reproducibilidad en pregrado permite transmitir un sentimiento de suspicacia con la producci\'on cientifica a los estudiantes.
\par
\textbf{Cites}\textbf{}\textbf{}
\par
\#\# This paper
\par
Nuestro objetivo fue crear una lista de lectura comentada que incluyera todos los temas com\'unmente considerados como pr\'acticas de ciencia abierta y que se han descrito en gu\'ias publicadas o art\'iculos metacient\'ificos. Para cada tema, proporcionamos un resumen accesible basado en un art\'iculo disponible p\'ublicamente, publicado y revisado por pares, y sugerimos lecturas adicionales. Al hacerlo, nuestro objetivo es hacer que las pr\'acticas de ciencia abierta sean tanto comprensibles como procesables para los lectores.
\par
\#\# Cite
\par
La mayor\'ia de los cient\'ificos est\'an de acuerdo en que existe una crisis de reproducibilidad, al menos hasta cierto punto (Baker, 2016). Sin embargo, no todos los cient\'ificos psicol\'ogicos han adoptado las mejores pr\'acticas recomendadas por los expertos para hacer que la ciencia sea m\'as reproducible (Ioannidis, Munaf\`o, Fusar-Poli, Nosek y David, 2014; O'Boyle, Banks y Gonzalez-Mul\'e, 2014).
\par
\#\# Important cite
\par
*\textasciidieresis Forma de organizar los conceptos*
\par
Por ejemplo, la transparencia y la accesibilidad de la investigaci\'on son esenciales para evaluar la credibilidad tanto de la evidencia estad\'istica como de las afirmaciones cient\'ificas. La credibilidad de la evidencia depende en parte de su reproducibilidad; dada la misma evidencia cuantitativa (es decir, datos) y el mismo an\'alisis estad\'istico, \textquestiondown se puede obtener el mismo resultado? La credibilidad de las afirmaciones cient\'ificas tambi\'en depende en parte de su replicabilidad; Si un experimento se repite con los mismos procedimientos, generando as\'i nuevos datos, \textquestiondown se obtendr\'a el mismo resultado (ver Plesser, 2018, para una discusi\'on de estas definiciones)? Ni la reproducibilidad ni la replicabilidad pueden evaluarse sin transparencia y accesibilidad
\par
\#\# Cite
\par
Debajo, destacamos dos instancias identificadas por Munaf\`o et al. d\'onde se est\'a avanzando:
\par
1) desarrollando y perfeccionando nuevas herramientas para proteger a los investigadores contra amenazas a la credibilidad, como el autoenga\~no, y
\par
2) creando nuevas estructuras de incentivos que recompensen a los investigadores por ser transparentes y realizar investigaciones y productos relacionados (por ejemplo, datos o c\'odigo) accesible.
\par
Estas y otras herramientas descritas en Munaf\`o et al. se les da un enfoque completo m\'as adelante en esta lista de lectura anotada.
\par
\#\# Important cite
\par
Argumentan que para que los resultados estad\'isticos sean confiables, la pr\'actica de presentar la investigaci\'on exploratoria como confirmatoria debe evitarse de manera integral. La soluci\'on radica en el registro previo: los investigadores se comprometen con las hip\'otesis, el dise\~no del estudio y los an\'alisis antes de que los datos sean accesibles.}
}

@article{dal-re_making_2014,
  title = {Making {{Prospective Registration}} of {{Observational Research}} a {{Reality}}},
  author = {{Dal-R{\'e}}, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
  year = {2014},
  month = feb,
  journal = {Science Translational Medicine},
  volume = {6},
  number = {224},
  pages = {224cm1-224cm1},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.3007513},
  abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data. Key information about human observational studies should be publicly available before the study is initiated. Key information about human observational studies should be publicly available before the study is initiated.},
  chapter = {Commentary},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  language = {en},
  pmid = {24553383},
  keywords = {reports}
}

@article{derond_publish_2005,
  title = {Publish or {{Perish}}: {{Bane}} or {{Boon}} of {{Academic Life}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {De Rond, Mark and Miller, Alan N.},
  year = {2005},
  month = dec,
  journal = {Journal of Management Inquiry},
  volume = {14},
  number = {4},
  pages = {321--329},
  publisher = {{SAGE Publications Inc}},
  issn = {1056-4926},
  doi = {10.1177/1056492605276850},
  abstract = {There are few more familiar aphorisms in the academic community than ``publish or perish.'' Venerated by many and dreaded by more, this phenomenon is the subject of the authors' essay. Here they consider the publish or perish principle that has come to characterize life at many business schools. They explain when and why it began and suggest reasons for its persistence. This exercise elicits questions that appear as relatively neglected but are integral to our profession, namely, the effect of publish or perish on the creativity, intellectual lives, morale, and psychological and emotional states of faculty.},
  language = {en},
  keywords = {business schools,institutional,publish,research,tenure}
}

@article{diaz_mala_2018,
  title = {{Mala conducta cient\'ifica en la publicaci\'on}},
  author = {D{\'i}az, Rosa Mar{\'i}a Lam},
  year = {2018},
  month = jan,
  journal = {Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  volume = {34},
  number = {1},
  issn = {1561-2996},
  abstract = {La publicaci\'on en revistas cient\'ificas constituye la forma m\'as aceptada para validar una investigaci\'on debido a que pasa por un riguroso proceso de revisi\'on por expertos, que deciden entre lo publicable y lo no publicable con vista a garantizar la calidad de los trabajos. A pesar de esto con frecuencia aparecen pr\'acticas incorrectas relacionadas con la \'etica durante la publicaci\'on, que se conocen como mala conducta cient\'ifica. Las manifestaciones de mala conducta cient\'ifica van desde el fraude cient\'ifico hasta una variedad de faltas que se cometen en el proceso de publicaci\'on. El fraude cient\'ifico incluye la invenci\'on, la falsificaci\'on y el plagio. Las faltas en el proceso de publicaci\'on incluyen la autor\'ia ficticia, la autor\'ia fantasma, la publicaci\'on duplicada, la publicaci\'on fragmentada o publicaci\'on salami, la publicaci\'on inflada, el autoplagio, la incorrecci\'on de citas bibliogr\'aficas, los sesgos de publicaci\'on y la publicaci\'on anticipada.},
  copyright = {Copyright (c) 2018 Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  language = {es},
  keywords = {autoría,conflicto de intereses,ética,fraude científico,investigación,plagio,practices,publicación}
}

@article{dutilh_seven_2016,
  title = {Seven {{Selfish Reasons}} for {{Preregistration}}},
  author = {Dutilh, Eric-Jan Wagenmakers {and} Gilles},
  year = {2016},
  month = oct,
  journal = {APS Observer},
  volume = {29},
  number = {9},
  abstract = {Psychological scientists Eric-Jan Wagenmakers and Gilles Dutilh present an illustrated guide to the career benefits of submitting your research plans before beginning your data collection.},
  language = {en-US}
}

@article{earth_reproducibility_2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {on Earth, Division and on Behavioral, Cognitive Board},
  year = {2019},
  journal = {undefined},
  abstract = {Semantic Scholar extracted view of \&quot;Reproducibility and Replicability in Science\&quot; by Division on Earth et al.},
  language = {en},
  note = {\textbf{Notas:}
\par
In the early 1990s, investigators began using the term ``reproducible research'' for studies that provided a complete digital compendium of data and code to reproduce their analysis, particularly in processing of seismic wave recordings (Claerbout and Karrenbach, 1992; Buckheit and Donoho, 1995). The emphasis was on ensuring that the computational analysis was transparent and documented so it could be verified by other researchers.
\par
This notion of ``reproducibility'' is quite different from situations in which a researcher gathers new data in the hopes of independently verifying previous results or a scientific inference; some scientific fields use the term ``reproducibility'' to refer to this practice.
\par
Peng et al. (2006, p. 783) referred to this scenario as ``replicability,'' saying: ``Scientific evidence is strengthened when important results are replicated by multiple independent investigators using independent data, analytical methods, laboratories, and instruments.'' Despite efforts to coalesce around the use of these terms, lack of consensus persists across disciplines. \textbf{The resulting confusion is an obstacle in moving forward to improve reproducibility and replicability} (Barba, 2018).}
}

@article{editors_observational_2014,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  journal = {PLOS Medicine},
  volume = {11},
  number = {8},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  language = {en},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy}
}

@article{editors_observational_2014a,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  journal = {PLOS Medicine},
  volume = {11},
  number = {8},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  language = {en},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy}
}

@article{editors_transparency_2015,
  title = {Transparency in {{Reporting Observational Studies}}: {{Reflections}} after a {{Year}}},
  shorttitle = {Transparency in {{Reporting Observational Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2015},
  month = oct,
  journal = {PLOS Medicine},
  volume = {12},
  number = {10},
  pages = {e1001896},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001896},
  abstract = {The PLOS Medicine Editors take stock of changes in the reporting of observational studies following our new transparency guidelines from August 2014.},
  language = {en},
  keywords = {Cohort studies,Diagnostic medicine,Health care policy,Observational studies,Open access medical journals,Peer review,Reflection,Water resources}
}

@article{elliott_taxonomy_2020,
  title = {A {{Taxonomy}} of {{Transparency}} in {{Science}}},
  author = {Elliott, Kevin C.},
  year = {2020},
  journal = {Canadian Journal of Philosophy},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2020.21},
  abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
  language = {en},
  keywords = {herramienta,open science,research ethics,science communication,transparency,value judgments,values and science}
}

@misc{engzell_improving_2020,
  title = {Improving {{Social Science}}: {{Lessons}} from the {{Open Science Movement}}},
  shorttitle = {Improving {{Social Science}}},
  author = {Engzell, Per and Rohrer, Julia M.},
  year = {2020},
  month = apr,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/6whjt},
  abstract = {The transdisciplinary movement towards greater research transparency opens the door for a meta-scientific exchange between different social sciences. In the spirit of such an exchange, we offer some lessons inspired by ongoing debates in psychology, highlighting the broad benefits of open science but also potential pitfalls, as well as practical challenges in the implementation that have not yet been fully resolved. Our discussion is aimed towards political scientists but relevant for population sciences more broadly.},
  keywords = {credibility,meta-science,open science,replication,reproducibility,Social and Behavioral Sciences,transparency}
}

@article{fanelli_how_2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta}}-{{Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  month = may,
  journal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc\ldots{} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86\textendash 4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once \textendash a serious form of misconduct by any standard\textendash{} and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91\textendash 19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  language = {en},
  keywords = {Deception,important,Medical journals,Medicine and health sciences,Metaanalysis,practices,Scientific misconduct,Scientists,Social research,Surveys}
}

@article{fanelli_opinion_2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  journal = {Proc Natl Acad Sci USA},
  volume = {115},
  number = {11},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  language = {en},
  keywords = {crisis,forrt},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-130}{}}

\begin{itemize}

\item Science is said to be in a crisis due to unreliable findings, poor research quality and integrity, low statistical power, and~questionable publication practices caused by~the~pressure to publish.

\item Fanelli questions this ``science in crisis'' narrative by critically examining the evidence for the existence of these problems.

\item Fraud and questionable research practices exist, but they are likely ~not common enough to seriously distort the scientific literature.

\item The pressure to publish has not been conclusively linked to scientific bias or misconduct.

\item Low power and replicability may differ~between subfields~and methodologies, and may be influenced by the~magnitude of the true effect size, and the~prior probability of the hypothesis being true.

\item There is little evidence to suggest that misconduct or questionable research practices have increased in recent years.

\item The ``science in crisis'' narrative is not well supported by the evidence and may be counterproductive, as it encourages values that can be used to discredit science. A narrative of ``new opportunities'' or ``revolution'' may be more empowering to scientists.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote-74}{}}

\begin{quotation}
``Science always was and always will be a struggle to produce knowledge for the benefit of all of humanity against the cognitive and moral limitations of individual human beings, including the limitations of scientists themselves.''~(p.2630)

\end{quotation}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-130}{}}

\par
Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.}
}

@misc{fernandez_derechos_2009,
  title = {Derechos de {{Autor}}},
  author = {Fern{\'a}ndez, Juan Carlos},
  year = {18 de Septiembre, 2009},
  journal = {Derechos de Autor en plataformas e-learning}
}

@inproceedings{fernandez_derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  booktitle = {{{VIII Conferencia Biredial}}-{{ISTEC}}},
  author = {Fern{\'a}ndez, Juan Carlos and Graziosi, Eduardo and Mart{\'i}nez, Daniel},
  year = {2018},
  address = {{Lima - Per\'u}}
}

@inproceedings{fernandezmolina_derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  shorttitle = {Derechos de Autor y Ciencia Abierta},
  booktitle = {{{VIII Conferencia Internacional}} Sobre {{Bibliotecas}} y {{Repositorios Digitales BIREDIAL}}-{{ISTEC}} ({{Lima}}, 2018)},
  author = {Fern{\'a}ndez Molina, Juan Carlos and Graziosi Silva, Eduardo and Mart{\'i}nez {\'A}vila, Daniel},
  year = {2018}
}

@incollection{fidler_reproducibility_2021,
  title = {Reproducibility of {{Scientific Results}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.}
}

@incollection{figueiredo_data_2020,
  title = {Data {{Collection With Indigenous People}}: {{Fieldwork Experiences From Chile}}},
  shorttitle = {Data {{Collection With Indigenous People}}},
  booktitle = {Researching {{Peace}}, {{Conflict}}, and {{Power}} in the {{Field}}: {{Methodological Challenges}} and {{Opportunities}}},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro},
  editor = {Acar, Yasemin G{\"u}ls{\"u}m and Moss, Sigrun Marie and Ulu{\u g}, {\"O}zden Melis},
  year = {2020},
  series = {Peace {{Psychology Book Series}}},
  pages = {105--127},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-44113-5_7},
  abstract = {At present, the Mapuche are the largest indigenous group living in Chile and, up until the present day, they are considered a disadvantaged group in Chilean society in terms of poverty, education and discrimination indicators. In recent decades, this group has been involved in a violent conflict with the Chilean state, forestry and hydroelectric industries and big landowners due mainly to territorial claims of the ancestral land that is currently inhabited and exploited by these different actors. In the present chapter, we narrate the process of data collection with indigenous participants within the framework of a three-year long project about representations of history and present-day intergroup relations between the Mapuche and the non-indigenous majority in Chile. We focus on the challenges that data collection involved by highlighting the process of participant recruitment and trust issues revolving around data collection, as well as retribution practices. Moreover, we also highlight the pros and cons of having non-indigenous Chilean and international researchers conducting fieldwork in this context. Another aspect we address is how methodological approaches may influence the data quality and participants' degree of involvement with the project, by highlighting how these issues interconnect with cultural differences and this indigenous group's worldview and cultural practices. We hope this chapter may provide significant insights on how to deal with some of the difficulties that data collection with indigenous people may involve.},
  isbn = {978-3-030-44113-5},
  language = {en},
  keywords = {Chile,Fieldwork,Mapuche,Qualitative research,Quantitative research}
}

@article{figueiredo_groupbased_2015,
  title = {Group-Based {{Compunction}} and {{Anger}}: {{Their Antecedents}} and {{Consequences}} in {{Relation}} to {{Colonial Conflicts}}},
  shorttitle = {Group-Based {{Compunction}} and {{Anger}}},
  author = {Figueiredo, Ana and Doosje, Bertjan and Valentim, Joaquim Pires},
  year = {2015},
  journal = {IJCV},
  volume = {9},
  pages = {90--105},
  issn = {1864-1385},
  doi = {10.4119/ijcv-3070},
  copyright = {Copyright (c) 2016 International Journal of Conflict and Violence},
  language = {en}
}

@incollection{figueiredo_representations_2019,
  title = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People: {{The Mapuche}} in {{Chile}}},
  shorttitle = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro and Ferreiro, Trinidad and Guerrero, Catarina and Varela O'Reilly, Micaela and Garc{\'i}a, Bernardita and Mu{\~n}oz, Loreto and Schmidt, Magdalena and Cornejo, Marcela and Licata, Laurent},
  year = {2019},
  month = jan,
  pages = {79--104},
  isbn = {978-1-5361-6164-9}
}

@article{figueiredo_too_2015,
  title = {Too Long to Compensate? {{Time}} Perceptions, Emotions, and Compensation for Colonial Conflicts},
  shorttitle = {Too Long to Compensate?},
  author = {Figueiredo, Ana Mateus and Valentim, Joaquim Pires and Doosje, Bertjan},
  year = {2015},
  journal = {Peace and Conflict: Journal of Peace Psychology},
  volume = {21},
  number = {3},
  pages = {500--504},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {1532-7949(ELECTRONIC),1078-1919(PRINT)},
  doi = {10.1037/pac0000114},
  abstract = {In the present article we analyze the role of perceptions of time and ingroup-focused compunction and anger on the desire to compensate the outgroup in relation to historical colonial conflicts. Furthermore, we analyze the relationships between the aforementioned variables and perceptions of the past as being violent and perceptions that compensation has been enough. By means of multiple group structural equation modeling using 1 Portuguese sample (N = 170) and 1 Dutch sample (N = 238), we were able to show that perceptions of the time passed between the negative events and the present day are negatively related to compensatory behavioral intentions. Furthermore, the belief that past compensation has been enough is negatively related to ingroup-focused anger and compunction. Anger (Portuguese sample only) and compunction are positively associated with intentions of compensation. The implications of our results for the field of intergroup relations are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Conflict,Emotions,History,Ingroup Outgroup,Time Perception}
}

@article{flier_faculty_2017,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  journal = {Nature},
  volume = {549},
  number = {7671},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  language = {en},
  keywords = {forrt},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-5}{}}

\begin{itemize}

\item Inadequate training, increased competition, problems in peer review and publishing, and occasionally scientific misconduct are some of the variables behind irreproducible research in the biomedical field.

\item Diverse causes make finding solutions for the problem of irreproducibility ~difficult, especially, as they must be implemented by independent constituencies including funders and publishers.

\item Academic institutions can and must do better to make science more reliable. One of the most effective (but least discussed) measures is to change how we appoint and promote our faculty members.

\item Promotion criteria has changed over time. Committees now consider how well a candidate participates in team science, but we still depend on imperfect metrics for judging research publications and our ability to assess reliability and accuracy is underdeveloped.

\item Reproducibility and robustness are under-emphasised when job applicants are evaluated and when faculty members are promoted.

\item Currently, reviewers of committees are asked to assess how a field would be different without a candidate's contributions, and to survey a candidate's accomplishment, scholarship, and recognition.

\item The promotion process should also encourage evaluators to say whether they feel candidates' work is problematic or over-stated and whether it has been reproduced and broadly accepted. If not, they should say whether they believe widespread reproducibility is likely or whether work will advance the field.

\item Applicants should also be asked to critically evaluate their research, including unanswered questions, controversies and uncertainties. This signals the importance of assessment and creates a mechanism to judge a candidate's capacity for critical self-reflection.

\item Evaluators should be asked to consider how technical and statistical issues were handled by candidates. Research and discovery is not simple and unidirectional, and evaluators should be sceptical of candidates who oversimplify.

\item Institutions need to incentivise data sharing and transparency. Efforts are more urgent as increasingly interdisciplinary projects extend beyond individual investigators' expertise.

\item Success will need creativity, pragmatism and diplomacy, because investigators bristle at any perceived imposition on their academic freedom.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote-2}{}}

\begin{quotation}
``Over time, efforts to increase the ratio of self-reflection to self-promotion may be the best way to improve science. It will be a slog, but if we don't take this on, formally and explicitly, nothing will change.'' (p.133)

\end{quotation}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-5}{}}

\par
\textbf{Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.}}
}

@article{flier_faculty_2017a,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  journal = {Nature},
  volume = {549},
  number = {7671},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Careers;Lab life;Research data;Research management Subject\_term\_id: careers;lab-life;research-data;research-management}
}

@misc{fortney_social_01deDiciembre20015,
  title = {A Social Networking Site Is Not an Open Access Repository},
  author = {Fortney, Katie and Gonder, Justin},
  year = {1 de Diciembre, 20015},
  journal = {Office of Scholary Communication University of California}
}

@article{franca_reproducibility_2019,
  title = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies: {{Untangling}} the Knot},
  shorttitle = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies},
  author = {Fran{\c c}a, Thiago F. A. and Monserrat, Jos{\'e} Maria},
  year = {2019},
  month = oct,
  journal = {Learned Publishing},
  volume = {32},
  number = {4},
  pages = {406--408},
  issn = {0953-1513, 1741-4857},
  doi = {10.1002/leap.1250},
  language = {en},
  keywords = {crisis}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  month = sep,
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  language = {en},
  keywords = {practices}
}

@article{freese_replication_2017,
  title = {Replication in {{Social Science}}},
  author = {Freese, Jeremy and Peterson, David},
  year = {2017},
  month = jul,
  journal = {Annu. Rev. Sociol.},
  volume = {43},
  number = {1},
  pages = {147--165},
  publisher = {{Annual Reviews}},
  issn = {0360-0572},
  doi = {10.1146/annurev-soc-060116-053450},
  abstract = {Across the medical and social sciences, new discussions about replication have led to transformations in research practice. Sociologists, however, have been largely absent from these discussions. The goals of this review are to introduce sociologists to these developments, synthesize insights from science studies about replication in general, and detail the specific issues regarding replication that occur in sociology. The first half of the article argues that a sociologically sophisticated understanding of replication must address both the ways that replication rules and conventions evolved within an epistemic culture and how those cultures are shaped by specific research challenges. The second half outlines the four main dimensions of replicability in quantitative sociology\textemdash verifiability, robustness, repeatability, and generalizability\textemdash and discusses the specific ambiguities of interpretation that can arise in each. We conclude by advocating some commonsense changes to promote replication while acknowledging the epistemic diversity of our field.}
}

@article{frey_publishing_2003,
  title = {Publishing as {{Prostitution}}? \textendash{} {{Choosing Between One}}'s {{Own Ideas}} and {{Academic Success}}},
  shorttitle = {Publishing as {{Prostitution}}?},
  author = {Frey, Bruno S.},
  year = {2003},
  month = jul,
  journal = {Public Choice},
  volume = {116},
  number = {1},
  pages = {205--223},
  issn = {1573-7101},
  doi = {10.1023/A:1024208701874},
  abstract = {Survival in academia depends on publications in refereedjournals. Authors only get their papers accepted if theyintellectually prostitute themselves by slavishly followingthe demands made by anonymous referees who have no propertyrights to the journals they advise. Intellectual prostitutionis neither beneficial to suppliers nor consumers. But it isavoidable. The editor (with property rights to the journal)should make the basic decision of whether a paper is worthpublishing or not. The referees should only offer suggestionsfor improvement. The author may disregard this advice. Thisreduces intellectual prostitution and produces more originalpublications.},
  language = {en},
  keywords = {institutional}
}

@article{gall_credibility_2017,
  title = {The Credibility Crisis in Research: {{Can}} Economics Tools Help?},
  shorttitle = {The Credibility Crisis in Research},
  author = {Gall, Thomas and Ioannidis, John P. A. and Maniadis, Zacharias},
  year = {2017},
  month = apr,
  journal = {PLOS Biology},
  volume = {15},
  number = {4},
  pages = {e2001846},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2001846},
  abstract = {The issue of nonreplicable evidence has attracted considerable attention across biomedical and other sciences. This concern is accompanied by an increasing interest in reforming research incentives and practices. How to optimally perform these reforms is a scientific problem in itself, and economics has several scientific methods that can help evaluate research reforms. Here, we review these methods and show their potential. Prominent among them are mathematical modeling and laboratory experiments that constitute affordable ways to approximate the effects of policies with wide-ranging implications.},
  language = {en},
  keywords = {crisis,Economic models,Economics,Experimental economics,Game theory,Health economics,Labor economics,Mathematical modeling,Randomized controlled trials}
}

@article{gerber_publication_2008,
  title = {Publication {{Bias}} in {{Empirical Sociological Research}}: {{Do Arbitrary Significance Levels Distort Published Results}}?},
  shorttitle = {Publication {{Bias}} in {{Empirical Sociological Research}}},
  author = {Gerber, Alan S. and Malhotra, Neil},
  year = {2008},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {37},
  number = {1},
  pages = {3--30},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124108318973},
  abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
  language = {en},
  keywords = {caliper test,hypothesis testing,meta-analysis,practices,publication bias}
}

@article{gerber_statistical_2008,
  title = {Do {{Statistical Reporting Standards Affect What Is Published}}? {{Publication Bias}} in {{Two Leading Political Science Journals}}},
  shorttitle = {Do {{Statistical Reporting Standards Affect What Is Published}}?},
  author = {Gerber, Alan and Malhotra, Neil},
  year = {2008},
  month = oct,
  journal = {QJPS},
  volume = {3},
  number = {3},
  pages = {313--326},
  publisher = {{Now Publishers, Inc.}},
  issn = {1554-0626, 1554-0634},
  doi = {10.1561/100.00008024},
  abstract = {Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals},
  language = {English},
  keywords = {practices}
}

@article{gilbert_comment_2016,
  title = {Comment on "{{Estimating}} the Reproducibility of Psychological Science"},
  author = {Gilbert, D. T. and King, G. and Pettigrew, S. and Wilson, T. D.},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aad7243},
  language = {en}
}

@article{goodman_what_2016,
  title = {What Does Research Reproducibility Mean?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  journal = {Science Translational Medicine},
  volume = {8},
  number = {341},
  pages = {341ps12-341ps12},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {{$<$}p{$>$}The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''{$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  language = {en},
  pmid = {27252173}
}

@article{guttinger_limits_2020,
  title = {The Limits of Replicability},
  author = {Guttinger, Stephan},
  year = {2020},
  month = jan,
  journal = {Euro Jnl Phil Sci},
  volume = {10},
  number = {2},
  pages = {10},
  issn = {1879-4920},
  doi = {10.1007/s13194-019-0269-1},
  abstract = {Discussions about a replicability crisis in science have been driven by the normative claim that all of science should be replicable and the empirical claim that most of it isn't. Recently, such crisis talk has been challenged by a new localism, which argues a) that serious problems with replicability are not a general occurrence in science and b) that replicability itself should not be treated as a universal standard. The goal of this article is to introduce this emerging strand of the debate and to discuss some of its implications and limitations. I will in particular highlight the issue of demarcation that localist accounts have to address, i.e. the question of how we can distinguish replicable science from disciplines where replicability does not apply.},
  language = {en}
}

@article{haven_preregistering_2019,
  title = {Preregistering Qualitative Research},
  author = {Haven, Tamarinde L. and Grootel, Dr Leonie Van},
  year = {2019},
  month = apr,
  journal = {Accountability in Research},
  volume = {26},
  number = {3},
  pages = {229--244},
  publisher = {{Taylor \& Francis}},
  issn = {0898-9621},
  doi = {10.1080/08989621.2019.1580147},
  abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
  pmid = {30741570},
  keywords = {Preregistration,qualitative research,reports,transparency},
  annotation = {\_eprint: https://doi.org/10.1080/08989621.2019.1580147}
}

@article{head_extent_2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  journal = {PLOS Biology},
  volume = {13},
  number = {3},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  language = {en},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,practices,Publication ethics,Reproducibility,Statistical data,Test statistics}
}

@article{hernandez_open_2007,
  title = {{{OPEN ACCESS}}: {{EL PAPEL DE LAS BIBLIOTECAS EN LOS REPOSITORIOS INSTITUCIONALES DE ACCESO ABIERTO}}},
  author = {Hern{\'a}ndez, Tony and Rodr{\'i}guez, David and {Bueno De la Fuente}, Gema},
  year = {2007},
  volume = {10},
  pages = {185--204},
  abstract = {En Espa\~na, como en muchos otros pa\'ises, el n\'umero de repositorios insti-tucionales  ha  ido  creciendo  paulatinamente  en  los  \'ultimos  tres  a\~nos.  En  febrero  de  2007 estos repositorios contienen ya m\'as de 30000 documentos en acceso abierto, es decir, disponibles a texto completo de forma gratuita, y con posibilidad de descarga, impresi\'on o copia sin coste a\~nadido. La pr\'actica totalidad de estos repositorios est\'an siendo  gestionados  por  los  servicios  de  biblioteca  de  las  distintas  instituciones  que  los albergan. Este art\'iculo explica las razones de la crisis del modelo tradicional de comunicaci\'on cient\'ifica, iniciado en la era de lo impreso y que se ha trasladado a la era digital, la alternativa que representa el modelo basado en el acceso abierto, y el importante papel que las bibliotecas pueden jugar, un reto y una oportunidad que no deben perder, en la construcci\'on de colecciones digitales propias}
}

@article{hernandez_por_2016,
  title = {\textquestiondown{{Por}} Qu\'e Open Access?},
  author = {Hern{\'a}ndez, Enrique},
  year = {2016},
  volume = {28},
  number = {1},
  issn = {2448-8771}
}

@article{hollenbeck_harking_2017,
  title = {Harking, {{Sharking}}, and {{Tharking}}: {{Making}} the {{Case}} for {{Post Hoc Analysis}} of {{Scientific Data}}},
  shorttitle = {Harking, {{Sharking}}, and {{Tharking}}},
  author = {Hollenbeck, John R. and Wright, Patrick M.},
  year = {2017},
  month = jan,
  journal = {Journal of Management},
  volume = {43},
  number = {1},
  pages = {5--18},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206316679487},
  abstract = {In this editorial we discuss the problems associated with HARKing (Hypothesizing After Results Are Known) and draw a distinction between Sharking (Secretly HARKing in the Introduction section) and Tharking (Transparently HARKing in the Discussion section). Although there is never any justification for the process of Sharking, we argue that Tharking can promote the effectiveness and efficiency of both scientific inquiry and cumulative knowledge creation. We argue that the discussion sections of all empirical papers should include a subsection that reports post hoc exploratory data analysis. We explain how authors, reviewers, and editors can best leverage post hoc analyses in the spirit of scientific discovery in a way that does not bias parameter estimates and recognizes the lack of definitiveness associated with any single study or any single replication. We also discuss why the failure to Thark in high-stakes contexts where data is scarce and costly may also be unethical.},
  language = {en},
  keywords = {macro topics,micro topics,philosophy of science,practices,research design,research methods,statistical methods}
}

@misc{horgan_psychology_,
  title = {Psychology's {{Credibility Crisis}}: The {{Bad}}, the {{Good}} and the {{Ugly}}},
  shorttitle = {Psychology's {{Credibility Crisis}}},
  author = {Horgan, John},
  journal = {Scientific American},
  doi = {10.1038/scientificamericanmind0716-18},
  abstract = {As more studies are called into question and researchers bicker over methodology, the field is showing a healthy willingness to face its problems\&nbsp;},
  howpublished = {https://www.scientificamerican.com/article/psychology-s-credibility-crisis-the-bad-the-good-and-the-ugly/},
  language = {en},
  keywords = {crisis}
}

@article{ioannidis_power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  issn = {1468-0297},
  doi = {10.1111/ecoj.12461},
  abstract = {We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical economics literatures that draw upon 64,076 estimates of economic parameters reported in more than 6,700 empirical studies. Half of the research areas have nearly 90\% of their results under-powered. The median statistical power is 18\%, or less. A simple weighted average of those reported results that are adequately powered (power {$\geq$} 80\%) reveals that nearly 80\% of the reported effects in these empirical economics literatures are exaggerated; typically, by a factor of two and with one-third inflated by a factor of four or more.},
  copyright = {\textcopyright{} 2017 Royal Economic Society},
  language = {en},
  keywords = {bias,credibility,empirical economics,practices,publication bias,statistical power},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecoj.12461}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  keywords = {Cancer risk factors,crisis,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,practices,Randomized controlled trials,Research design,Schizophrenia},
  note = {\textbf{BITSS Sum}
\par
\section{Why most published research findings are false}

\par
\textbf{In 2005, John Ioannidis, well known for his research on the validity of studies in the health and medical sciences, wrote an essay titled ``Why Most Published Research Findings are False.'' The blunt title and Ioannidis's provocative and compelling arguments have made this paper one of the foundational pieces of literature in the areas of meta-science and research transparency. You'd be hard-pressed to find an article on these topics \textendash{} published in a journal or the popular media \textendash{} that doesn't mention it.}
\par
In this video, I introduce you to the different types of errors that can occur in research, their probabilities, and the concept of statistical power. We will also learn about Positive Predictive Value, or the believability of a study's findings, as well as how biases can impact results. The last part of the video lays out six corollaries that characterize scientific research and what scientists can do to improve the validity of research. We go into more depth about these corollaries below.
\par
In the article, Ioannidis lays out a framework for demonstrating:
\par
\begin{itemize}

\item the probability that research findings are false,

\item the proportion of findings in a given research field that are valid,

\item how different biases affect the outcomes of research, and

\item what can be done to reduce error and bias.

\end{itemize}

\par
Ioannidis first defines~\emph{bias}~as ``the combination of various design, data, analysis, and presentation factors that tend to produce research findings when they should not be produced.'' He goes on to say that ``bias can entail manipulation in the analysis or reporting of findings. Selective or distorted reporting is a typical form of such bias''.
\par
With increasing bias, the chances that findings are true decreases. And reverse bias \textendash{} the rejection of true relationships due to measurement error, inefficient use of data, and failure to recognize statistically significant relationships \textendash{} becomes less likely as technology advances.
\par
Another important point Ioannidis makes is that, while multiple research teams often study the same or similar research questions, it is the norm that the scientific community as a whole tends to focus on an individual discovery, rather than on broader evidence.
\par
He goes on to list corollaries about the probability that a research finding is indeed true:
\par
Corollary 1:~\textbf{``The smaller the studies conducted in a scientific field, the less likely the research findings are to be true.''}~He refers here to sample size. Research findings are more likely to be true with larger studies such as randomized controlled trials.
\par
Corollary 2:~\textbf{``The smaller the effect sizes in a scientific field, the less likely the research findings are to be true.''}~Also remember that effect size is related to power. An example of a large effect that is useful and likely true is the impact of smoking on cancer or cardiovascular disease. This is more reliable than small postulated effects like genetic risk factors on disease. Very small effect sizes can be indicative of false positive claims.
\par
Corollary 3:~\textbf{``The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true.''}~If the pre-study probability that a finding is true influences the post-study probability that is true, it follows that findings are more likely to be true in confirmatory research than in exploratory research.
\par
Corollary 4:~\textbf{``The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.''}~``Flexibility'', Ioannidis tells us, ``increases the potential for transforming what would be `negative' results into `positive' results''. To combat this, efforts have been made to standardize research conduct and reporting with the belief that adherence to such standards will increase true findings. True findings may also be more common when the outcomes are universally agreed upon, whereas experimental analytical methods may be subject to bias and selective outcome reporting.
\par
Corollary 5:~\textbf{``The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true.''}~Conflicts of interest may be inadequately reported and may increase bias. Prejudice may also arise due to a scientist's belief or commitment to a theory or their own work. Additionally, some research is conducted out of self-interest to give researchers qualifications for promotion or tenure. These can all distort results.
\par
Corollary 6:~\textbf{``The hotter a scientific field (with more researchers and teams involved), the less likely the research findings are to be true.''}When there are many players involved, getting ahead of the competition may become the priority, which can lead to rushed experiments or a focus on obtaining flashy and positive results that are more publishable than negative ones. Additionally, when teams focus on publishing ``positive'' results, others may want to respond by finding ``negative'' results to disprove them. What results then, is something called the~\emph{Proteus phenomenon}, which describes rapidly alternating extreme research claims and opposite refutations.
\par
Using his framework for determining Positive Predictive Value and the corresponding corollaries, Ioannidis concludes that ``most research findings are false for most research designs and for most fields.''
\par
While the wide extent of biased and false research findings may seem a harsh reality, the situation can be improved in a few ways. First, higher powered and larger studies can lower the proportion of false findings in a literature, with the caveats that such studies are more helpful when they test questions for which the pre-study probability is high and when they focus on broader concepts rather than specific questions. Second, rather than focusing on significant findings from individual studies, researchers should emphasize the totality of evidence. Third, bias can be reduced by enhancing research standards, especially by encouraging pre-study registration. Finally, Ioannidis suggests that, instead of only chasing statistical significance, researchers should focus on understanding pre-study odds.
\par
\textbf{After reading this, what are your reactions? Are you surprised? How, if at all, does this change your perception about research in general? How might the individual factors described in the corollaries influence each other to exacerbate bias?}}
}

@article{jara_tracing_2018,
  title = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}: {{Official}} and {{Grassroots Initiatives}}},
  shorttitle = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}},
  author = {Jara, Daniela and Badilla, Manuela and Figueiredo, Ana and Cornejo, Marcela and Riveros, Victoria},
  year = {2018},
  month = nov,
  journal = {International Journal of Transitional Justice},
  volume = {12},
  number = {3},
  pages = {479--498},
  issn = {1752-7716},
  doi = {10.1093/ijtj/ijy025},
  abstract = {This article critically examines the official misrecognition of Mapuche experiences of violence during Augusto Pinochet's dictatorship (1973\textendash 1990) in state-sponsored truth commissions in Chile. We examine official post-dictatorial truth commission politics, narratives and procedures, analyzing how they envisioned the Mapuche as a political (absent) subject and how specific and homogenizing notions of victimhood were produced. We draw attention to three forms of cultural response by the Mapuche toward the official practices of the truth commissions from a bottom-up perspective: indifference, ambivalence and cultural resistance. We then draw attention to unofficial initiatives by nongovernmental organizations (NGOs) and grassroots groups that have aimed to tackle this gap in the transitional justice mechanisms by creating oppositional knowledge. We see in these counter initiatives a valuable knowledge that could allow the creation of bridges between Mapuche communities, mechanisms of transitional justice, grassroots and NGO activism and the Chilean state.}
}

@article{jerolmack_ethical_2019,
  title = {The {{Ethical Dilemmas}} and {{Social Scientific Trade}}-Offs of {{Masking}} in {{Ethnography}}},
  author = {Jerolmack, Colin and Murphy, Alexandra K.},
  year = {2019},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {48},
  number = {4},
  pages = {801--827},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117701483},
  abstract = {Masking, the practice of hiding or distorting identifying information about people, places, and organizations, is usually considered a requisite feature of ethnographic research and writing. This is justified both as an ethical obligation to one's subjects and as a scientifically neutral position (as readers are enjoined to treat a case's idiosyncrasies as sociologically insignificant). We question both justifications, highlighting potential ethical dilemmas and obstacles to constructing cumulative social science that can arise through masking. Regarding ethics, we show, on the one hand, how masking may give subjects a false sense of security because it implies a promise of confidentiality that it often cannot guarantee and, on the other hand, how naming may sometimes be what subjects want and expect. Regarding scientific tradeoffs, we argue that masking can reify ethnographic authority, exaggerate the universality of the case (e.g., ``Middletown''), and inhibit replicability (or ``revisits'') and sociological comparison. While some degree of masking is ethically and practically warranted in many cases and the value of disclosure varies across ethnographies, we conclude that masking should no longer be the default option that ethnographers unquestioningly choose.},
  language = {en},
  keywords = {disclosure,ethics,ethnography,generalizability,masking,pseudonyms}
}

@article{jl_it_2017,
  title = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}: {{Thoughts}} for and {{From Clinical Psychological Science}}},
  shorttitle = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}},
  author = {Jl, Tackett and So, Lilienfeld and Cj, Patrick and Sl, Johnson and Rf, Krueger and Jd, Miller and Tf, Oltmanns and Pe, Shrout},
  year = {2017},
  month = sep,
  journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  volume = {12},
  number = {5},
  publisher = {{Perspect Psychol Sci}},
  issn = {1745-6924},
  doi = {10.1177/1745691617690042},
  abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. N \ldots},
  language = {en},
  pmid = {28972844}
}

@article{john_measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychol Sci},
  volume = {23},
  number = {5},
  pages = {524--532},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  language = {en},
  keywords = {disclosure,judgment,methodology,practices,professional standards}
}

@article{kapiszewski_openness_2019,
  title = {Openness in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2019},
  month = jun,
  doi = {10.33774/apsa-2019-if2he},
  abstract = {The discipline of political science has been engaged in discussion about when, why, and how to make scholarship more open for at least three decades.This piece argues that the best way to resolve our differences and develop appropriate norms and guidelines for making different types of qualitative research more open is to move from ``if'' to ``how'' \textendash{} for individual political scientists to make their work more open \textendash{} generating examples from which we can learn and on which we can build. We begin by articulating a series of principles that underlie our views on openness. We then consider the ``state of the debate,'' briefly outlining the contours of the scholarship on openness in political and other social sciences, highlighting the fractured nature of the discussion. The heart of the piece considers various strategies, illustrated by exemplary applications, for making qualitative research more open.},
  language = {en}
}

@article{kapiszewski_transparency_2021,
  title = {Transparency in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2021},
  month = apr,
  journal = {PS: Political Science \& Politics},
  volume = {54},
  number = {2},
  pages = {285--291},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000955},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000955/resource/name/firstPage-S1049096520000955a.jpg},
  language = {en},
  keywords = {transparency}
}

@article{klein_practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jun,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  keywords = {herramienta}
}

@techreport{lareferencia.consejodirectivo_comunicacion_2019,
  title = {{Comunicaci\'on Acad\'emica y Acceso Abierto: Acciones para un Pol\'itica P\'ublica en Am\'erica Latina}},
  shorttitle = {{Comunicaci\'on Acad\'emica y Acceso Abierto}},
  author = {LA Referencia. Consejo Directivo},
  year = {2019},
  month = may,
  institution = {{Zenodo}},
  doi = {10.5281/ZENODO.3229410},
  abstract = {Documento redactado como insumo  para las autoridades regionales que asistieron a la reuni\'on anual del Global Research Council con acuerdo del Consejo Directivo.   La publicaci\'on y difusi\'on del mismo se realiza con el fin de favorecer el di\'alogo y la construcci\'on de una visi\'on conjunta sobre la cual se debe profundizar y actualizar a la luz de los desaf\'ios del Acceso Abierto en la regi\'on en el corto y mediano plazo. La comunicaci\'on cient\'ifica y el cambio del modelo; la situaci\'on de Am\'erica Latina; el sistema de comunicaci\'on acad\'emica de la regi\'on, principios y acciones y recomendaciones para repositorios, consorcios y revistas son los ejes tem\'aticos que se abordan a lo largo de sus p\'aginas. El art\'iculo  refuerza la premisa de que se deben tomar acciones decididas para que los resultados financiados total o parcialmente con fondos p\'ublicos est\'en en Acceso Abierto y reafirma el rol central de los organismos de CyT para lograrlo. Basado en la realidad regional, propone principios generales y acciones para los repositorios de Acceso Abierto, consorcios y revistas con una mirada m\'as sist\'emica desde las pol\'iticas p\'ublicas. Concluye con la necesidad de un di\'alogo con iniciativas como el ``Plan S'' se\~nalando los puntos de acuerdo, as\'i como diferencias, dado el contexto regional, en temas como el APC o una valorizaci\'on del rol de los repositorios. Presentado en la reuni\'on  de   COAR. 2019.  Technical and Strategic Meeting of Repository Networks. Mayo 21, 2019 - Lyon, France. Alberto Cabezas Bullemore, Secretario Ejecutivo,   LA Referencia.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es},
  keywords = {Acceso Abierto,Ciencia Abierta,Financiadores,ONCYTs,Plan S,Repositorios,revisado}
}

@article{lewandowsky_research_2016,
  title = {Research Integrity: {{Don}}'t Let Transparency Damage Science},
  shorttitle = {Research Integrity},
  author = {Lewandowsky, Stephan and Bishop, Dorothy},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {459--461},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/529459a},
  abstract = {Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.},
  copyright = {2016 Nature Publishing Group},
  language = {en},
  keywords = {transparency}
}

@article{lindsay_seven_2020,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science.},
  author = {Lindsay, D. Stephen},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology/Psychologie canadienne},
  volume = {61},
  number = {4},
  pages = {310--317},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000222},
  language = {en},
  keywords = {forrt,transparency},
  note = {\textbf{FORRT Sum}\textbf{}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-132}{}}

\begin{itemize}

\item Publication bias, small sample sizes,~and p-hacking exaggerate effect sizes in the literature, contributing to the replication crisis.

\item Lindsay proposes seven steps to improve transparency and replicability:

\item 1. ~ Tell the truth. Be honest and advocate research-if the idea was inspired by data, state so. Report effect size with 95\% confidence intervals around them.

\item 2. ~ Assess your understanding of inferential statistical tools.~We need ~improved statistical sophistication for researchers to test hypotheses about populations based on their samples - reward quality and accuracy of methods, not quantity and flashiness of results.

\item 3. Consider standardizing aspects of your approach to conducting hypothesis testing research. Create a detailed research plan providing a priori hypotheses, sample size planning, data exclusion rules, analyses, transformations, covariates etc. Be transparent and register a research plan (cf. Pre-registration and Registered reports).

\item 4. ~ Consider developing a lab manual. Include standardised procedures in data exclusion, data transformations, data-cleaning, authorship, file naming conventions, etc.

\item 5. ~Make your materials, data, and analysis scripts transparent. They should be Findable, Accessible, Interoperable and Reusable (FAIR).

\item 6. ~ Address constraints on the generality of your findings. Under what conditions should your results replicate, and not replicate? Failure to replicate could be due to differences in procedures, albeit original work did not indicate such differences modulate effect.

\item 7. ~ Consider collaborative approaches to conducting research.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote-76}{}}

\begin{quotation}
``The aim of the methodological reform movement is not to restrict psychological research to procedures that meet some fixed criterion of replicability. Replicability is not in itself the goal of science. Rather, the central aim of methodological reform is to make research reports more transparent, so that readers can gain an accurate understanding of how the data were obtained and analyzed and can therefore better gauge how much confidence to place in the findings. A second aim is to discourage practices that contribute to effect-size exaggeration and false discoveries of non-existent phenomena. As per Vazire's analogy, the call is not for car dealerships to sell nothing but new Ferraris, but rather for dealers to be forthcoming about the weaknesses of what they have on the lot. The grand aim of science is to develop better, more accurate, and more useful understandings of reality. Methodological reform cannot in and of itself deliver on that goal, but it can help.'' (p.19).

\end{quotation}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-132}{}}

\par
\textbf{Psychological scientists strive to advance understanding of how and why we animals do and think and feel as we do. This is difficult, in part because flukes of chance and measurement error obscure researchers' perceptions. Many psychologists use inferential statistical tests to peer through the murk of chance and discern relationships between variables. Those tests are powerful tools, but they must be wielded with skill. Moreover, research reports must convey to readers a detailed and accurate understanding of how the data were obtained and analyzed. Research psychologists often fall short in those regards. This paper attempts to motivate and explain ways to enhance the transparency and replicability of psychological science. Specifically, I speak to how publication bias and p hacking contribute to effect-size exaggeration in the published literature, and how effect-size exaggeration contributes, in turn, to replication failures. Then I present seven steps toward addressing these problems: Telling the truth; upgrading statistical knowledge; standardizing aspects of research practices; documenting lab procedures in a lab manual; making materials, data, and analysis scripts transparent; addressing constraints on generality; and collaborating.}}
}

@article{linton_publish_2011,
  title = {Publish or {{Perish}}: {{How Are Research}} and {{Reputation Related}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {Linton, Jonathan D. and Tierney, Robert and Walsh, Steven T.},
  year = {2011},
  month = dec,
  journal = {Serials Review},
  volume = {37},
  number = {4},
  pages = {244--257},
  publisher = {{Routledge}},
  issn = {0098-7913},
  doi = {10.1080/00987913.2011.10765398},
  abstract = {A study of twenty-seven fields in 350 highly ranked universities examines the relationship between reputation and rank. We find that many metrics associated with research prowess significantly correlate to university reputation. However, the next logical step\textendash{} looking at the relationship that links different academic fields with the reputation of the university\textendash did not always offer the expected results. The phrase ``publish or perish'' clearly has very different meanings in different fields.},
  keywords = {Academic reputation,institutional,Interdisciplinary studies,Publish or perish,University research reputation},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00987913.2011.10765398}
}

@misc{loredo_derecho_2012,
  title = {Derecho {{Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  author = {Loredo, Alejandro},
  year = {2012},
  journal = {Portal de gobierno electr\'onico, inclusi\'on digital y sociedad del conocimiento}
}

@article{loredo_mexico_2006,
  title = {M\'exico: {{Derecho Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  shorttitle = {M\'exico},
  author = {Loredo, Alejandro},
  year = {2006},
  journal = {AR: Revista de Derecho Inform\'atico},
  number = {91},
  pages = {2},
  publisher = {{Alfa-Redi}}
}

@techreport{luke_epistemological_2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Epistemological and {{Ontological Priors}}: {{Explicating}} the {{Perils}} of {{Transparency}}},
  shorttitle = {Epistemological and {{Ontological Priors}}},
  author = {Luke, Timothy W. and {V{\'a}zquez-Arroyo}, Antonio and Hawkesworth, Mary},
  year = {2019},
  month = feb,
  number = {ID 3332878},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3332878},
  abstract = {The discipline of political science encompasses multiple research communities, which have grown out of and rely upon different epistemological and ontological presuppositions.  Recent debates about transparency raise important questions about which of these research communities will be accredited within the discipline, whose values, norms, and methods of knowledge production will gain ascendency and whose will be marginalized.  Although the language of "transparency" makes it appear that these debates are apolitical, simply elaborating standards that all political scientists share, the intensity and content of recent contestations about DA-RT, JETS, and QTD attest to the profoundly political nature of these methodological discussions. This report traces the epistemological and ontological assumptions that have shaped diverse research communities within the discipline, situating "transparency" in relation to classical (Aristotelian), modern (Baconian) and twentieth-century (positivist, critical rationalist, and postpositivist) versions of empiricism.  It shows how recent discussions of transparency accredit certain empirical approaches by collapsing the scope of empirical investigation and the parameters of the knowable.  And it argues that "transparency" is inappropriate as a regulative ideal for political science because it misconstrues the roles of theory, social values, and critique in scholarly investigation.},
  language = {en},
  keywords = {epistemology,ontology,philosophy of science,qualitative methods,Qualitative Transparency Deliberations,research transparency}
}

@article{martinson_scientists_2005,
  title = {Scientists Behaving Badly},
  author = {Martinson, Brian C. and Anderson, Melissa S. and {de Vries}, Raymond},
  year = {2005},
  month = jun,
  journal = {Nature},
  volume = {435},
  number = {7043},
  pages = {737--738},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/435737a},
  abstract = {To protect the integrity of science, we must look beyond falsification, fabrication and plagiarism, to a wider range of questionable research practices, argue Brian C. Martinson, Melissa S. Anderson and Raymond de Vries.},
  copyright = {2005 Nature Publishing Group},
  language = {en},
  keywords = {practices},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion}
}

@article{mcgrail_publish_2006,
  title = {Publish or Perish: A Systematic Review of Interventions to Increase Academic Publication Rates},
  shorttitle = {Publish or Perish},
  author = {McGrail, Matthew R. and Rickard, Claire M. and Jones, Rebecca},
  year = {2006},
  month = feb,
  journal = {Higher Education Research \& Development},
  volume = {25},
  number = {1},
  pages = {19--35},
  publisher = {{Routledge}},
  issn = {0729-4360},
  doi = {10.1080/07294360500453053},
  abstract = {Academics are expected to publish. In Australia universities receive extra funding based on their academic publication rates and academic promotion is difficult without a good publication record. However, the reality is that only a small percentage of academics are actively publishing. To fix this problem, a number of international universities and other higher education institutions have implemented interventions with the main aim being to increase the number of publications. A comprehensive literature search identified 17 studies published between 1984 and 2004, which examined the effects of these interventions. Three key types of interventions were identified: writing courses, writing support groups and writing coaches. The resulting publication output varied, but all interventions led to an increase in average publication rates for the participants.},
  keywords = {institutional},
  annotation = {\_eprint: https://doi.org/10.1080/07294360500453053}
}

@article{mckiernan_how_2016,
  title = {How Open Science Helps Researchers Succeed},
  author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
  editor = {Rodgers, Peter},
  year = {2016},
  month = jul,
  journal = {eLife},
  volume = {5},
  pages = {e16800},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.16800},
  abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
  keywords = {open access,open data,open science,open source,research}
}

@article{mcvay_transparency_2021,
  title = {Transparency and Openness in Behavioral Medicine Research},
  author = {McVay, Megan A and Conroy, David E},
  year = {2021},
  month = jan,
  journal = {Translational Behavioral Medicine},
  volume = {11},
  number = {1},
  pages = {287--290},
  issn = {1869-6716},
  doi = {10.1093/tbm/ibz154},
  abstract = {Behavioral medicine aims to improve the health of individuals and communities by addressing behavioral, psychosocial, and environmental contributors to health. Succeeding in this endeavor requires rigorous research and effective communication of this research to relevant stakeholders and the public at large [1]. Both research rigor and effective communication of research may benefit from adopting transparent and open research practices [2\textendash 4], sometimes called ``open science.'' Such practices include preregistering designs, hypotheses, and data analysis plans; making publically available study materials, data, and analytic code; sharing preprints (works-in-progress) of articles; and publishing open access [2]. In this commentary, we describe the evolving pressures to increase the transparency and openness of research, examine the status of open science practices in behavioral medicine, and recommend a path forward to find the right fit for these practices in behavioral medicine research.},
  keywords = {transparency}
}

@article{melero_revistas_2008,
  title = {Revistas {{Open Access}}: Caracter\'isticas, Modelos Econ\'omicos y Tendencias},
  author = {Melero, Remedios and Abad, Mar{\'i}a Francisca},
  year = {2008},
  volume = {20},
  issn = {1575-5886}
}

@article{mertens_preregistration_2019,
  title = {Preregistration of {{Analyses}} of {{Preexisting Data}}},
  author = {Mertens, Ga{\"e}tan and Krypotos, Angelos-Miltiadis},
  year = {2019},
  journal = {Psychol Belg},
  volume = {59},
  number = {1},
  pages = {338--352},
  issn = {0033-2879},
  doi = {10.5334/pb.493},
  abstract = {The preregistration of a study's hypotheses, methods, and data-analyses steps is becoming a popular psychological research practice. To date, most of the discussion on study preregistration has focused on the preregistration of studies that include the collection of original data. However, much of the research in psychology relies on the (re-)analysis of preexisting data. Importantly, this type of studies is different from original studies as researchers cannot change major aspects of the study (e.g., experimental manipulations, sample size). Here, we provide arguments as to why it is useful to preregister analyses of preexisting data, discuss practical considerations, consider potential concerns, and introduce a preregistration template tailored for studies focused on the analyses of preexisting data. We argue that the preregistration of hypotheses and data-analyses for analyses of preexisting data is an important step towards more transparent psychological research.},
  pmcid = {PMC6706998},
  pmid = {31497308}
}

@article{miguel_promoting_2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and der Laan, M. Van},
  year = {2014},
  month = jan,
  journal = {Science},
  volume = {343},
  number = {6166},
  pages = {30--31},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245317},
  abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests (1, 2). Social scientists should adopt higher transparency standards to improve the quality and credibility of research. Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  language = {en},
  pmid = {24385620},
  keywords = {transparency}
}

@article{moore_preregister_2016,
  title = {Preregister If You Want To},
  author = {Moore, Don A.},
  year = {2016},
  month = apr,
  journal = {Am Psychol},
  volume = {71},
  number = {3},
  pages = {238--239},
  issn = {1935-990X},
  doi = {10.1037/a0040195},
  abstract = {Prespecification of confirmatory hypothesis tests is a useful tool that makes our statistical tests informative. On the other hand, selectively reporting studies, measures, or statistical tests renders the probability of false positives higher than the p values would imply. The bad news is that it is usually difficult to tell how much higher the probability is. Fortunately, there are enormous opportunities to improve the quality of our science by preregistering our research plans. Preregistration is a highly distinctive strength that should increase our faith in the veracity and replicability of a research result.},
  language = {eng},
  pmid = {27042885},
  keywords = {Clinical Trials as Topic,Humans,Information Dissemination,Reproducibility of Results,Research Design,Science}
}

@article{motta_dynamics_2018,
  title = {The {{Dynamics}} and {{Political Implications}} of {{Anti}}-{{Intellectualism}} in the {{United States}}},
  author = {Motta, Matthew},
  year = {2018},
  month = may,
  journal = {American Politics Research},
  volume = {46},
  number = {3},
  pages = {465--498},
  publisher = {{SAGE Publications Inc}},
  issn = {1532-673X},
  doi = {10.1177/1532673X17719507},
  abstract = {Recently, Americans have become increasingly likely to hold anti-intellectual attitudes (i.e., negative affect toward scientists and other experts). However, few have investigated the political implications of anti-intellectualism, and much empirical uncertainty surrounds whether or not these attitudes can be mitigated. Drawing on cross-sectional General Social Survey (GSS) data and a national election panel in 2016, I find that anti-intellectualism is associated with not only the rejection of policy-relevant matters of scientific consensus but support for political movements (e.g., ``Brexit'') and politicians (e.g., George Wallace, Donald Trump) who are skeptical of experts. Critically, though, I show that these effects can be mitigated. Verbal intelligence plays a strong role in mitigating anti-intellectual sympathies, compared with previously studied potential mitigators. I conclude by discussing how scholars might build on this research to study the political consequences of anti-intellectualism in the future.},
  language = {en},
  keywords = {anti-intellectualism,antiscience attitudes,political psychology,public opinion,verbal intelligence}
}

@article{motyl_state_2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  month = jul,
  journal = {J Pers Soc Psychol},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (PsycINFO Database Record},
  language = {eng},
  pmid = {28447837},
  keywords = {Attitude of Health Personnel,Ethics; Research,Female,Humans,Male,Personality,Psychology,Psychology; Social,Research,Research Design,Surveys and Questionnaires}
}

@article{munafo_manifesto_2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nat Hum Behav},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  copyright = {2017 Macmillan Publishers Limited},
  language = {en},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Social sciences Subject\_term\_id: social-sciences}
}

@misc{nassi-calo_open_2013,
  title = {Open {{Access}} and a Call to Prevent the Looming Crisis in Science | {{SciELO}} in {{Perspective}}},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2013},
  month = jul,
  abstract = {The number of retracted articles has recently been on the rise. Bj\"orn Brembs identifies this tendency as a reflection of an imminent crisis in science whose},
  language = {en-US},
  keywords = {crisis}
}

@misc{nassi-calo_reproducibilidad_2014,
  title = {La Reproducibilidad En Los Resultados de Investigaci\'on: La Mirada Subjetiva | {{SciELO}} En {{Perspectiva}}},
  shorttitle = {La Reproducibilidad En Los Resultados de Investigaci\'on},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2014},
  month = feb,
  abstract = {En una \'epoca en que las discusiones sobre \'etica en la experimentaci\'on y la publicaci\'on cient\'ifica traspasan los laboratorios y ambientes acad\'emicos para},
  language = {en-US}
}

@article{naturehumanbehaviour_tell_2020,
  title = {Tell It like It Is},
  author = {{Nature human behaviour}},
  year = {2020},
  month = jan,
  journal = {Nat Hum Behav},
  volume = {4},
  number = {1},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0818-9},
  abstract = {Every research paper tells a story, but the pressure to provide `clean' narratives is harmful for the scientific endeavour.},
  copyright = {2020 Springer Nature Limited},
  language = {en},
  keywords = {forrt,transparency},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-135}{}}

\begin{itemize}

\item A manuscript provides an account of how a research question(s) is/are addressed, reports findings, and explains how findings support or contradict hypotheses.

\item Current research culture is defined by a pressure to present research projects as conclusive narratives that leave no room for ambiguity.

\item The pressure to produce clean narratives represents a threat to validity and counter reality of what science looks like.

\item Clean narratives often report only outcomes to confirm original predictions or exclude~research findings that provide messy results.

\item These questionable research practices create a distorted picture of research that prevents cumulative knowledge.

\item Pre-registration has little value if not heeded or transparently reported.

\item It sometimes becomes~evident during peer review that a pre-registered analysis is inappropriate or suboptimal. Authors should ~indicate deviations from their original plan, ~and explain why they did these deviations.

\item To ensure transparency, unless a preregistered analysis plan is unquestionably flawed, authors should also report the results of their preregistered analyses.

\item In multi-study research papers authors should report all work they executed, irrespective of outcomes.

\item All research papers must include a limitation section that explains study shortcomings and explicitly acknowledges alternative interpretations of the findings.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote-78}{}}

\begin{quotation}
``No research project is perfect; there are always limitations that also need to be transparently reported. In 2019, we made it a requirement that all our research papers include a limitations section, in which authors explain methodological and other shortcomings and explicitly acknowledge alternative interpretations of their findings\ldots{} Science is messy, and the results of research rarely conform fully to plan or expectation. `Clean' narratives are an artefact of inappropriate pressures and the culture they have generated. We strongly support authors in their efforts to be transparent about what they did and what they found, and we commit to publishing work that is robust, transparent and appropriately presented, even if it does not yield `clean' narratives'' p.1

\end{quotation}}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {PNAS},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  language = {en},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration,reports,revisado},
  note = {\textbf{Brief description/principal idea}
\par
El art\'iculo parte de la idea de que "la ciencia se basa en parte en generar hip\'otesis con observaciones existentes y probar hip\'otesis con nuevas observaciones". Esto implica marcar una distinci\'on muy clara entre lo que es la investigaci\'on \textbf{confirmatoria }y la investigaci\'on \textbf{exploratoria. }En la pr\'actica esta distinci\'on se quiebra: an\'alisis exploratorios se hacen pasar por confirmatorios, y eso le quita credibilidad a los hallazgos. El art\'iculo propone los \textbf{preregistros }para ayudar a marcar esta distinci\'on en la practica.
\par
Tambi\'en, se se\~nalan una serie de desaf\'ios para el uso de preregistros:
\par
\begin{itemize}

\item Desaf\'io 1: Cambios en el procedimiento durante la administraci\'on del estudio.

\item 

\par
Desaf\'io 2: Descubrimiento de violaciones de supuestos durante el an\'alisis.
\par
\item 

\par
Desaf\'io 3: Los datos son preexistentes
\par
\item 

\par
Desaf\'io 4: Estudios longitudinales y grandes bases de datos multivariadas
\par
\item 

\par
Desaf\'io 5: Muchos experimentos
\par
\item 

\par
Desaf\'io 6: Un Programa de investigaci\'on
\par
\item 

\par
Desaf\'io 7: Algunas expectativas a priori.
\par
\item 

\par
Desaf\'io 8: Predicciones en competencia
\par
\item 

\par
Desaf\'io 9: Inferencias narrativas y conclusiones
\par
\end{itemize}

\par
\textbf{Cites}
\par
\#\# Important cite
\par
*Importancia de la distinci\'on*
\par
Presentar las postdicciones como predicciones puede aumentar el atractivo y la publicabilidad de los hallazgos al reducir falsamente la incertidumbre. En \'ultima instancia, esto disminuye la reproducibilidad.
\par
\#\# Cite
\par
*Problema de usar indistintamente predicci\'on y postdicci\'on*
\par
El problema con esto se entiende como teorizaci\'on post hoc o hip\'otesis despu\'es de que se conocen los resultados (12). Es un ejemplo de razonamiento circular: generar una hip\'otesis basada en la observaci\'on de datos y luego evaluar la validez de la hip\'otesis basada en los mismos datos.
\par
\#\# First Sentence
\par
El progreso de la ciencia se caracteriza por la reducci\'on de la incertidumbre sobre la naturaleza.
\par
\#\# Cite
\par
Los cient\'ificos mejoran los modelos generando hip\'otesis basadas en observaciones existentes y probar esas hip\'otesis obteniendo nuevas observaciones.
\par
Hypothesis-generating -{$>$} Postdiction
\par
Hypothesis-testing -{$>$} Prediction
\par
\#\# Cite
\par
Los resultados positivos \textendash\textendash{} encontrar una relaci\'on entre las variables o un efecto de los tratamientos sobre los resultados \textendash\textendash{} son recompensados \mbox\mbox m\'as que los resultados negativos \textendash\textendash{} no encontrar una relaci\'on o efecto; Los resultados limpios que proporcionan una narrativa s\'olida se recompensan m\'as que los resultados que muestran incertidumbre o excepciones a la narrativa preferida (9, 19-21).
\par
\#\# Cite
\par
Si los investigadores informan de manera selectiva los resultados positivos con m\'as frecuencia que los resultados negativos, entonces aumentar\'a la probabilidad de falsos positivos (38\textendash 40).}
}

@article{nosek_preregistration_2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and {van 't Veer}, Anna E. and Vazire, Simine},
  year = {2019},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.07.009},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  language = {en},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency}
}

@article{nosek_promoting_2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  journal = {Science},
  volume = {348},
  number = {6242},
  pages = {1422--1425},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  language = {en},
  pmid = {26113702},
  keywords = {revisado}
}

@article{nosek_registered_2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  month = may,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  language = {en},
  keywords = {forrt,reports},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-1}{}}

\begin{itemize}

\item This editorial discusses the value of pre-registration and replication, as not all articles are published.

\item Direct replication adds data that increases the precision of effect size estimates for meta-analytic research. No direct replication, means it is difficult to identify false positives.

\item Conceptual replications are more popular than direct replications, as the former conceptualises a phenomenon from its original operationalisation, thus contributing to our theoretical understanding of the effect.

\item Direct replication encourages generalisability of effects, providing evidence that the effect was not due to sampling, procedural or contextual error.

\item If direct replication produces negative results, this~improves the identification of boundary conditions for real effects.

\item The benefit of a registered report is that the feedback provided from peer review allows researchers to improve their experimental design.

\item Following peer review, the manuscript can be resubmitted for review and acceptance or rejection based on feedback.

\item Successful proposals tend to be high-powered, high quality, and faithful replication designs.

\item One benefit of a registered report is that this can be all done before the research is conducted.

\item Peer reviewers will focus on the methodological quality of the research, allowing conflict of interests to be ~reduced and peer reviewers can provide a fair assessment of the manuscript.

\item The original studies can provide an exaggerated effect size. When this study is replicated, the effect size usually decreases as a result of a larger sample size.

\item Registered reports enable exploratory and confirmatory analyses, but a distinction is required. However, more trust can be placed in confirmatory analyses, as it follows a plan and ensures the interpretability of reported p value.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote-1}{}}

\begin{quotation}
``No single replication provides the definitive word for or against the reality of an effect, just as no original study provides definitive evidence for it. Original and replication research each provides a piece of accumulating evidence for understanding an effect and the conditions necessary to obtain it. Following this special issue, Social Psychology will publish some commentaries and responses by original and replication authors of their reflections on the inferences from the accumulated data, and questions that could be addressed in follow-up research.'' (p. 139)

\end{quotation}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-1}{}}

\par
\textbf{Professor Daniel Laken and Professor Brian Nosek provide an editorial on how pre-registration and registered reports can be used for the journal of Social Psychology in order to increase the credibility of individual results and findings.}}
}

@article{nosek_transparency_2014,
  title = {Transparency and {{Openness Promotion}} ({{TOP}}) {{Guidelines}}},
  author = {Nosek, Brian A. and Alter, George and Banks, George Christopher and Borsboom, Denny and Bowman, Sara and Breckler, Steven and Buck, Stuart and Chambers, Chris and Chin, Gilbert and Christensen, Garret},
  year = {2014},
  month = aug,
  publisher = {{OSF}},
  abstract = {The Transparency and Openness Promotion (TOP) Committee met in November 2014 to address one important element of the incentive systems - journals' procedures and  policies for publication. The outcome of the effort is the TOP Guidelines. There are eight standards in the TOP guidelines; each move scientific communication toward greater openness.  These standards are modular, facilitating adoption in whole or in part. However, they also complement each other, in that commitment to one standard may facilitate adoption of others. Moreover, the guidelines are sensitive to barriers to openness by articulating, for example, a process for exceptions to sharing because of ethical issues, intellectual property concerns, or availability of necessary resources.      Hosted on the Open Science Framework},
  language = {en},
  keywords = {(,transparency}
}

@misc{nw_trust_2019,
  title = {Trust and {{Mistrust}} in {{Americans}}' {{Views}} of {{Scientific Experts}}},
  author = {NW, 1615 L. St and Suite 800Washington and Inquiries, DC 20036USA202-419-4300 | Main202-857-8562 | Fax202-419-4372 | Media},
  year = {2019},
  month = aug,
  journal = {Pew Research Center Science \& Society},
  abstract = {Public confidence in scientists is on the upswing, and six-in-ten Americans say scientists should play an active role in policy debates about scientific issues, according to a new Pew Research Center survey.},
  language = {en-US}
}

@article{oboyle_chrysalis_2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  journal = {Journal of Management},
  volume = {43},
  number = {2},
  pages = {376--399},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  language = {en},
  keywords = {ethics,morality and moral behavior,philosophy of science,revisado,statistical methods,transparency},
  note = {\textbf{Brief description/principal idea}\textbf{}
\par
El art\'iculo contiene tres grandes aportes. Primero, hacen una sintesis de las practicas cuestionables de investigaci\'on (QRP, por sus siglas en ingl\'es). Esta sintesis la hacen bajo el marco de la "strain theory", buscando explicar el "Chrysalis effect". Este efecto refiere a c\'omo resultados no atractivos se vuelven en un reporte de investigaci\'on atractivo (crisalida, paso de oruga a mariposa.
\par
El segundo aporte es que hace un an\'alisis emp\'irico sobre el Chrysalis effect, siendo el halllazgo principal: "que desde la disertaci\'on hasta el art\'iculo de revista, la proporci\'on de hip\'otesis respaldadas y no respaldadas se duplic\'o con creces (0,82 a 1,00 frente a 1,94 a 1,00). El aumento en la precisi\'on predictiva result\'o de la eliminaci\'on de hip\'otesis estad\'isticamente no significativas, la adici\'on de hip\'otesis estad\'isticamente significativas, la inversi\'on de la direcci\'on predicha de las hip\'otesis y las alteraciones de los datos"
\par
El tercer aporte es una serie de sugerencias para el evitar las QRP y el Chrysalis effect.
\par
QRP
\par
1. Eliminaci\'on o adici\'on de datos despu\'es de pruebas de hip\'otesis.
\par
2. Alterando los datos despu\'es de la prueba de hip\'otesis.
\par
3. Supresi\'on selectiva o adici\'on de variables.
\par
4. Invertir la direcci\'on o reformular hip\'otesis para respaldar los datos
\par
5. Eliminaci\'on o adici\'on post hoc de hip\'otesis.
\par
Motivos para caer en QRP
\par
1. Anticipaci\'on a las reacciones de los revisores o al cumplimiento de las solicitudes de los revisores.
\par
2. Listas de revistas y heur\'istica de publicaciones.
\par
Oportunidades para QRP
\par
1. Pr\'acticas de informaci\'on y privacidad de datos.
\par
2. Falta de replicaci\'on.
\par
Sugerencias para evitar caer en QRP
\par
1. Clausula de \'etica de no haber participado en QRP al enviar manoescritos
\par
2. Todo articulo original (p.ej tesis doctoral) debe estar disponible para descarfa
\par
3. Que las revistas cuenten con espacio dedicado a replicaci\'on.
\par
\textbf{Cites}
\par
\#\# Cite
\par
Estas presiones pueden llevar a los investigadores a involucrarse en pr\'acticas de investigaci\'on cuestionables (QRP) para reforzar sus posibilidades de publicaci\'on (Chen, 2011; Fanelli, 2010a, Kepes \& McDaniel, 2013)
\par
\#\# Important cite
\par
*QRP description*
\par
A diferencia de la flagrante mala conducta cient\'ifica (por ejemplo, la fabricaci\'on de datos), los QRP bordean la l\'inea entre el comportamiento \'etico y no \'etico. Los ejemplos de QRP incluyen la presentaci\'on de hallazgos post hoc como hip\'otesis a priori y la eliminaci\'on de puntos de datos basados \mbox\mbox en criterios post hoc para lograr resultados estad\'isticamente significativos (Leung, 2011). El resultado de estos QRP es un aumento de los errores de Tipo I y una supresi\'on de los efectos nulos, lo que sesga la literatura (John, Loewenstein y Prelec, 2012). A esta forma de sesgo en el informe de resultados la etiquetamos como "Efecto Cris\'alida" despu\'es del proceso de metamorfosis por el cual una oruga fea (resultados iniciales) se convierte en una hermosa mariposa (art\'iculo de revista).
\par
\# Argument
\par
Argumentamos que el proceso de publicaci\'on y los sistemas de recompensa en la academia brindan los medios, los motivos y la oportunidad para participar en los QRP. Espec\'ificamente, la investigaci\'on emp\'irica ha documentado la continua dependencia impl\'icita en los sistemas de revisi\'on por pares de las pruebas de significaci\'on de hip\'otesis nulas como un sustituto del rigor y la relevancia (por ejemplo, Emerson et al., 2008). Sin embargo, contrariamente a lo que se desea, la dependencia de tal criterio ofrece a los investigadores un conjunto de medios de publicaci\'on independientes del rigor y la relevancia. Adem\'as, el sistema de compensaci\'on que se encuentra en la academia es un sistema de recompensa al estilo de un torneo (Rynes \& Gerhart, 2003), que pone gran \'enfasis en las publicaciones. Este es un sistema en gran medida positivo que motiva la innovaci\'on y la nueva teor\'ia de la gesti\'on, pero tambi\'en crea un motivo para participar en los QRP. Finalmente, la falta de supervisi\'on en el proceso de investigaci\'on y revisi\'on brinda la oportunidad de participar en QRP con pocas posibilidades de detecci\'on.
\par
\#\# Cite
\par
Independientemente de los medios y motivos para mejorar las posibilidades de publicaci\'on a trav\'es de pr\'acticas no ideales, es poco probable que los investigadores se involucren en QRP si la probabilidad de detecci\'on fuera alta. Por lo tanto, la falta casi total de supervisi\'on en la recopilaci\'on y el an\'alisis de datos brinda a los investigadores de gesti\'on la oportunidad de participar en QRP sin temor a ser detectados.
\par
\#\# Cite
\par
A pesar de una mayor conciencia de lo que son los QRP y el da\~no que causan (por ejemplo, Bedeian et al., 2010; Martinson, Anderson y De Vries, 2005), los QRP persisten. Sostenemos que esto se debe a que, como campo, recompensamos a los QRP y estamos inmersos en una cultura que reduce la probabilidad de su detecci\'on. Como tal, es poco probable que se produzcan reducciones de QRP si se asigna la responsabilidad de las mejores pr\'acticas al investigador individual.}
}

@misc{omatos_aspectos_2013,
  title = {Aspectos {{Legales}} En La {{Educaci\'on}}},
  author = {Omatos, Antonio},
  year = {2013}
}

@article{opensciencecollaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  language = {en}
}

@article{ospina_problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Ospina, Adriana Mar{\'i}a Restrepo},
  year = {2014},
  journal = {Estudios de Derecho},
  volume = {71},
  number = {158},
  pages = {69--96}
}

@article{patil_visual_2019,
  title = {A Visual Tool for Defining Reproducibility and Replicability},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  year = {2019},
  month = jul,
  journal = {Nat Hum Behav},
  volume = {3},
  number = {7},
  pages = {650--652},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0629-z},
  abstract = {Reproducibility and replicability are fundamental requirements of scientific studies. Disagreements over universal definitions for these terms have affected the interpretation of large-scale replication attempts. We provide a visual tool for representing definitions and use it to re-examine these attempts.},
  copyright = {2019 Springer Nature Limited},
  language = {en},
  keywords = {herramienta},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Software;Statistics Subject\_term\_id: software;statistics}
}

@misc{pena_declaracion_2003,
  title = {{Declaraci\'on de Bethesda sobre Publicaci\'on de Acceso Abierto}},
  author = {Pe{\~n}a, Ismaes},
  year = {20 de Junio, 2003},
  language = {Traducido}
}

@article{penders_rinse_2019,
  title = {Rinse and {{Repeat}}: {{Understanding}} the {{Value}} of {{Replication}} across {{Different Ways}} of {{Knowing}}},
  shorttitle = {Rinse and {{Repeat}}},
  author = {Penders, Bart and Holbrook, J. Britt and {de Rijcke}, Sarah},
  year = {2019},
  month = sep,
  journal = {Publications},
  volume = {7},
  number = {3},
  pages = {52},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7030052},
  abstract = {The increasing pursuit of replicable research and actual replication of research is a political project that articulates a very specific technology of accountability for science. This project was initiated in response to concerns about the openness and trustworthiness of science. Though applicable and valuable in many fields, here we argue that this value cannot be extended everywhere, since the epistemic content of fields, as well as their accountability infrastructures, differ. Furthermore, we argue that there are limits to replicability across all fields; but in some fields, including parts of the humanities, these limits severely undermine the value of replication to account for the value of research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {accountability,epistemic pluralism,humanities,Replicability,replication,reproducibility,reproduction}
}

@article{peng_reproducibility_2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  journal = {Significance},
  volume = {12},
  number = {3},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  language = {en},
  keywords = {crisis},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x}
}

@article{peng_reproducible_2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, R. D.},
  year = {2011},
  month = dec,
  journal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  language = {en}
}

@article{petousi_contextualising_2020,
  title = {Contextualising Harm in the Framework of Research Misconduct. {{Findings}} from Discourse Analysis of Scientific Publications},
  author = {Petousi, Vasiliki and Sifaki, Eirini},
  year = {2020},
  month = jan,
  journal = {International Journal of Sustainable Development},
  volume = {23},
  number = {3-4},
  pages = {149--174},
  publisher = {{Inderscience Publishers}},
  issn = {0960-1406},
  doi = {10.1504/IJSD.2020.115206},
  abstract = {This article reports on research, which deals with dimensions of harm resulting from research misconduct, in articles published in scientific journals. An appropriate sample of publications retrieved from Pubmed, Scopus and WOS was selected across various disciplines and topics. Implementing discourse analysis, articles were classified according to the narratives of 'individual impurity', 'institutional failure' and 'structural crisis'. Most of the articles analysed fall within the narrative of structural crisis. The main argument advanced is that research misconduct harms the scientific enterprise as a whole. Harm is narrated in the context of institutional characteristics, policies, procedures, guidelines, and work environment. Mainly, however, harm is narrated in the context of structural characteristics of contemporary scientific practices, which result in normative dissonance for scientists and loss of trust in science in the relation between science and society and within the scientific enterprise itself. We conclude that new grounds for building trust and confidence in science are needed.},
  keywords = {practices}
}

@article{piwowar_future_2019,
  title = {The {{Future}} of {{OA}}: {{A}} Large-Scale Analysis Projecting {{Open Access}} Publication and Readership},
  shorttitle = {The {{Future}} of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Orr, Richard},
  year = {2019},
  journal = {BioRxiv},
  pages = {795310},
  publisher = {{Cold Spring Harbor Laboratory}}
}

@article{piwowar_state_2018,
  title = {The State of {{OA}}: A Large-Scale Analysis of the Prevalence and Impact of {{Open Access}} Articles},
  shorttitle = {The State of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Larivi{\`e}re, Vincent and Alperin, Juan Pablo and Matthias, Lisa and Norlander, Bree and Farley, Ashley and West, Jevin and Haustein, Stefanie},
  year = {2018},
  journal = {PeerJ},
  volume = {6},
  pages = {e4375},
  publisher = {{PeerJ Inc.}}
}

@incollection{poumadere_credibility_1991,
  title = {The {{Credibility Crisis}}},
  booktitle = {Chernobyl: {{A Policy Response Study}}},
  author = {Poumad{\`e}re, Marc},
  editor = {Segerst{\aa}hl, Boris},
  year = {1991},
  series = {Springer {{Series}} on {{Environmental Management}}},
  pages = {149--171},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-84367-9_8},
  abstract = {Those who said that the twentieth century is the century of the atom didn't know how right they were. Certainly, the scientific discovery of a particularly powerful new energy source had produced hopes and applications in both the civil and military sectors. In the final equation, though, it is the major accident at Chernobyl that dramatically and suddenly brought the reality of the atom's presence home to many persons and groups around the world.},
  isbn = {978-3-642-84367-9},
  language = {en},
  keywords = {Chernobyl Accident,crisis,Nuclear Energy,Nuclear Power Plant,Social Defense,Social Distance}
}

@article{price_problem_2020,
  title = {Problem with p Values: Why p Values Do Not Tell You If Your Treatment Is Likely to Work},
  shorttitle = {Problem with p Values},
  author = {Price, Robert and Bethune, Rob and Massey, Lisa},
  year = {2020},
  month = jan,
  journal = {Postgrad Med J},
  volume = {96},
  number = {1131},
  pages = {1--3},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/postgradmedj-2019-137079},
  language = {en},
  keywords = {forrt,practices}
}

@article{redish_opinion_2018,
  title = {Opinion: {{Reproducibility}} Failures Are Essential to Scientific Inquiry},
  shorttitle = {Opinion},
  author = {Redish, A. David and Kummerfeld, Erich and Morris, Rebecca Lea and Love, Alan C.},
  year = {2018},
  month = may,
  journal = {PNAS},
  volume = {115},
  number = {20},
  pages = {5042--5046}
}

@article{restrepo_problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Restrepo, Adriana Mar{\'i}a},
  year = {30 de Junio, 2014},
  volume = {71},
  number = {158},
  pages = {69--96}
}

@misc{rinke_probabilistic_2018,
  title = {Probabilistic {{Misconceptions Are Pervasive Among Communication Researchers}}},
  author = {Rinke, Eike Mark and Schneider, Frank M.},
  year = {2018},
  month = sep,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/h8zbe},
  abstract = {Across all areas of communication research, the most popular approach to generating insights about communication is the classical significance test (also called null hypothesis significance testing, NHST). The predominance of NHST in communication research is in spite of serious concerns about the ability of researchers to properly interpret its results. We draw on data from a survey of the ICA membership to assess the evidential basis of these concerns. The vast majority of communication researchers misinterpreted NHST (91\%) and the most prominent alternative, confidence intervals (96\%), while overestimating their competence. Academic seniority and statistical experience did not predict better interpretation outcomes. These findings indicate major problems regarding the generation of knowledge in the field of communication research.},
  keywords = {Communication,confidence intervals,data analysis,misconceptions,practices,significance testing,Social and Behavioral Sciences,statistical inference,statistics}
}

@article{rodriguez-sanchez_ciencia_2016,
  title = {{Ciencia reproducible: qu\'e, por qu\'e, c\'omo}},
  shorttitle = {{Ciencia reproducible}},
  author = {{Rodriguez-Sanchez}, Francisco and {P{\'e}rez-Luque}, Antonio Jes{\'u}s and Bartomeus, Ignasi and Varela, Sara},
  year = {2016},
  month = jul,
  journal = {ECOS},
  volume = {25},
  number = {2},
  pages = {83--92},
  issn = {1697-2473},
  doi = {10.7818/ECOS.2016.25-2.11},
  copyright = {Derechos de autor},
  language = {es},
  keywords = {reproducibilidad,revisado},
  note = {\textbf{Brief description/principal idea}
\par
Explicaci\'on sobre: en qu\'e consiste la reproducibilidad, por qu\'e es necesaria en ciencia, y c\'omo podemos hacer ciencia reproducible. Se presentan una serie de recomendaciones y herramientas para el manejo y an\'alisis de datos, control de versiones de archivos, organizaci\'on de ficheros y manejo de programas inform\'aticos que nos permiten desarrollar flujos de trabajo reproducibles en el contexto actual de la ecolog\'ia.
\par
\textbf{Cites}
\par
\#\# Cite
\par
La inmensa mayor\'ia de los art\'iculos cient\'ificos no son reproducibles, esto es, resulta muy di- f\'icil o imposible trazar claramente el proceso de obtenci\'on de los resultados y volver a obtenerlos (reproducirlos) \textexclamdown incluso trat\'an- dose del mismo equipo de autores!
\par
\#\# Cite
\par
Pero la reproducibilidad no deber\'ia ser vista como una obligaci\'on impuesta externamente, sino como una oportunidad de mejoar nuestra manera de hacer ciencia y aumentar la contribuci\'on de nuestros trabajos al avance cient\'ifico general. Hacer ciencia reproducible trae consigo m\'ultiples ventajas para el investigador (ver Tabla 1), a pesar del esfuerzo inicial que siempre conlleva aprender nuevas t\'ecnicas de trabajo.
\par
\#\# CIte
\par
Para empezar, tener flujos de trabajo reproducibles evita mu-
\par
chos de los problemas planteados al comienzo de este art\'iculo. Por ejemplo, tras corregir un error en los datos o introducir nue- vas observaciones podemos volver a generar -sin ning\'un es- fuerzo extra- todas las tablas, figuras y resultados de un trabajo. Esto no s\'olo ahorra tiempo sino que disminuye dr\'asticamente los errores en el manuscrito final. Igualmente, la existencia de un c\'o- digo que documenta fielmente el proceso de an\'alisis facilita tanto la escritura del manuscrito como su interpretaci\'on por coautores, revisores y lectores finales (Markowetz 2015). Adem\'as, dicha transparencia le da un sello de calidad al trabajo y facilita su acep- taci\'on, incrementando su impacto posterior en t\'erminos de citas y reconocimiento (Piwowar et al. 2007; Vandewalle 2012).
\par
\#\# Cite
\par
Para que un estudio sea reproducible, todo el an\'alisis debe realizarse mediante `scripts' de c\'odigo, desde la manipulaci\'on de datos hasta la generaci\'on de tablas y figuras. Eso significa que de- bemos evitar hacer ning\'un cambio directamente sobre los datos originales (e.g. en una hoja de c\'alculo como Microsoft Excel): los datos originales son intocables, y cualquier modificaci\'on posterior debe realizarse mediante c\'odigo de manera que quede un registro de todos los cambios realizados.}
}

@misc{rowe_preview_2018,
  title = {Preview My New Book: {{Introduction}} to {{Reproducible Science}} in {{R}} | {{R}}-Bloggers},
  shorttitle = {Preview My New Book},
  author = {Rowe, Brian Lee Yung},
  year = {2018},
  month = nov,
  abstract = {I'm pleased to share Part I of my new book ``Introduction to Reproducible Science in R``. The purpose of this \ldots Continue reading \textrightarrow},
  language = {en-US}
}

@article{sadaba_acceso_2014,
  title = {{{EL ACCESO ABIERTO EN CIENCIAS SOCIALES}}: {{NOTAS SOCIOL\'OGICAS SOBRE PUBLICACIONES}}, {{COMUNIDADES Y CAMPOS}}},
  author = {S{\'a}daba, Igor},
  year = {2014},
  volume = {17},
  pages = {93--113},
  issn = {1139-3327},
  abstract = {En el presente art\'iculo proponemos evitar las caracterizaciones abstractas y pol\'iticas del Open Access para pasar a evaluar emp\'iricamente su funcionamiento. Solo apart\'andonos de los manifiestos program\'aticos y los listados de beneficios te\'oricos de dichas pr\'acticas podremos valorar en su justa medida las resistencias existentes y aprovechar sus potencialidades reales. En concreto, se propone estudiar el Open Access en las Ciencias Sociales (en comparaci\'on con las Ciencias Naturales) y entender que todav\'ia estamos ante un proceso desigual de difusi\'on del conocimiento acad\'emico debido, en parte, a dos nociones sociol\'ogicas centrales (de dos autores tambi\'en centrales en las propias Ciencias Sociales): i) la arquitectura diferencial de sus ``comunidades cient\'ificas'' (Merton) y ii) las diferentes reglas de ``campo acad\'emico'' (Bourdieu) configuradas a partir del dominio de los \'indices de impacto en las ciencias contempor\'aneas.}
}

@article{schnell_reproducible_2018,
  title = {``{{Reproducible}}'' {{Research}} in {{Mathematical Sciences Requires Changes}} in Our {{Peer Review Culture}} and {{Modernization}} of Our {{Current Publication Approach}}},
  author = {Schnell, Santiago},
  year = {2018},
  month = dec,
  journal = {Bull Math Biol},
  volume = {80},
  number = {12},
  pages = {3095--3105},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0500-9},
  abstract = {The nature of scientific research in mathematical and computational biology allows editors and reviewers to evaluate the findings of a scientific paper. Replication of a research study should be the minimum standard for judging its scientific claims and considering it for publication. This requires changes in the current peer review practice and a strict adoption of a replication policy similar to those adopted in experimental fields such as organic synthesis. In the future, the culture of replication can be easily adopted by publishing papers through dynamic computational notebooks combining formatted text, equations, computer algebra and computer code.},
  language = {en}
}

@misc{serevicionacionaldelpatrimoniocultural_tratados_,
  title = {Tratados {{Inernacionales}}},
  author = {, Serevicio Nacional del Patrimonio Cultural},
  journal = {Departamento de Derechos Intelectuales}
}

@article{serra-garcia_nonreplicable_2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  journal = {Sci Adv},
  volume = {7},
  number = {21},
  pages = {eabd1705},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  abstract = {Published papers that fail to replicate are cited more than those that replicate, even after the failure is published., We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility.},
  pmcid = {PMC8139580},
  pmid = {34020944}
}

@article{simmons_falsepositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychol Sci},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  language = {en},
  keywords = {practices}
}

@article{simonsohn_pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534--547},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves\textemdash containing more low (.01s) than high (.04s) significant p values\textemdash only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {herramienta,Hypothesis Testing,practices,Psychology,Scientific Communication,Statistics},
  note = {\textbf{Apuntes de BITSS}
\par
\section{P-Curve: A tool for detecting publication bias}

\par
How can we tell when publication bias has led to data mining? In this video, we'll show you what t he distribution of p-values should look like when (1) there are no observable effects of a treatment, (2) there are observable effects, and (3) data mining is likely to have occurred. We also discuss a 2014 article authored by Uri Simonsohn, Leif Nelson, and Joseph Simmons (the same authors of ``False-Positive Psychology'') in the Journal of Experimental Psychology that demonstrated widespread data mining in that body of literature.
\par
In this article, authors Joseph Simmons, Leif Nelson, and Uri Simonsohn propose a way to distinguish between truly significant findings and false positives resulting from selective reporting and specification searching, or~\emph{p-hacking}.
\par
P values indicate ``how likely one is to observe an outcome at least as extreme as the one observed if the studied effect were nonexistent.'' As a reminder, most academic journals will only publish studies with p values less than 0.05, the most common threshold for statistical significance.
\par
Some researchers use~\textbf{p-hacking}~to ``find statistically significant support for nonexistent effects,'' allowing them to ``get most studies to reveal significant relationships between truly unrelated variables.''
\par
The~\textbf{p-curve}~can be used to detect p-hacking. The authors define this curve as ``the distribution of statistically significant p values for a set of independent findings. Its shape is a diagnostic of the evidential value of that set of findings.''
\par
In order for p-curve inferences to be credible, the p values selected must be:
\par
\begin{enumerate}

\item associated with the hypothesis of interest,

\item statistically independent from other selected p values, and

\item distributed uniformly under the null.

\end{enumerate}

\par
It is also important to clarify that the p-curve assesses only reported data and not the theories that they are testing. Similarly, it's important to keep in mind that if a set of values is found to have evidential value, it doesn't automatically imply internal or external validity.
\par
Using the p-curve to detect p-hacking is fairly straightforward. If the curve is right-skewed as in the chart to the right in the figure below, there are more low (0.01s) than high (0.04s) significant p values, suggesting truly significant p values. When non-existent effects are studied (i.e., a study's null hypothesis is true), all p values are equally likely to be observed, thus producing a uniform curve or a straight line. In the figure below, each chart incorporates a uniform curve that is dotted and red for comparison. Curves that are left-skewed however, as in the chart to the left in the figure below, indicate more high p values than low ones; p-hacking has likely occurred.
\par
\href{https://i2.wp.com/ugc.futurelearn.com/uploads/assets/1b/62/1b62cfd2-c73c-46a8-bbac-088fadd9a989.jpg?ssl=1}{}\textbf{``P-curves for Journal of Personality and Social Psychology''}~\href{https://ugc.futurelearn.com/uploads/assets/1b/62/1b62cfd2-c73c-46a8-bbac-088fadd9a989.jpg}{(Click to expand)}
\par
The above figure displays the results of the authors' demonstration of the p-curve through the analysis of two sets of findings taken from the Journal of Personality and Social Psychology (JPSP). They hypothesized that one set was p-hacked, while the other was not. In the set in which they suspected p-hacking, they realized that the authors of the publication reported results only with a covariate. While there is nothing wrong with including covariates in study's design, many researchers will include one only after their initial analysis (without the covariate) was found to be non-significant.
\par
Simmons, Nelson, and Simonsohn provide guidelines to follow when selecting studies to analyze with the p-curve:
\par
\begin{enumerate}

\item \textbf{Create a selection rule.}~Authors should decide in advance which studies to use.

\item \textbf{Disclose the selection rule.}

\item \textbf{Maintain robustness to resolutions of ambiguity}~If it is unclear whether or not a study should be included, authors should report results both with and without that study. This allows readers to see the extent of the influence of these ambiguous cases.

\item \textbf{Replicate single-article p-curves.}~Because of the risk of cherry-picking single articles, the authors suggest a direct replication of at least one of the studies in the article to improve the credibility of the p-curve.

\end{enumerate}

\par
In addition to these guidelines, Simmons, Nelson, and Simonsohn also provide five steps to ensure that the selected p-values meet the three selection criteria we mentioned earlier:
\par
\begin{enumerate}

\item Identify researchers' stated hypothesis and study design.

\item Identify the statistical result testing the stated hypothesis.

\item Report the statistical results of interest.

\item Recompute precise p-values based on reported test statistics. \textendash{} This has been made easy through an online app, which you can find at~\href{http://p-curve.com/}{http://p-curve.com/}.

\item Report robustness results of the p-values to your selection rules.

\end{enumerate}

\par
As with anything, the p-curve is not one hundred percent accurate one hundred percent of the time. The validity of the judgments made from a p-curve may depend on ``the number of studies being p-curved, their statistical power, and the intensity of p-hacking''. There isn't much concern over cherry-picking p-curves to ensure the result of a lack of evidential value. However, such a practice can be prevented simply with the disclosure of selections, ambiguity, sample size, and other study details.
\par
Additionally, there are a few limitations with the p-curve. First, it ``does not yet technically apply to studies analyzed using discrete test statistics'' and is ``less likely to conclude data have evidential value when a covariate correlates with the independent variable of interest.'' It also has a hard time detecting confounding variables; if there is a real effect, but also mild p-hacking, it usually won't detect the latter.
\par
Simmons, Nelson, and Simonsohn conclude that, with the examination of a distribution of p-values, one will be able to identify whether selective reporting was used or not.~\textbf{What do you think about the p-curve? Would you use this tool?}}
}

@misc{socha_cuanto_2018,
  title = {{\textquestiondown Cu\'anto cobran los principales editores comerciales por tener un art\'iculo en acceso abierto?}},
  author = {Socha, Beata},
  year = {2018},
  journal = {Universo Abierto},
  abstract = {Los cuatro grandes actores de la industria editorial, Elsevier, Springer, Wiley y Taylor \& Francis, han adoptado el acceso abierto (Open Access, OA), a trav\'es de la modalidad ``El autor paga'' aunque en distintos grados. Tambi\'en han empleado estrategias muy diferentes en cuanto a cu\'anto cobran a sus autores. Para cualquier autor que desee publicar su investigaci\'on en Acceso Abierto en alguna de estas revistas probablemente necesita conocer lo que el mercado editorial tiene para ofrecer y qu\'e gama de precios existe. Los datos primarios proceden de las listas de precios oficiales de los editores disponibles en sus sitios web.},
  language = {Traducido}
}

@article{sociedadmaxplanck_declaracion_2003,
  title = {La {{Declaraci\'on}} de {{Berl\'in}} Sobre Acceso Abierto.},
  author = {, Sociedad Max Planck},
  year = {2003},
  volume = {1},
  number = {2},
  pages = {152--154}
}

@techreport{soderberg_initial_2020,
  type = {Preprint},
  title = {Initial {{Evidence}} of {{Research Quality}} of {{Registered Reports Compared}} to the {{Traditional Publishing Model}}},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia G. and Singleton Thorn, Felix and Vazire, Simine and Esterling, Kevin and Nosek, Brian A.},
  year = {2020},
  month = nov,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/7x9vy},
  abstract = {In Registered Reports (RRs), initial peer review and in-principle acceptance occurs before knowing the research outcomes. This combats publication bias and distinguishes planned and unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs outperformed comparison papers on all 19 criteria (mean difference=0.46; Scale range -4 to +4) with effects ranging from little improvement in novelty (0.13, 95\% credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14, 0.58]) to larger improvements in rigor of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
  keywords = {reports,transparency}
}

@article{stamkou_cultural_2019,
  title = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}: {{Effects}} on {{Power Perception}}, {{Moral Emotions}}, and {{Leader Support}}},
  shorttitle = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}},
  author = {Stamkou, Eftychia and {van Kleef}, Gerben A. and Homan, Astrid C. and Gelfand, Michele J. and {van de Vijver}, Fons J. R. and {van Egmond}, Marieke C. and Boer, Diana and Phiri, Natasha and Ayub, Nailah and Kinias, Zoe and Cantarero, Katarzyna and Efrat Treister, Dorit and Figueiredo, Ana and Hashimoto, Hirofumi and Hofmann, Eva B. and Lima, Renata P. and Lee, I-Ching},
  year = {2019},
  month = jun,
  journal = {Pers Soc Psychol Bull},
  volume = {45},
  number = {6},
  pages = {947--964},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-1672},
  doi = {10.1177/0146167218802832},
  abstract = {Responses to norm violators are poorly understood. On one hand, norm violators are perceived as powerful, which may help them to get ahead. On the other hand, norm violators evoke moral outrage, which may frustrate their upward social mobility. We addressed this paradox by considering the role of culture. Collectivistic cultures value group harmony and tight cultures value social order. We therefore hypothesized that collectivism and tightness moderate reactions to norm violators. We presented 2,369 participants in 19 countries with a norm violation or a norm adherence scenario. In individualistic cultures, norm violators were considered more powerful than norm abiders and evoked less moral outrage, whereas in collectivistic cultures, norm violators were considered less powerful and evoked more moral outrage. Moreover, respondents in tighter cultures expressed a stronger preference for norm followers as leaders. Cultural values thus influence responses to norm violators, which may have downstream consequences for violators' hierarchical positions.},
  language = {en},
  keywords = {collectivism,leadership,moral emotions,norm violation,tightness}
}

@article{steneck_fostering_2006,
  title = {Fostering Integrity in Research: {{Definitions}}, Current Knowledge, and Future Directions},
  shorttitle = {Fostering Integrity in Research},
  author = {Steneck, Nicholas H.},
  year = {2006},
  month = mar,
  journal = {SCI ENG ETHICS},
  volume = {12},
  number = {1},
  pages = {53--74},
  issn = {1471-5546},
  doi = {10.1007/PL00022268},
  abstract = {Over the last 25 years, a small but growing body of research on research behavior has slowly provided a more complete and critical understanding of research practices, particularly in the biomedical and behavioral sciences. The results of this research suggest that some earlier assumptions about irresponsible conduct are not reliable, leading to the conclusion that there is a need to change the way we think about and regulate research behavior. This paper begins with suggestions for more precise definitions of the terms ``responsible conduct of research,'' ``research ethics,'' and ``research integrity.'' It then summarizes the findings presented in some of the more important studies of research behavior, looking first at levels of occurrence and then impact. Based on this summary, the paper concludes with general observations about priorities and recommendations for steps to improve the effectiveness of efforts to respond to misconduct and foster higher standards for integrity in research.},
  language = {en},
  keywords = {practices}
}

@misc{stewart_preregistration_2020,
  title = {Pre-Registration and {{Registered Reports}}: A {{Primer}} from {{UKRN}}},
  shorttitle = {Pre-Registration and {{Registered Reports}}},
  author = {Stewart, Suzanne and Rinke, Eike Mark and McGarrigle, Ronan and Lynott, Dermot and Lunny, Carole and Lautarescu, Alexandra and Galizzi, Matteo M. and Farran, Emily K. and Crook, Zander},
  year = {2020},
  month = oct,
  institution = {{OSF Preprints}},
  doi = {10.31219/osf.io/8v2n7},
  abstract = {Help reduce questionable research practices, and prevent selective reporting.},
  keywords = {Architecture,Arts and Humanities,Business,Education,Engineering,Law,Life Sciences,Medicine and Health Sciences,Physical Sciences and Mathematics,pre-analysis plan,pre-registration,preregistration,primer,primers,prospective registration,registered reports,registration,reproducibility,Social and Behavioral Sciences,UK Reproducibility Network,UKRN}
}

@article{stodden_trust_2011,
  title = {Trust {{Your Science}}? {{Open Your Data}} and {{Code}}},
  shorttitle = {Trust {{Your Science}}?},
  author = {Stodden, Victoria C.},
  year = {2011},
  volume = {409},
  pages = {21--22},
  doi = {10.7916/D8CJ8Q0P},
  abstract = {This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.},
  language = {en},
  keywords = {forrt},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways}{}}

\begin{itemize}

\item Computational results suffer from problems of errors in final published conclusions.

\item In order to allow independent replication and reproducible work, release the scripts and data files, and if the researcher~uses MATLAB for graphs etc, please provide the graphical user interface.

\item The standards for code quality~are~more precise definitions of verification, validation, and error quantification in scientific computing.

\item Research workflow involves changes made to data, including analysis, that affects data interpretation.

\item To conclude, open data is a prerequisite for verifiable research.

\end{itemize}

\subsubsection{Quote\href{https://forrt.org/summaries/open-reproducible/#quote}{}}

\begin{quotation}
``Science has never been about open data per se, but openness is something hard fought and won in the context of reproducibility'' (p. 22).

\end{quotation}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract}{}}

\par
This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.}
}

@book{swan_directrices_2013,
  title = {Directrices Para Pol\'iticasde Desarrollo y Promoci\'on Del Acceso Abierto},
  author = {Swan, Alma},
  year = {2013},
  publisher = {{UNESCO}},
  isbn = {978-959-18-0928- 5}
}

@article{szollosi_preregistration_2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {2},
  pages = {94--95},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.11.009},
  language = {en},
  keywords = {forrt,reports},
  note = {\textbf{FORRT Sum}
\par
\subsubsection{Main Takeaways:\href{https://forrt.org/summaries/open-reproducible/#main-takeaways-136}{}}

\begin{itemize}

\item Pre-registration should be an option to improve research. Pre-registration intends to solve statistical problems and forces people to think more deeply about theories, methods, and analyses. But, needing, rewarding, or promoting it is not worthwhile. Requiring pre-registration could harm the progress in our field.

\item Scientific inference is the process to develop better theories. Statistical models are simplified mathematical abstractions of scientific problems, simplifications to aid scientific inference but to allow abstraction.

\item Diagnosticity of statistical tests depends on how well statistical models map onto theories and improved statistical techniques does little to improve theories when mapping is weak.

\item Models are useful depending on how accurately the theory is matched to the model. Many statistical models (e.g. general linear model) in psychology are poor estimates of the theory.

\item Bad theories can be pre-registered~with~predictions barely better than randomly picking an outcome. Pre-registration does not improve theories but should allow researchers to think more deeply on how to improve theories through better planning, more precise operationalisation of constructs, and clear motivation for statistical planning.

\item We should improve theories when encountering difficulties with pre-registration or when pre-registered predictions are wrong.~There is no problem with post-hoc scientific inference when the theories are strong.

\item Any improvement depends on a good understanding of how to improve a theory, and pre-registration provides no understanding. Pre-registration encourages thinking, but it is unclear whether the thinking is better or worse.

\item Poor operationalisation, imprecise measurement, weak connection between theory and statistical method should take precedence over problems of statistical inference.

\end{itemize}

\subsubsection{Abstract\href{https://forrt.org/summaries/open-reproducible/#abstract-136}{}}

\par
\textbf{Proponents of preregistration argue that, among other benefits, it improves the diagnosticity of statistical tests. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depends entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses).}}
}

@article{taylor_altmetric_2020,
  title = {An Altmetric Attention Advantage for Open Access Books in the Humanities and Social Sciences},
  author = {Taylor, Michael},
  year = {2020},
  journal = {Scientometrics},
  volume = {125},
  pages = {2523--2543},
  doi = {10.1007/s11192-020-03735-8},
  abstract = {The last decade has seen two significant phenomena emerge in research communication: the rise of open access (OA) publishing, and the easy availability of evidence of online sharing in the form of altmetrics. There has been limited examination of the effect of OA on online sharing for journal articles, and little for books. This paper examines the altmetrics of a set of 32,222 books (of which 5\% are OA) and a set of 220,527 chapters (of which 7\% are OA) indexed by the scholarly database Dimensions in the Social Sciences and Humanities. Both OA books and chapters have significantly higher use on social networks, higher coverage in the mass media and blogs, and evidence of higher rates of social impact in policy documents. OA chapters have higher rates of coverage on Wikipedia than their non-OA equivalents, and are more likely to be shared on Mendeley. Even within the Humanities and Social Sciences, disciplinary differences in altmetric activity are evident. The effect is confirmed for chapters, although sampling issues prevent the strong conclusion that OA facilitates extra attention at the whole book level, the apparent OA altmetrics advantage suggests that the move towards OA is increasing social sharing and broader impact.},
  language = {English}
}

@article{tennant_ten_2019,
  title = {Ten {{Hot Topics}} around {{Scholarly Publishing}}},
  author = {Tennant, Jonathan P. and Crane, Harry and Crick, Tom and Davila, Jacinto and Enkhbayar, Asura and Havemann, Johanna and Kramer, Bianca and Martin, Ryan and Masuzzo, Paola and Nobes, Andy and Rice, Curt and {Rivera-L{\'o}pez}, B{\'a}rbara and {Ross-Hellauer}, Tony and Sattler, Susanne and Thacker, Paul D. and Vanholsbeeck, Marc},
  year = {2019},
  month = jun,
  journal = {Publications},
  volume = {7},
  number = {2},
  pages = {34},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7020034},
  abstract = {The changing world of scholarly communication and the emerging new wave of \&lsquo;Open Science\&rsquo; or \&lsquo;Open Research\&rsquo; has brought to light a number of controversial and hotly debated topics. Evidence-based rational debate is regularly drowned out by misinformed or exaggerated rhetoric, which does not benefit the evolving system of scholarly communication. This article aims to provide a baseline evidence framework for ten of the most contested topics, in order to help frame and move forward discussions, practices, and policies. We address issues around preprints and scooping, the practice of copyright transfer, the function of peer review, predatory publishers, and the legitimacy of \&lsquo;global\&rsquo; databases. These arguments and data will be a powerful tool against misinformation across wider academic research, policy and practice, and will inform changes within the rapidly evolving scholarly publishing system.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {copyright,impact factor,open access,open science,peer review,research evaluation,scholarly communication,Scopus,web of science}
}

@article{thibodeaux_production_2016,
  title = {Production as Social Change: {{Policy}} Sociology as a Public Good},
  shorttitle = {Production as Social Change},
  author = {Thibodeaux, Jarrett},
  year = {2016},
  month = may,
  journal = {Sociological Spectrum},
  volume = {36},
  number = {3},
  pages = {183--190},
  publisher = {{Routledge}},
  issn = {0273-2173},
  doi = {10.1080/02732173.2015.1102666},
  abstract = {Burawoy described two ways sociology can aid the public, through: (1) instrumental (policy) sociology and (2) reflexive (public) sociology. This article elaborates the different assumptions of how social change occurs according to policy and public sociology (and how sociology effects social change). Policy sociology assumes social change occurs through the scientific elaboration of the best means to achieve goals. However, policy sociology largely takes the public as an object of power rather than subjects who can utilize scientific knowledge. Public sociology assumes that social change occurs through the exposure of contradictions in goals, which elaborates better goals. However, the elaboration of contradictions assumes that there is a fundamental thesis/antithesis in society. If there are multiple goals/theses, public sociology fails in at least three ways. Policy sociology, when reflexively selecting its public, provides the best way sociology can aid the public.},
  annotation = {\_eprint: https://doi.org/10.1080/02732173.2015.1102666}
}

@inproceedings{unesco_declaracion_1999,
  title = {Declaraci\'on Sobre La Ciencia y El Uso Del Saber Cinet\'ifico},
  booktitle = {Conferencia Mundial Sobre La Ciencia},
  author = {UNESCO},
  year = {1999},
  address = {{Hungry - Budapest}}
}

@misc{universoabierto_rutas_2019,
  title = {Las 5 Rutas Para Llegar al Acceso Abierto: Verde, Dorada, Bronce, H\'ibrida y Diamante},
  author = {Universo Abierto},
  year = {2019},
  journal = {Blog de la biblioteca de Traducci\'on y Documentaci\'on de la Universidad de Salamanca}
}

@article{vantveer_preregistration_2016,
  title = {Pre-Registration in Social Psychology\textemdash{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied\textemdash reviewed and unreviewed pre-registration\textemdash and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  language = {en},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)}
}

@misc{velterop_suscripciones_2018,
  title = {{De suscripciones y Tasas de Procesamiento de Art\'iculos}},
  author = {Velterop, Jan},
  year = {2018},
  journal = {Scielo en Perspectiva},
  language = {es}
}

@article{warren_how_2019,
  title = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}? {{Or}} to {{Get Tenure}}? {{Trends}} over a {{Generation}}},
  shorttitle = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}?},
  author = {Warren, John Robert},
  year = {2019},
  month = feb,
  journal = {Sociological Science},
  volume = {6},
  pages = {172--196},
  issn = {2330-6696},
  doi = {10.15195/v6.a7},
  abstract = {Many sociologists suspect that publication expectations have risen over time\textemdash that how much graduate students have published to get assistant professor jobs and how much assistant professors have published to be promoted have gone up. Using information about faculty in 21 top sociology departments from the American Sociological Association's Guide to Graduate Departments of Sociology, online curricula vitae, and other public records, I provide empirical evidence to support this suspicion. On the day they start their first jobs, new assistant professors in recent years have already published roughly twice as much as their counterparts did in the early 1990s. Trends for promotion to associate professor are not as dramatic but are still remarkable. I evaluate several potential explanations for these trends and conclude that they are driven mainly by changes over time in the fiscal and organizational realities of universities and departments.},
  language = {en-US}
}

@article{weston_recommendations_2019,
  title = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  year = {2019},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {3},
  pages = {214--227},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919848684},
  abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more true than now, when technological advances enable both sharing data across labs and continents and mining large sources of preexisting data. However, secondary data analysis is easily overlooked as a key domain for developing new open-science practices or improving analytic methods for robust data analysis. In this article, we provide researchers with the knowledge necessary to incorporate secondary data analysis into their methodological toolbox. We explain that secondary data analysis can be used for either exploratory or confirmatory work, and can be either correlational or experimental, and we highlight the advantages and disadvantages of this type of research. We describe how transparency-enhancing practices can improve and alter interpretations of results from secondary data analysis and discuss approaches that can be used to improve the robustness of reported results. We close by suggesting ways in which scientific subfields and institutions could address and improve the use of secondary data analysis.},
  language = {en},
  keywords = {bias,file drawer,p-hacking,panel design,preexisting data,preregistration,reproducibility,secondary analysis,transparency}
}

@article{wilson_replication_1973,
  title = {The {{Replication Problem}} in {{Sociology}}: {{A Report}} and a {{Suggestion}}*},
  shorttitle = {The {{Replication Problem}} in {{Sociology}}},
  author = {Wilson, Franklin D. and Smoke, Gale L. and Martin, J. David},
  year = {1973},
  journal = {Sociological Inquiry},
  volume = {43},
  number = {2},
  pages = {141--149},
  issn = {1475-682X},
  doi = {10.1111/j.1475-682X.1973.tb00711.x},
  abstract = {The deleterious effects of joint bias in favor of statistical inference and against replication are becoming well known. The acceptance of numerous Type I errors into the literature is by far the most serious of these. Data on the contents of three major journals support the contention that a joint bias for statistical significance tests, for rejections, and against replication exists in modern sociology. This finding replicates that of Sterling (1959) for psychology. A speculative analysis of the dynamics of publication decisions suggests that a compact format for reporting replications might make their publication more attractive to editors, and thus increase their frequency in the literature. A possible format for briefly reporting replication studies is suggested.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682X.1973.tb00711.x}
}

@article{wingen_no_2020,
  title = {No {{Replication}}, {{No Trust}}? {{How Low Replicability Influences Trust}} in {{Psychology}}},
  shorttitle = {No {{Replication}}, {{No Trust}}?},
  author = {Wingen, Tobias and Berkessel, Jana B. and Englich, Birte},
  year = {2020},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {11},
  number = {4},
  pages = {454--463},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550619877412},
  abstract = {In the current psychological debate, low replicability of psychological findings is a central topic. While the discussion about the replication crisis has a huge impact on psychological research, we know less about how it impacts public trust in psychology. In this article, we examine whether low replicability damages public trust and how this damage can be repaired. Studies 1\textendash 3 provide correlational and experimental evidence that low replicability reduces public trust in psychology. Additionally, Studies 3\textendash 5 evaluate the effectiveness of commonly used trust-repair strategies such as information about increased transparency (Study 3), explanations for low replicability (Study 4), or recovered replicability (Study 5). We found no evidence that these strategies significantly repair trust. However, it remains possible that they have small but potentially meaningful effects, which could be detected with larger samples. Overall, our studies highlight the importance of replicability for public trust in psychology.},
  language = {en},
  keywords = {crisis,open science,public trust,replicability,replication crisis}
}

@misc{wipo_frequently_s/f,
  title = {Frequently {{Asked Questions}}: {{IP Policies}} for {{Universities}} and {{Research Institutions}}},
  author = {WIPO},
  year = {s/f},
  journal = {World Intelectual Property Organization}
}

@book{wipo_what_2020,
  title = {What Is Intellectual Property?},
  author = {WIPO},
  year = {2020}
}

@article{zenk-moltgen_factors_2018,
  title = {Factors Influencing the Data Sharing Behavior of Researchers in Sociology and Political Science},
  author = {{Zenk-M{\"o}ltgen}, Wolfgang and Akdeniz, Esra and Katsanidou, Alexia and Na{\ss}hoven, Verena and Balaban, Ebru},
  year = {2018},
  month = jan,
  journal = {Journal of Documentation},
  volume = {74},
  number = {5},
  pages = {1053--1073},
  publisher = {{Emerald Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/JD-09-2017-0126},
  abstract = {Purpose Open data and data sharing should improve transparency of research. The purpose of this paper is to investigate how different institutional and individual factors affect the data sharing behavior of authors of research articles in sociology and political science. Design/methodology/approach Desktop research analyzed attributes of sociology and political science journals (n=262) from their websites. A second data set of articles (n=1,011; published 2012-2014) was derived from ten of the main journals (five from each discipline) and stated data sharing was examined. A survey of the authors used the Theory of Planned Behavior to examine motivations, behavioral control, and perceived norms for sharing data. Statistical tests (Spearman's {$\rho$}, {$\chi$}2) examined correlations and associations. Findings Although many journals have a data policy for their authors (78 percent in sociology, 44 percent in political science), only around half of the empirical articles stated that the data were available, and for only 37 percent of the articles could the data be accessed. Journals with higher impact factors, those with a stated data policy, and younger journals were more likely to offer data availability. Of the authors surveyed, 446 responded (44 percent). Statistical analysis indicated that authors' attitudes, reported past behavior, social norms, and perceived behavioral control affected their intentions to share data. Research limitations/implications Less than 50 percent of the authors contacted provided responses to the survey. Results indicate that data sharing would improve if journals had explicit data sharing policies but authors also need support from other institutions (their universities, funding councils, and professional associations) to improve data management skills and infrastructures. Originality/value This paper builds on previous similar research in sociology and political science and explains some of the barriers to data sharing in social sciences by combining journal policies, published articles, and authors' responses to a survey.},
  keywords = {Data availability,Data policy,Data sharing,Political science,practices,Replication,Research data management,Research transparency,Sociology,Theory of Planned Behaviour}
}


