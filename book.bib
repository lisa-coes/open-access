
@misc{_figueiredo_2021,
  title = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}: {{Collective Memories}} and {{Present}}-{{Day Intergroup Relations}}: {{Introduction}} to the {{Special Thematic Section}}},
  shorttitle = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}},
  year = {2021},
  month = jun,
  doi = {10.5964/jspp.v5i2.895},
  howpublished = {https://jspp.psychopen.eu/index.php/jspp/article/download/4995/4995.html?inline=1}
}

@misc{_impact_2021,
  title = {The Impact of a Multicultural Exchange between Indigenous and Non-Indigenous History Teachers for Students' Attitudes: Preliminary Evidence from a Pilot Study in {{Chile}}: {{Multicultural Education Review}}: {{Vol}} 12, {{No}} 3},
  year = {2021},
  month = jun,
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/2005615X.2020.1808927}
}

@misc{_increasing_,
  title = {Increasing the {{Credibility}} of {{Political Science Research}}: {{A Proposal}} for {{Journal Reforms}}-{{Web}} of {{Science Core Collection}}},
  howpublished = {https://www-webofscience-com.uchile.idm.oclc.org/wos/woscc/full-record/WOS:000359291900014}
}

@misc{_retraction_,
  title = {Retraction {{Watch}}},
  journal = {Retraction Watch},
  abstract = {Tracking retractions as a window into the scientific process},
  howpublished = {https://retractionwatch.com/},
  language = {en-US}
}

@book{abrilruiz_manzanas_2019,
  title = {{Manzanas podridas: Malas pr\'acticas de investigaci\'on y ciencia descuidada}},
  shorttitle = {{Manzanas podridas}},
  author = {Abril Ruiz, Angel},
  year = {2019},
  isbn = {978-1-07-075536-6},
  language = {Spanish},
  annotation = {OCLC: 1120499121}
}

@article{aczel_consensusbased_2020,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and {de la Guardia}, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {1},
  pages = {4--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  language = {en},
  keywords = {forrt,herramienta}
}

@misc{agencianacionaldeinvestigacionydesarrollo_consulta_2020,
  title = {Consulta {{P\'ublica}}: {{Pol\'itica Acceso Abierto}} a {{Informaci\'on Cient\'ifica}}},
  author = {{Agencia Nacional de Investigaci{\'o}n y Desarrollo}, (ANID)},
  year = {2020}
}

@article{allen_open_2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  language = {en},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data}
}

@article{allison_reproducibility_2018,
  title = {Reproducibility of Research: {{Issues}} and Proposed Remedies},
  shorttitle = {Reproducibility of Research},
  author = {Allison, David B. and Shiffrin, Richard M. and Stodden, Victoria},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2561--2562}
}

@book{alperin_indicadores_2014,
  title = {{Indicadores de acceso abierto y comunicaciones acad\'emicas en Am\'erica Latina}},
  shorttitle = {{Indicadores de AA}},
  author = {Alperin, Juan Pablo and Babini, Dominique and Fischman, Gustavo},
  year = {2014},
  edition = {Juan Pablo Alperin},
  volume = {1},
  publisher = {{Consejo Latinoamericano de Ciencias Sociales, CLACSO}},
  address = {{Buenos Aires, Argentina}},
  abstract = {El mundo hoy en d\'ia cuenta con frases como ``muerte de la distancia'' lo que sugiere que la distancia ya no es un factor limitante en la capacidad de las personas para interactuar y comunicarse. Otro aforismo es que el mundo est\'a ``aplanado'' en t\'erminos de oportunidades, que son facilitadas por el avance de las Tecnolog\'ias de la Comunicaci\'on y de la Informaci\'on (TIC) que han permitido la convergencia de los consorcios y recursos de conocimiento de todo el mundo. A medida que las sociedades se van transformando, los paisajes del conocimiento y su interacci\'on dentro y entre las sociedades tambi\'en est\'an cambiando.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  isbn = {978-987-722-042-1},
  language = {es}
}

@article{an_crisis_2018,
  title = {The {{Crisis}} of {{Reproducibility}}, the {{Denominator Problem}} and the {{Scientific Role}} of {{Multi}}-Scale {{Modeling}}},
  author = {An, Gary},
  year = {2018},
  month = dec,
  journal = {Bulletin of Mathematical Biology},
  volume = {80},
  number = {12},
  pages = {3071--3080},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0497-0},
  abstract = {The ``Crisis of Reproducibility'' has received considerable attention both within the scientific community and without. While factors associated with scientific culture and practical practice are most often invoked, I propose that the Crisis of Reproducibility is ultimately a failure of generalization with a fundamental scientific basis in the methods used for biomedical research. The Denominator Problem describes how limitations intrinsic to the two primary approaches of biomedical research, clinical studies and preclinical experimental biology, lead to an inability to effectively characterize the full extent of biological heterogeneity, which compromises the task of generalizing acquired knowledge. Drawing on the example of the unifying role of theory in the physical sciences, I propose that multi-scale mathematical and dynamic computational models, when mapped to the modular structure of biological systems, can serve a unifying role as formal representations of what is conserved and similar from one biological context to another. This ability to explicitly describe the generation of heterogeneity from similarity addresses the Denominator Problem and provides a scientific response to the Crisis of Reproducibility.},
  language = {en},
  keywords = {crisis}
}

@article{andrea_why_2018,
  title = {Why Science's Crisis Should Not Become a Political Battling Ground},
  author = {Andrea, Saltelli},
  year = {2018},
  month = dec,
  journal = {Futures},
  volume = {104},
  pages = {85--90},
  issn = {0016-3287},
  doi = {10.1016/j.futures.2018.07.006},
  abstract = {A science war is in full swing which has taken science's reproducibility crisis as a battleground. While conservatives and corporate interests use the crisis to weaken regulations, their opponent deny the existence of a science's crisis altogether. Thus, for the conservative National Association of Scholars NAS the crisis is real and due to the progressive assault on higher education with ideologies such as ``neo-Marxism, radical feminism, historicism, post-colonialism, deconstructionism, post-modernism, liberation theology''. In the opposite field, some commentators claim that there is no crisis in science and that saying the opposite is irresponsible. These positions are to be seen in the context of the ongoing battle against regulation, of which the new rules proposed at the US Environmental Protection Agency (EPA) are but the last chapter. In this optic, Naomi Oreskes writes on Nature that what constitutes the crisis is the conservatives' attack on science. This evident right-left divide in the reading of the crisis is unhelpful and dangerous to the survival of science itself. An alternative reading ignored by the contendents would suggest that structural contradictions have emerged in modern science, and that addressing these should be the focus of our attention.},
  language = {en},
  keywords = {crisis,Evidence-based policy,History and philosophy of science,Post-normal science,Science and technology studies,Science’s crisis,Science’s reproducibility,Science’s war,Scientism}
}

@article{angell_publish_1986,
  title = {Publish or {{Perish}}: {{A Proposal}}},
  shorttitle = {Publish or {{Perish}}},
  author = {Angell, Marcia},
  year = {1986},
  month = feb,
  journal = {Annals of Internal Medicine},
  volume = {104},
  number = {2},
  pages = {261--262},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-104-2-261},
  keywords = {institutional}
}

@misc{anid_propuesta_2020,
  title = {Propuesta de {{Pol\'itica}} de Acceso Abierto a La Informaci\'on Cient\'ifica y Adatos de Investigaci\'onfinanciados Con Fondos P\'ublicos de La {{ANID}}},
  author = {ANID},
  year = {2020},
  publisher = {{ANID}}
}

@article{anvari_replicability_2018,
  title = {The Replicability Crisis and Public Trust in Psychological Science},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2018},
  month = sep,
  journal = {Comprehensive Results in Social Psychology},
  volume = {3},
  number = {3},
  pages = {266--286},
  publisher = {{Routledge}},
  issn = {2374-3603},
  doi = {10.1080/23743603.2019.1684822},
  abstract = {Replication failures of past findings in several scientific disciplines, including psychology, medicine, and experimental economics, have created a ``crisis of confidence'' among scientists. Psychological science has been at the forefront of tackling these issues, with discussions about replication failures and scientific self-criticisms of questionable research practices (QRPs) increasingly taking place in public forums. How this replicability crisis impacts the public's trust is a question yet to be answered by research. Whereas some researchers believe that the public's trust will be positively impacted or maintained, others believe trust will be diminished. Because it is our field of expertise, we focus on trust in psychological science. We performed a study testing how public trust in past and future psychological research would be impacted by being informed about (i) replication failures (replications group), (ii) replication failures and criticisms of QRPs (QRPs group), and (iii) replication failures, criticisms of QRPs, and proposed reforms (reforms group). Results from a mostly European sample (N = 1129) showed that, compared to a control group, people in the replications, QRPs, and reforms groups self-reported less trust in past research. Regarding trust in future research, the replications and QRPs groups did not significantly differ from the control group. Surprisingly, the reforms group had less trust in future research than the control group. Nevertheless, people in the replications, QRPs, and reforms groups did not significantly differ from the control group in how much they believed future research in psychological science should be supported by public funding. Potential explanations are discussed.},
  keywords = {crisis of confidence,open science,Replicability crisis,reproducibility crisis,trust in science},
  annotation = {\_eprint: https://doi.org/10.1080/23743603.2019.1684822}
}

@article{armeni_widescale_2021,
  title = {Towards Wide-Scale Adoption of Open Science Practices: {{The}} Role of Open Science Communities},
  shorttitle = {Towards Wide-Scale Adoption of Open Science Practices},
  author = {Armeni, Kristijan and Brinkman, Loek and Carlsson, Rickard and Eerland, Anita and Fijten, Rianne and Fondberg, Robin and Heininga, Vera E and Heunis, Stephan and Koh, Wei Qi and Masselink, Maurits and Moran, Niall and Baoill, Andrew {\'O} and Sarafoglou, Alexandra and Schettino, Antonio and Schwamm, Hardy and Sjoerds, Zsuzsika and Teperek, Marta and {van den Akker}, Olmo R and {van't Veer}, Anna and {Zurita-Milla}, Raul},
  year = {2021},
  month = jul,
  journal = {Science and Public Policy},
  number = {scab039},
  issn = {0302-3427},
  doi = {10.1093/scipol/scab039},
  abstract = {Despite the increasing availability of Open Science (OS) infrastructure and the rise in policies to change behaviour, OS practices are not yet the norm. While pioneering researchers are developing OS practices, the majority sticks to status quo. To transition to common practice, we must engage a critical proportion of the academic community. In this transition, OS Communities (OSCs) play a key role. OSCs are bottom-up learning groups of scholars that discuss OS within and across disciplines. They make OS knowledge more accessible and facilitate communication among scholars and policymakers. Over the past two years, eleven OSCs were founded at several Dutch university cities. In other countries, similar OSCs are starting up. In this article, we discuss the pivotal role OSCs play in the large-scale transition to OS. We emphasize that, despite the grassroot character of OSCs, support from universities is critical for OSCs to be viable, effective, and sustainable.}
}

@inproceedings{babini_universidades_2014,
  title = {{Universidades y acceso abierto: hora de tomar protagonismo}},
  booktitle = {{Foro Revista Iberoamericana de Ciencia, Tecnolog\'ia y Sociedad}},
  author = {Babini, Dominique},
  year = {2014},
  pages = {1--3},
  publisher = {{2015}},
  abstract = {Las universidades est\'an en condiciones tener mayor protagonismo en la construcci\'on de un acceso abierto global cooperativo no comercial, sustentable e inclusivo. Pueden: desarrollar sus propios portales con las revistas que publica cada universidad, crear repositorios digitales institucionales que reflejen la propia producci\'on cient\'ifica y acad\'emica de cada instituci\'on disponible gratis en texto completo, participar activamente en los sistemas nacionales de repositorios de sus pa\'ises, aportar una revisi\'on cr\'itica de las actuales modalidades de evaluaci\'on de la investigaci\'on.},
  language = {es}
}

@article{baker_500_2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Nature Publishing Group},
  language = {en},
  keywords = {crisis}
}

@misc{bakker_ensuring_2018,
  title = {Ensuring the Quality and Specificity of Preregistrations},
  author = {Bakker, Marjan and Veldkamp, Coosje Lisabet Sterre and van Assen, Marcel A. L. M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David Thomas and Wicherts, Jelte},
  year = {2018},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdgyh},
  abstract = {Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of `researcher degrees of freedom' aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration (now called ``OSF Preregistration'', http://osf.io/prereg/). The Prereg Challenge format was a structured workflow with detailed instructions, and an independent review to confirm completeness; the ``Standard'' format was unstructured with minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that the structured format restricted the opportunistic use of researcher degrees of freedom better (Cliff's Delta = 0.49) than the unstructured format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
  keywords = {Meta-science,preregistration,Quantitative Methods,Questionable research practices,researcher degrees of freedom,Social and Behavioral Sciences,Statistical Methods}
}

@article{banzato_soberania_2019,
  title = {{Soberan\'ia del conocimiento para superar inequidades: pol\'iticas de Acceso Abierto para revistas cient\'ificas en Am\'erica Latina}},
  author = {Banzato, Guillermo},
  year = {2019},
  journal = {Mecila Working Paper Series},
  volume = {18},
  pages = {1--18},
  abstract = {Desde el comienzo de la era digital, determinadas pol\'iticas de gesti\'on de la ciencia  han incrementado las inequidades en las condiciones de producci\'on del conocimiento  y en las posibilidades de di\'alogo entre los colectivos de investigadores. A fines del  siglo XX y principios del XXI se inici\'o una reacci\'on en las m\'as prestigiosas bibliotecas  y comunidades cient\'ificas de Am\'erica del Norte y Europa Occidental, y Am\'erica Latina  comenz\'o  el  desarrollo  de  sistemas  de  visibilidad  propios,  al  tiempo  que  sucesivas   declaraciones  fueron  definiendo  al  Acceso  Abierto  como  estrategia  para  superar  tales inequidades. En esta direcci\'on, se han desarrollado revistas en Acceso Abierto  cuya  sustentabilidad  est\'a  siendo  puesta  a  prueba.  Este  trabajo  presenta  un  breve   estado de situaci\'on actualizado sobre algunos problemas que enfrentan los autores,  evaluadores y editores latinoamericanos en la gesti\'on y publicaci\'on de los resultados  de las investigaciones. Asimismo, en \'el se argumenta en pro del Acceso Abierto como  herramienta primordial para garantizar la soberan\'ia del conocimiento en el Sur Global,  y  se  sostiene  que  la  propuesta  colaborativa  para  la  construcci\'on  conjunta  de  un   sistema sustentable de edici\'on cient\'ifica en Acceso Abierto puede ayudar a superar  las inequidades en la producci\'on y difusi\'on del conocimiento latinoamericano},
  language = {es}
}

@article{barba_terminologies_2018,
  title = {Terminologies for {{Reproducible Research}}},
  author = {Barba, Lorena A.},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.03311 [cs]},
  eprint = {1802.03311},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries}
}

@article{becerrilgarcia_end_2019,
  title = {The {{End}} of a {{Centralized Open Access Project}} and the {{Beginning}} of a {{Community}}-{{Based Sustainable Infrastructure}} for {{Latin America}}},
  author = {Becerril Garc{\'i}a, Arianna and Aguado L{\'o}pez, Eduardo},
  year = {2019},
  journal = {OpenEdition Press},
  pages = {41--55},
  doi = {10.4000/books.oep. 9003.},
  abstract = {The Latin American region has an ecosystem where the nature of publication is conceived as the act of making public, of sharing, not as the publishing industry. International, national and institutional contexts have led to redefine a project\textemdash Redalyc.org\textemdash that began in 2003 and that has already fulfilled its original mission: give visibility to knowledge coming from Latin America and promote qualitative scientific journals. Nevertheless, it has to be transformed from a Latin American platform based in Mexico into a community- based regional infrastructure that continues assessing journals' quality and providing access to full-text, thus allowing visibility for journals and free access to knowledge. It is a framework that generates technology in favor of the empowerment and professionalization of journal editors, making sustainable the editorial task in open access so that Redalyc may sustain itself collectively. This work describes Redalyc's first model, presents the problematic in process and the new business model Redalyc is designing and adopting to operate.}
}

@article{beigel_relaciones_2018,
  title = {{Las relaciones de poder en la ciencia mundial}},
  shorttitle = {{NUSO}},
  author = {Beigel, Fernanda},
  year = {2018},
  journal = {Nueva Sociedad},
  volume = {274},
  pages = {13--28},
  issn = {0251-3552},
  abstract = {Los rankings universitarios se crearon principalmente para intervenir en los flujos internacionales de estudiantes, pero se convirtieron progresivamente en una fuente directa para reforzar el prestigio de un peque\~no grupo de universidades, de sus principales revistas y editoriales oligop\'olicas. Su aplicaci\'on tiende a volver cada vez m\'as perif\'erica a la ciencia desarrollada en los espacios alejados del circuito mainstream o de corriente principal. Por eso es necesario crear nuevas herramientas de medici\'onde la producci\'on cient\'ifica de la periferia que contemplen las interacciones de sus universidades en sus distintas direcciones, y no solo con los circuitos dominantes.},
  language = {es}
}

@article{benjamin-chung_internal_2020,
  title = {Internal Replication of Computational Workflows in Scientific Research},
  author = {{Benjamin-Chung}, Jade and Colford, Jr., John M. and Mertens, Andrew and Hubbard, Alan E. and Arnold, Benjamin F.},
  year = {2020},
  month = jun,
  journal = {Gates Open Research},
  volume = {4},
  pages = {17},
  issn = {2572-4754},
  doi = {10.12688/gatesopenres.13108.2},
  abstract = {Failures to reproduce research findings across scientific disciplines from psychology to physics have garnered increasing attention in recent years. External replication of published findings by outside investigators has emerged as a method to detect errors and bias in the published literature. However, some studies influence policy and practice before external replication efforts can confirm or challenge the original contributions. Uncovering and resolving errors before publication would increase the efficiency of the scientific process by increasing the accuracy of published evidence. Here we summarize the rationale and best practices for internal replication, a process in which multiple independent data analysts replicate an analysis and correct errors prior to publication. We explain how internal replication should reduce errors and bias that arise during data analyses and argue that it will be most effective when coupled with pre-specified hypotheses and analysis plans and performed with data analysts masked to experimental group assignments. By improving the reproducibility of published evidence, internal replication should contribute to more rapid scientific advances.},
  language = {en}
}

@article{benning_registration_2019,
  title = {The {{Registration Continuum}} in {{Clinical Science}}: {{A Guide Toward Transparent Practices}}},
  shorttitle = {The {{Registration Continuum}} in {{Clinical Science}}},
  author = {Benning, Stephen D. and Bachrach, Rachel L. and Smith, Edward A. and Freeman, Andrew J. and Wright, Aidan G. C.},
  year = {2019},
  month = aug,
  journal = {Journal of Abnormal Psychology},
  volume = {128},
  number = {6},
  pages = {528--540},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {0021-843X},
  doi = {10.1037/abn0000451},
  abstract = {Clinical scientists can use a continuum of registration efforts that vary in their disclosure and timing relative to data collection and analysis. Broadly speaking, registration benefits investigators by offering stronger, more powerful tests of theory with particular methods in tandem with better control of long-run false positive error rates. Registration helps clinical researchers in thinking through tensions between bandwidth and fidelity that surround recruiting participants, defining clinical phenotypes, handling comorbidity, treating missing data. and analyzing rich and complex data. In particular. registration helps record and justify the reasons behind specific study design decisions, though it also provides the opportunity to register entire decision trees with specific endpoints. Creating ever more faithful registrations and standard operating procedures may offer alternative methods of judging a clinical investigator's scientific skill and eminence because study registration increases the transparency of clinical researchers' work.},
  language = {English},
  keywords = {coregistration,credibility,disorders,flexibility,framework,postregistration,preregistration,psychopathology,registered-reports,symptoms,transparency},
  annotation = {WOS:000478024300006}
}

@article{bergh_there_2017,
  title = {Is There a Credibility Crisis in Strategic Management Research? {{Evidence}} on the Reproducibility of Study Findings},
  shorttitle = {Is There a Credibility Crisis in Strategic Management Research?},
  author = {Bergh, Donald D and Sharp, Barton M and Aguinis, Herman and Li, Ming},
  year = {2017},
  month = aug,
  journal = {Strategic Organization},
  volume = {15},
  number = {3},
  pages = {423--436},
  publisher = {{SAGE Publications}},
  issn = {1476-1270},
  doi = {10.1177/1476127017701076},
  abstract = {Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70\% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.},
  language = {en},
  keywords = {crisis,knowledge credibility,replication,reproducibility}
}

@article{bergkvist_preregistration_2020,
  title = {Preregistration as a Way to Limit Questionable Research Practice in Advertising Research},
  author = {Bergkvist, Lars},
  year = {2020},
  month = oct,
  journal = {International Journal of Advertising},
  volume = {39},
  number = {7},
  pages = {1172--1180},
  publisher = {{Routledge Journals, Taylor \& Francis Ltd}},
  address = {{Abingdon}},
  issn = {0265-0487},
  doi = {10.1080/02650487.2020.1753441},
  abstract = {This paper discusses two phenomena that threaten the credibility of scientific research and suggests an approach to limiting the extent of their use in advertising research. HARKing (hypothesizing after the results are known) refers to when hypotheses are formulated or modified after the results of a study are known. P-hacking refers to various practices (e.g., adding respondents, introducing control variables) that increase the likelihood of obtaining statistically significant results from a study. Both of these practices increase the risk of false positives (Type I errors) in research results and it is in the interest of the advertising research field that they are limited. Voluntary preregistration, where researchers commit to and register their research design and analytical approach before conducting the study, is put forward as a means to limiting both HARKing and p-hacking.},
  language = {English},
  keywords = {HARKing,journals,methodology,P-hacking,preregistration,publication bias,questionable research practice,replication},
  annotation = {WOS:000559843700001}
}

@article{berlin_declaracion_2003,
  title = {La {{Declaraci\'on}} de {{Berl\'in}} Sobre Acceso Abierto},
  author = {Berl{\'i}n},
  year = {2003},
  series = {Sociedad {{Max Planck}}},
  volume = {1},
  number = {2},
  pages = {152--154}
}

@misc{bethesda_declaracion_2003,
  title = {Declaraci\'on de {{Bethesda}} Sobre Publicaci\'on de Acceso Abierto},
  author = {Bethesda},
  year = {2003}
}

@article{bishop_rein_2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, Dorothy},
  year = {2019},
  month = apr,
  journal = {Nature},
  volume = {568},
  number = {7753},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  language = {en},
  keywords = {forrt}
}

@book{bjork_developing_2014,
  title = {Developing an {{Effective}}  {{Market}} for {{Open Access}}  {{Article Processing Charges}}},
  author = {Bj{\"o}rk, Bo-Christer and Solomon, David},
  year = {2014},
  publisher = {{Weolcome Trust}}
}

@article{bjork_gold_2017,
  title = {Gold, {{Green}} and {{Black Open Access}}},
  author = {Bj{\"o}rk, Bo-Christer},
  year = {2017},
  journal = {Learned Publishing},
  volume = {30},
  number = {2},
  pages = {173--175},
  doi = {10.1002/leap.1096},
  abstract = {Universal open access (OA) to scholarly research publications is deceptively simple as a concept. Any scientific publications, whether found via a Google keyword search, or by trying to access a citation would be just one click away. But the path to get there from the current subscription-dominated journal pub- lishing model has proved to be complex and filled with obstacles. Since the terms gold and green OA were coined almost 15 years ago, much of the debate inside the OA movement has been focused on the relative merits of these two paths (Harnad et al., 2004)},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {English}
}

@misc{boai_diez_2012,
  title = {{Diez a\~nos desde la Budapest Open Access Initiative: hacia lo abierto por defecto}},
  author = {BOAI, Budapest Open Access Initiative},
  year = {2012},
  collaborator = {Melero, Remedios and Babini, Dominique},
  language = {Traducido}
}

@misc{boai_iniciativa_2002,
  title = {{Iniciativa de Budapest para el Acceso Abierto}},
  author = {BOAI, Budapest Open Access Initiative},
  year = {2002},
  language = {Traducido}
}

@article{bohannon_who_2016,
  title = {Who's Downloading Pirated Papers? {{Everyone}}},
  author = {Bohannon, John},
  year = {2016},
  journal = {American Association for the Advancement of Science},
  volume = {352},
  number = {6285},
  pages = {508--512},
  issn = {1095-9203},
  doi = {10.1126/science.352.6285.508},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@article{bowers_how_2016,
  title = {How to Improve Your Relationship with Your Future Self},
  author = {Bowers, Jake and Voors, Maarten},
  year = {2016},
  month = dec,
  journal = {Revista de ciencia pol\'itica (Santiago)},
  volume = {36},
  number = {3},
  pages = {829--848},
  issn = {0718-090X},
  doi = {10.4067/S0718-090X2016000300011}
}

@article{breznau_does_2021,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  month = mar,
  journal = {Societies},
  volume = {11},
  number = {1},
  pages = {9},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/soc11010009},
  abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {crisis of science,Habermas,Merton,open science,p-hacking,publication bias,replication,research ethics,revisado,science community,sociology legitimation,transparency,Weber}
}

@article{breznau_does_2021a,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  volume = {11}
}

@misc{breznau_observing_2021,
  title = {Observing {{Many Researchers Using}} the {{Same Data}} and {{Hypothesis Reveals}} a {{Hidden Universe}} of {{Uncertainty}}},
  author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Adem, Muna and Adriaans, Jule and {Alvarez-Benjumea}, Amalia and Andersen, Henrik Kenneth and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin and Castillo, Juan Carlos and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Am{\'e}lie and Gr{\"o}mping, Max and Gro{\ss}, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and H{\"o}vermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignacz, Zsofia and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Ko{\l}czy{\'n}ska, Marta and Kuk, John Seungmin and Kuni{\ss}en, Katharina and Sinatra, Dafina Kurti and Greinert, Alexander and Lersch, Philipp M. and L{\"o}bel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar Jose and McManus, Patricia and Wagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan J. B. and Moya, Crist{\'o}bal and Neunhoeffer, Marcel and N{\"u}st, Daniel and Nyg{\aa}rd, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Katja M. and Schmidt, Regine and {Schmidt-Catran}, Alexander and Schmiedeberg, Claudia and Schneider, J{\"u}rgen and Schoonvelde, Martijn and {Schulte-Cloos}, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, J{\"u}rgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian B. and Vagni, Giacomo and Assche, Jasper Van and van der Linden, Meta and van der Noll, Jolanda and Hootegem, Arno Van and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and {\.Z}{\'o}{\l}tak, Tomasz and Nguyen, Hung H. V.},
  year = {2021},
  month = mar,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/cd5j9},
  abstract = {How does noise generated by researcher decisions undermine the credibility of science? We test this by observing all decisions made among 73 research teams as they independently conduct studies on the same hypothesis with identical starting data. We find excessive variation of outcomes. When combined, the 107 observed research decisions taken across teams explained at most 2.6\% of the total variance in effect sizes and 10\% of the deviance in subjective conclusions. Expertise, prior beliefs and attitudes of the researchers explain even less. Each model deployed to test the hypothesis was unique, which highlights a vast universe of research design variability that is normally hidden from view and suggests humility when presenting and interpreting scientific findings.},
  keywords = {Analytical Flexibility,Crowdsourced Replication Initiative,Crowdsourcing,Economics,Garden of Forking Paths,Immigration,Many Analysts,Meta-Science,Noise,Other Social and Behavioral Sciences,Political Science,Psychology,Researcher Degrees of Freedom,Researcher Variability,Social and Behavioral Sciences,Social Policy,Sociology}
}

@misc{breznau_open_,
  type = {Billet},
  title = {Open Science in Sociology. {{What}}, Why and Now.},
  author = {Breznau, Nate},
  journal = {Crowdid},
  abstract = {WHAT By now you've heard the term ``open science''. Although it has no global definition, its advocates tend toward certain agreements. Most definitions focus on the practical aspects of accessibility. ``\ldots the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely \ldots{} Continue reading Open science in sociology. What, why and now.},
  language = {en-US}
}

@techreport{brodeur_methods_2018,
  type = {{{IZA Discussion Paper}}},
  title = {Methods {{Matter}}: {{P}}-{{Hacking}} and {{Causal Inference}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2018},
  month = aug,
  number = {11796},
  institution = {{Institute of Labor Economics (IZA)}},
  abstract = {The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25\% of claims of marginally significant results in IV papers are misleading.},
  keywords = {causal inference,p-curves,p-hacking,practices,publication bias,research methods}
}

@article{brodeur_star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = {2016},
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  language = {en},
  keywords = {Market for Economists; Estimation: General,Role of Economics,Role of Economists}
}

@misc{budapestopenaccessinitiative_diez_2012,
  title = {{Diez a\~nos desde la Budapest Open Access Initiative: hacia lo abierto por defecto}},
  author = {Budapest Open Access Initiative},
  year = {12 de Septiembre, 2012},
  journal = {BOAI},
  language = {Traducido}
}

@misc{budapestopenaccessinitiative_iniciativa_2002,
  title = {{Iniciativa de Budapest para el Aceso Abierto}},
  author = {Budapest Open Access Initiative},
  year = {14 de Febrero, 2002},
  journal = {Budapest Open Access Initiative},
  language = {Traducido}
}

@article{burlig_improving_2018,
  title = {Improving Transparency in Observational Social Science Research: {{A}} Pre-Analysis Plan Approach},
  shorttitle = {Improving Transparency in Observational Social Science Research},
  author = {Burlig, Fiona},
  year = {2018},
  month = jul,
  journal = {Economics Letters},
  volume = {168},
  pages = {56--60},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2018.03.036},
  abstract = {Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I highlight three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data.},
  language = {en},
  keywords = {Confidential data,Observational research,Pre-registration,Transparency}
}

@misc{businessmanagementink_protecting_2016,
  title = {Protecting {{Students}}' {{Intellectual Property}}},
  author = {Business \& Management INK},
  year = {2016},
  journal = {social science space}
}

@article{buttner_are_2020,
  title = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine? {{The}} Proportion of Supported Hypotheses Is Implausibly High},
  shorttitle = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine?},
  author = {Buttner, Fionn and Toomey, Elaine and McClean, Shane and Roe, Mark and Delahunt, Eamonn},
  year = {2020},
  month = nov,
  journal = {British Journal of Sports Medicine},
  volume = {54},
  number = {22},
  pages = {1365--1371},
  publisher = {{Bmj Publishing Group}},
  address = {{London}},
  issn = {0306-3674},
  doi = {10.1136/bjsports-2019-101863},
  abstract = {Questionable research practices (QRPs) are intentional and unintentional practices that can occur when designing, conducting, analysing, and reporting research, producing biased study results. Sport and exercise medicine (SEM) research is vulnerable to the same QRPs that pervade the biomedical and psychological sciences, producing false-positive results and inflated effect sizes. Approximately 90\% of biomedical research reports supported study hypotheses, provoking suspicion about the field-wide presence of systematic biases to facilitate study findings that confirm researchers' expectations. In this education review, we introduce three common QRPs (ie, HARKing, P-hacking and Cherry-picking), perform a cross-sectional study to assess the proportion of original SEM research that reports supported study hypotheses, and draw attention to existing solutions and resources to overcome QRPs that manifest in exploratory research. We hypothesised that {$>$}= 85\% of original SEM research studies would report supported study hypotheses. Two independent assessors systematically identified, screened, included, and extracted study data from original research articles published between 1 January 2019 and 31 May 2019 in the British Journal of Sports Medicine, Sports Medicine, the American Journal of Sports Medicine, and the Journal of Orthopaedic \& Sports Physical Therapy. We extracted data relating to whether studies reported that the primary hypothesis was supported or rejected by the results. Study hypotheses, methodologies, and analysis plans were preregistered at the Open Science Framework. One hundred and twenty-nine original research studies reported at least one study hypothesis, of which 106 (82.2\%) reported hypotheses that were supported by study results. Of 106 studies reporting that primary hypotheses were supported by study results, 75 (70.8\%) studies reported that the primary hypothesis was fully supported by study results. The primary study hypothesis was partially supported by study results in 28 (26.4\%) studies. We detail open science practices and resources that aim to safe-guard against QRPs that bely the credibility and replicability of original research findings.},
  language = {English},
  keywords = {education,harking,incentives,methodological,publication decisions,publish,registered-reports,replicability,research,science,sport,statistics,tests,truth},
  annotation = {WOS:000584953300013}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en},
  keywords = {practices}
}

@article{byington_solutions_2017,
  title = {Solutions to the {{Credibility Crisis}} in {{Management Science}}},
  author = {Byington, Eliza and Felps, Will},
  year = {2017},
  month = mar,
  journal = {Academy of Management Learning and Education, The},
  volume = {16},
  pages = {142--162},
  doi = {10.5465/amle.2015.0035},
  abstract = {This article argues much academic misconduct can be explained as the result of social dilemmas occurring at two levels of Management science. First, the career benefits associated with engaging in Noncredible Research Practices (NCRPs) (e.g. data manipulation, fabricating results, data hoarding, undisclosed HARKing) result in many academics choosing self-interest over collective welfare. These perverse incentives derive from journal gatekeepers who are pressed into a similar social dilemma. Namely, an individual journal's status (i.e. its ``impact factor'') is likely to suffer from unilaterally implementing practices that help ensure the credibility of Management science claims (e.g. dedicating journal space to strict replications, crowd-sourcing replications, data submission requirements, in-house analysis checks, registered reports, Open Practice badges). Fortunately, research on social dilemmas and collective action offers solutions. For example, journal editors could pledge to publish a certain number of credibility boosting articles contingent on a proportion of their ``peer'' journals doing the same. Details for successful implementation of conditional pledges, other social dilemma solutions \textendash{} including actions for Management academics who support changes in journal practices (e.g. reviewer boycotts / buycotts), and insights on credibility supportive journal practices from other fields are provided.},
  keywords = {crisis}
}

@article{caldwell_moving_2020,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  shorttitle = {Moving {{Sport}} and {{Exercise Science Forward}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, R{\'e}mi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary and Lohse, Keith R. and Nunan, David and {Consortium for Transparency in Exercise Science (COTES) Collaborators}},
  year = {2020},
  month = mar,
  journal = {Sports Medicine},
  volume = {50},
  number = {3},
  pages = {449--459},
  issn = {1179-2035},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR\$\$\textbackslash chi \$\$iv: https://osf.io/preprints/sportrxiv/fxe7a/.},
  language = {en},
  keywords = {transparency}
}

@article{caldwell_moving_2020a,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  shorttitle = {Moving {{Sport}} and {{Exercise Science Forward}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, Remi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Lohse, Keith and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Nunan, David and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary},
  year = {2020},
  month = mar,
  journal = {Sports Medicine},
  volume = {50},
  number = {3},
  pages = {449--459},
  publisher = {{Adis Int Ltd}},
  address = {{Northcote}},
  issn = {0112-1642},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportRiv: https://osf.io/ prepr ints/sport rxiv/fxe7a/.},
  language = {English},
  keywords = {associations,health,increase,model,power,publication bias,registered-reports,true},
  annotation = {WOS:000511041300002}
}

@article{camerer_evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  language = {en},
  keywords = {crisis}
}

@article{campbell_enhancing_2014,
  title = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings: {{A}} Guide for Relationship Researchers},
  shorttitle = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings},
  author = {Campbell, Lorne and Loving, Timothy J. and Lebel, Etienne P.},
  year = {2014},
  journal = {Personal Relationships},
  volume = {21},
  number = {4},
  pages = {531--545},
  issn = {1475-6811},
  doi = {10.1111/pere.12053},
  abstract = {The purpose of this paper is to extend to the field of relationship science, recent discussions and suggested changes in open research practises. We demonstrate different ways that greater transparency of the research process in our field will accelerate scientific progress by increasing accuracy of reported research findings. Importantly, we make concrete recommendations for how relationship researchers can transition to greater disclosure of research practices in a manner that is sensitive to the unique design features of methodologies employed by relationship scientists. We discuss how to implement these recommendations for four different research designs regularly used in relationship research and practical limitations regarding implementing our recommendations and provide potential solutions to these problems.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/pere.12053}
}

@article{card_role_2011,
  title = {The {{Role}} of {{Theory}} in {{Field Experiments}}},
  author = {Card, David and DellaVigna, Stefano and Malmendier, Ulrike},
  year = {2011},
  month = sep,
  journal = {Journal of Economic Perspectives},
  volume = {25},
  number = {3},
  pages = {39--62},
  issn = {0895-3309},
  doi = {10.1257/jep.25.3.39},
  abstract = {studies that estimate structural parameters in a completely specified model. We also classify laboratory experiments published in these journals over the same period and find that economic theory has played a more central role in the laboratory than in the field. Finally, we discuss in detail three sets of field experiments\textemdash on gift exchange, on charitable giving, and on negative income tax\textemdash that illustrate both the benefits and the potential costs of a tighter link between experimental design and theoretical underpinnings.},
  language = {en},
  keywords = {Field Experiments}
}

@article{carey_fraud_2011,
  title = {Fraud {{Case Seen}} as a {{Red Flag}} for {{Psychology Research}}},
  author = {Carey, Benedict},
  year = {2011},
  month = nov,
  journal = {The New York Times},
  issn = {0362-4331},
  abstract = {A Dutch scholar was found to have falsified findings in dozens of papers, in a field that critics say is vulnerable to such abuses.},
  chapter = {Health},
  language = {en-US},
  keywords = {Falsification of Data,Frauds and Swindling,Psychology and Psychologists,Research,Stapel; Diederik}
}

@article{carrier_facing_2017,
  title = {Facing the {{Credibility Crisis}} of {{Science}}: {{On}} the {{Ambivalent Role}} of {{Pluralism}} in {{Establishing Relevance}} and {{Reliability}}},
  shorttitle = {Facing the {{Credibility Crisis}} of {{Science}}},
  author = {Carrier, Martin},
  year = {2017},
  month = may,
  journal = {Perspectives on Science},
  volume = {25},
  number = {4},
  pages = {439--464},
  issn = {1063-6145},
  doi = {10.1162/POSC_a_00249},
  abstract = {Science at the interface with society is regarded with mistrust among parts of the public. Scientific judgments on matters of practical concern are not infrequently suspected of being incompetent and biased. I discuss two proposals for remedying this deficiency. The first aims at strengthening the independence of science and suggests increasing the distance to political and economic powers. The drawback is that this runs the risk of locking science in an academic ivory tower. The second proposal favors ``counter-politicization'' in that research is strongly focused on projects ``in the public interest,'' that is, on projects whose expected results will benefit all those concerned by these results. The disadvantage is that the future use of research findings cannot be delineated reliably in advance. I argue that the underlying problem is the perceived lack of relevance and reliability and that pluralism is an important step toward its solution. Pluralism serves to stimulate a more inclusive research agenda and strengthens the well-testedness of scientific approaches. However, pluralism also prevents the emergence of clear-cut practical suggestions. Accordingly, pluralism is part of the solution to the credibility crisis of science, but also part of the problem. In order for science to be suitable as a guide for practice, the leeway of scientific options needs to be narrowed \textendash{} in spite of uncertainty in epistemic respect. This reduction can be achieved by appeal to criteria that do not focus on the epistemic credentials of the suggestions but on their appropriateness in practical respect.},
  keywords = {crisis}
}

@article{chambers_registered_2013,
  title = {Registered {{Reports}}: {{A}} New Publishing Initiative at~{{Cortex}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2013},
  month = mar,
  journal = {Cortex},
  volume = {49},
  number = {3},
  pages = {609--610},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.12.016},
  language = {en},
  keywords = {forrt,reports}
}

@misc{chambers_registered_2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  author = {Chambers, Christopher D.},
  year = {2014},
  journal = {Reviewers' Update},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  howpublished = {https://www.elsevier.com/connect/reviewers-update/registered-reports-a-step-change-in-scientific-publishing},
  language = {en},
  keywords = {forrt,reports}
}

@article{chambers_registered_2015,
  title = {Registered Reports: Realigning Incentives in Scientific Publishing},
  shorttitle = {Registered Reports},
  author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
  year = {2015},
  month = may,
  journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
  volume = {66},
  pages = {A1-2},
  issn = {1973-8102},
  doi = {10.1016/j.cortex.2015.03.022},
  language = {eng},
  pmid = {25892410},
  keywords = {Biomedical Research,Editorial Policies,forrt,Humans,Motivation,Peer Review; Research,Publication Bias,Publishing,reports,Reproducibility of Results}
}

@article{chambers_ten_2015,
  title = {Ten {{Reasons Why Journals Must Review Manuscripts Before Results Are Known}}},
  author = {Chambers, Christopher D.},
  year = {2015},
  month = jan,
  journal = {Addiction},
  volume = {110},
  number = {1},
  pages = {10--11},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0965-2140},
  doi = {10.1111/add.12728},
  language = {English},
  keywords = {False positives,incentives,publication bias,questionable research practices,registered reports,registered-reports,reproducibility,study pre-registration,truth},
  annotation = {WOS:000346699700004}
}

@article{chopik_relationship_2020,
  title = {Relationship Science and the Credibility Revolution: {{An}} Introduction to the First Part of the Special Issue},
  shorttitle = {Relationship Science and the Credibility Revolution},
  author = {Chopik, William J. and Chartier, Christopher R. and Campbell, Lorne and Donnellan, M. Brent},
  year = {2020},
  month = mar,
  journal = {Personal Relationships},
  volume = {27},
  number = {1},
  pages = {132--137},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {1350-4126},
  doi = {10.1111/pere.12312},
  abstract = {In the past 10 years, the field of relationship science-like many other fields-has been exposed to dramatic changes in how scientists approach the research process. Relationship science has been at the forefront of many recent changes in the field, whether it be high profile replication attempts or broader discussions about how to increase rigor and reproducibility. A major goal of this special issue was to provide an opportunity for relationship scientists to engage with these issues and reforms. The first four articles in this special issue represent a sampling of different approaches relationship researchers have used to enhance the credibility of their work.},
  language = {English},
  keywords = {credibility revolution,history,incentives,increase,personal relationships,registered reports,registered-reports,replication,special issue,truth},
  annotation = {WOS:000518878700007}
}

@article{christensen_transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = {2018},
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  language = {en},
  keywords = {Market for Economists; Methodological Issues: General; Higher Education,Research Institutions,Role of Economics,Role of Economists}
}

@book{christensen_transparent_2019,
  title = {Transparent and Reproducible Social Science Research: How to Do Open Science},
  shorttitle = {Transparent and Reproducible Social Science Research},
  author = {Christensen, Garret S. and Freese, Jeremy and Miguel, Edward},
  year = {2019},
  publisher = {{University of California Press}},
  address = {{Oakland, California}},
  abstract = {"Social science practitioners have recently witnessed numerous episodes of influential research that fell apart upon close scrutiny. These instances have spurred suspicions that other published results may contain errors or may at least be less robust than they appear. In response, an influential movement has emerged across the social sciences for greater research transparency, openness, and reproducibility. Transparent and Reproducible Social Science Research crystallizes the new insights, practices, and methods of this rising interdisciplinary field"--Provided by publisher},
  isbn = {978-0-520-96923-0},
  lccn = {Q180.55.S7},
  keywords = {Reproducible research,Research,Social sciences,transparency}
}

@article{chubin_open_1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  journal = {Science, Technology, \& Human Values},
  volume = {10},
  number = {2},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  language = {en}
}

@misc{conicyt_chile_2017,
  title = {Chile y {{Argentina}} Son Destacados Como Ejemplos de Pol\'iticas de Acceso Abierto a La Informaci\'on},
  author = {CONICYT},
  year = {9 de Enero, 2017}
}

@misc{creativecommons_sobre_2017,
  title = {Sobre Las Licencias: {{Lo}} Que Nuestras Licencias Hacen},
  author = {Creative Commons},
  year = {7 de Noviembre, 2017}
}

@misc{cruwell_easy_2018,
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2018},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  keywords = {Meta-science,Meta-Science,Open Access,Open Science,other,Psychology,Reproducibility,revisado,Social and Behavioral Sciences,Transparency}
}

@article{dal-re_making_2014,
  title = {Making {{Prospective Registration}} of {{Observational Research}} a {{Reality}}},
  author = {{Dal-R{\'e}}, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
  year = {2014},
  month = feb,
  journal = {Science Translational Medicine},
  volume = {6},
  number = {224},
  pages = {224cm1-224cm1},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.3007513},
  abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data. Key information about human observational studies should be publicly available before the study is initiated. Key information about human observational studies should be publicly available before the study is initiated.},
  chapter = {Commentary},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  language = {en},
  pmid = {24553383},
  keywords = {reports}
}

@article{derond_publish_2005,
  title = {Publish or {{Perish}}: {{Bane}} or {{Boon}} of {{Academic Life}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {De Rond, Mark and Miller, Alan N.},
  year = {2005},
  month = dec,
  journal = {Journal of Management Inquiry},
  volume = {14},
  number = {4},
  pages = {321--329},
  publisher = {{SAGE Publications Inc}},
  issn = {1056-4926},
  doi = {10.1177/1056492605276850},
  abstract = {There are few more familiar aphorisms in the academic community than ``publish or perish.'' Venerated by many and dreaded by more, this phenomenon is the subject of the authors' essay. Here they consider the publish or perish principle that has come to characterize life at many business schools. They explain when and why it began and suggest reasons for its persistence. This exercise elicits questions that appear as relatively neglected but are integral to our profession, namely, the effect of publish or perish on the creativity, intellectual lives, morale, and psychological and emotional states of faculty.},
  language = {en},
  keywords = {business schools,institutional,publish,research,tenure}
}

@article{diaz_mala_2018,
  title = {{Mala conducta cient\'ifica en la publicaci\'on}},
  author = {D{\'i}az, Rosa Mar{\'i}a Lam},
  year = {2018},
  month = jan,
  journal = {Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  volume = {34},
  number = {1},
  issn = {1561-2996},
  abstract = {La publicaci\'on en revistas cient\'ificas constituye la forma m\'as aceptada para validar una investigaci\'on debido a que pasa por un riguroso proceso de revisi\'on por expertos, que deciden entre lo publicable y lo no publicable con vista a garantizar la calidad de los trabajos. A pesar de esto con frecuencia aparecen pr\'acticas incorrectas relacionadas con la \'etica durante la publicaci\'on, que se conocen como mala conducta cient\'ifica. Las manifestaciones de mala conducta cient\'ifica van desde el fraude cient\'ifico hasta una variedad de faltas que se cometen en el proceso de publicaci\'on. El fraude cient\'ifico incluye la invenci\'on, la falsificaci\'on y el plagio. Las faltas en el proceso de publicaci\'on incluyen la autor\'ia ficticia, la autor\'ia fantasma, la publicaci\'on duplicada, la publicaci\'on fragmentada o publicaci\'on salami, la publicaci\'on inflada, el autoplagio, la incorrecci\'on de citas bibliogr\'aficas, los sesgos de publicaci\'on y la publicaci\'on anticipada.},
  copyright = {Copyright (c) 2018 Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  language = {es},
  keywords = {autoría,conflicto de intereses,ética,fraude científico,investigación,plagio,practices,publicación}
}

@article{dutilh_seven_2016,
  title = {Seven {{Selfish Reasons}} for {{Preregistration}}},
  author = {Dutilh, Eric-Jan Wagenmakers {and} Gilles},
  year = {2016},
  month = oct,
  journal = {APS Observer},
  volume = {29},
  number = {9},
  abstract = {Psychological scientists Eric-Jan Wagenmakers and Gilles Dutilh present an illustrated guide to the career benefits of submitting your research plans before beginning your data collection.},
  language = {en-US}
}

@article{earth_reproducibility_2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {on Earth, Division and on Behavioral, Cognitive Board},
  year = {2019},
  journal = {undefined},
  abstract = {Semantic Scholar extracted view of \&quot;Reproducibility and Replicability in Science\&quot; by Division on Earth et al.},
  language = {en}
}

@article{editors_observational_2014,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  journal = {PLOS Medicine},
  volume = {11},
  number = {8},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  language = {en},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy}
}

@article{editors_observational_2014a,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  journal = {PLOS Medicine},
  volume = {11},
  number = {8},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  language = {en},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy}
}

@article{editors_transparency_2015,
  title = {Transparency in {{Reporting Observational Studies}}: {{Reflections}} after a {{Year}}},
  shorttitle = {Transparency in {{Reporting Observational Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2015},
  month = oct,
  journal = {PLOS Medicine},
  volume = {12},
  number = {10},
  pages = {e1001896},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001896},
  abstract = {The PLOS Medicine Editors take stock of changes in the reporting of observational studies following our new transparency guidelines from August 2014.},
  language = {en},
  keywords = {Cohort studies,Diagnostic medicine,Health care policy,Observational studies,Open access medical journals,Peer review,Reflection,Water resources}
}

@article{elliott_taxonomy_2020,
  title = {A {{Taxonomy}} of {{Transparency}} in {{Science}}},
  author = {Elliott, Kevin C.},
  year = {2020},
  journal = {Canadian Journal of Philosophy},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2020.21},
  abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
  language = {en},
  keywords = {herramienta,open science,research ethics,science communication,transparency,value judgments,values and science}
}

@misc{engzell_improving_2020,
  title = {Improving {{Social Science}}: {{Lessons}} from the {{Open Science Movement}}},
  shorttitle = {Improving {{Social Science}}},
  author = {Engzell, Per and Rohrer, Julia M.},
  year = {2020},
  month = apr,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/6whjt},
  abstract = {The transdisciplinary movement towards greater research transparency opens the door for a meta-scientific exchange between different social sciences. In the spirit of such an exchange, we offer some lessons inspired by ongoing debates in psychology, highlighting the broad benefits of open science but also potential pitfalls, as well as practical challenges in the implementation that have not yet been fully resolved. Our discussion is aimed towards political scientists but relevant for population sciences more broadly.},
  keywords = {credibility,meta-science,open science,replication,reproducibility,Social and Behavioral Sciences,transparency}
}

@article{fanelli_how_2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta}}-{{Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  month = may,
  journal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc\ldots{} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86\textendash 4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once \textendash a serious form of misconduct by any standard\textendash{} and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91\textendash 19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  language = {en},
  keywords = {Deception,important,Medical journals,Medicine and health sciences,Metaanalysis,practices,Scientific misconduct,Scientists,Social research,Surveys}
}

@article{fanelli_opinion_2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  language = {en},
  keywords = {crisis,forrt}
}

@misc{fernandez_derechos_2009,
  title = {Derechos de {{Autor}}},
  author = {Fern{\'a}ndez, Juan Carlos},
  year = {2009},
  journal = {Derechos de Autor en plataformas e-learning}
}

@inproceedings{fernandez_derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  booktitle = {{{VIII Conferencia Biredial}}-{{ISTEC}}},
  author = {Fern{\'a}ndez, Juan Carlos and Graziosi, Eduardo and Mart{\'i}nez, Daniel},
  year = {2018},
  address = {{Lima - Per\'u}}
}

@inproceedings{fernandezmolina_derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  shorttitle = {Derechos de Autor y Ciencia Abierta},
  booktitle = {{{VIII Conferencia Internacional}} Sobre {{Bibliotecas}} y {{Repositorios Digitales BIREDIAL}}-{{ISTEC}} ({{Lima}}, 2018)},
  author = {Fern{\'a}ndez Molina, Juan Carlos and Graziosi Silva, Eduardo and Mart{\'i}nez {\'A}vila, Daniel},
  year = {2018}
}

@incollection{fidler_reproducibility_2021,
  title = {Reproducibility of {{Scientific Results}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.}
}

@incollection{figueiredo_data_2020,
  title = {Data {{Collection With Indigenous People}}: {{Fieldwork Experiences From Chile}}},
  shorttitle = {Data {{Collection With Indigenous People}}},
  booktitle = {Researching {{Peace}}, {{Conflict}}, and {{Power}} in the {{Field}}: {{Methodological Challenges}} and {{Opportunities}}},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro},
  editor = {Acar, Yasemin G{\"u}ls{\"u}m and Moss, Sigrun Marie and Ulu{\u g}, {\"O}zden Melis},
  year = {2020},
  series = {Peace {{Psychology Book Series}}},
  pages = {105--127},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-44113-5_7},
  abstract = {At present, the Mapuche are the largest indigenous group living in Chile and, up until the present day, they are considered a disadvantaged group in Chilean society in terms of poverty, education and discrimination indicators. In recent decades, this group has been involved in a violent conflict with the Chilean state, forestry and hydroelectric industries and big landowners due mainly to territorial claims of the ancestral land that is currently inhabited and exploited by these different actors. In the present chapter, we narrate the process of data collection with indigenous participants within the framework of a three-year long project about representations of history and present-day intergroup relations between the Mapuche and the non-indigenous majority in Chile. We focus on the challenges that data collection involved by highlighting the process of participant recruitment and trust issues revolving around data collection, as well as retribution practices. Moreover, we also highlight the pros and cons of having non-indigenous Chilean and international researchers conducting fieldwork in this context. Another aspect we address is how methodological approaches may influence the data quality and participants' degree of involvement with the project, by highlighting how these issues interconnect with cultural differences and this indigenous group's worldview and cultural practices. We hope this chapter may provide significant insights on how to deal with some of the difficulties that data collection with indigenous people may involve.},
  isbn = {978-3-030-44113-5},
  language = {en},
  keywords = {Chile,Fieldwork,Mapuche,Qualitative research,Quantitative research}
}

@article{figueiredo_groupbased_2015,
  title = {Group-Based {{Compunction}} and {{Anger}}: {{Their Antecedents}} and {{Consequences}} in {{Relation}} to {{Colonial Conflicts}}},
  shorttitle = {Group-Based {{Compunction}} and {{Anger}}},
  author = {Figueiredo, Ana and Doosje, Bertjan and Valentim, Joaquim Pires},
  year = {2015},
  journal = {International Journal of Conflict and Violence (IJCV)},
  volume = {9},
  pages = {90--105},
  issn = {1864-1385},
  doi = {10.4119/ijcv-3070},
  copyright = {Copyright (c) 2016 International Journal of Conflict and Violence},
  language = {en}
}

@incollection{figueiredo_representations_2019,
  title = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People: {{The Mapuche}} in {{Chile}}},
  shorttitle = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro and Ferreiro, Trinidad and Guerrero, Catarina and Varela O'Reilly, Micaela and Garc{\'i}a, Bernardita and Mu{\~n}oz, Loreto and Schmidt, Magdalena and Cornejo, Marcela and Licata, Laurent},
  year = {2019},
  month = jan,
  pages = {79--104},
  isbn = {978-1-5361-6164-9}
}

@article{figueiredo_too_2015,
  title = {Too Long to Compensate? {{Time}} Perceptions, Emotions, and Compensation for Colonial Conflicts},
  shorttitle = {Too Long to Compensate?},
  author = {Figueiredo, Ana Mateus and Valentim, Joaquim Pires and Doosje, Bertjan},
  year = {2015},
  journal = {Peace and Conflict: Journal of Peace Psychology},
  volume = {21},
  number = {3},
  pages = {500--504},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {1532-7949(ELECTRONIC),1078-1919(PRINT)},
  doi = {10.1037/pac0000114},
  abstract = {In the present article we analyze the role of perceptions of time and ingroup-focused compunction and anger on the desire to compensate the outgroup in relation to historical colonial conflicts. Furthermore, we analyze the relationships between the aforementioned variables and perceptions of the past as being violent and perceptions that compensation has been enough. By means of multiple group structural equation modeling using 1 Portuguese sample (N = 170) and 1 Dutch sample (N = 238), we were able to show that perceptions of the time passed between the negative events and the present day are negatively related to compensatory behavioral intentions. Furthermore, the belief that past compensation has been enough is negatively related to ingroup-focused anger and compunction. Anger (Portuguese sample only) and compunction are positively associated with intentions of compensation. The implications of our results for the field of intergroup relations are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Conflict,Emotions,History,Ingroup Outgroup,Time Perception}
}

@article{flier_faculty_2017,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  journal = {Nature},
  volume = {549},
  number = {7671},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  language = {en},
  keywords = {forrt}
}

@article{flier_faculty_2017a,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  journal = {Nature},
  volume = {549},
  number = {7671},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Careers;Lab life;Research data;Research management Subject\_term\_id: careers;lab-life;research-data;research-management}
}

@misc{fortney_social_2015,
  title = {A Social Networking Site Is Not an Open Access Repository},
  author = {Fortney, Katie and Gonder, Justin},
  year = {2015},
  journal = {Office of Scholary Communication University of California}
}

@article{franca_reproducibility_2019,
  title = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies: {{Untangling}} the Knot},
  shorttitle = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies},
  author = {Fran{\c c}a, Thiago F. A. and Monserrat, Jos{\'e} Maria},
  year = {2019},
  month = oct,
  journal = {Learned Publishing},
  volume = {32},
  number = {4},
  pages = {406--408},
  issn = {0953-1513, 1741-4857},
  doi = {10.1002/leap.1250},
  language = {en},
  keywords = {crisis}
}

@article{franco_publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  month = sep,
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  language = {en},
  keywords = {practices}
}

@article{freese_replication_2017,
  title = {Replication in {{Social Science}}},
  author = {Freese, Jeremy and Peterson, David},
  year = {2017},
  month = jul,
  journal = {Annual Review of Sociology},
  volume = {43},
  number = {1},
  pages = {147--165},
  publisher = {{Annual Reviews}},
  issn = {0360-0572},
  doi = {10.1146/annurev-soc-060116-053450},
  abstract = {Across the medical and social sciences, new discussions about replication have led to transformations in research practice. Sociologists, however, have been largely absent from these discussions. The goals of this review are to introduce sociologists to these developments, synthesize insights from science studies about replication in general, and detail the specific issues regarding replication that occur in sociology. The first half of the article argues that a sociologically sophisticated understanding of replication must address both the ways that replication rules and conventions evolved within an epistemic culture and how those cultures are shaped by specific research challenges. The second half outlines the four main dimensions of replicability in quantitative sociology\textemdash verifiability, robustness, repeatability, and generalizability\textemdash and discusses the specific ambiguities of interpretation that can arise in each. We conclude by advocating some commonsense changes to promote replication while acknowledging the epistemic diversity of our field.}
}

@article{frey_publishing_2003,
  title = {Publishing as {{Prostitution}}? \textendash{} {{Choosing Between One}}'s {{Own Ideas}} and {{Academic Success}}},
  shorttitle = {Publishing as {{Prostitution}}?},
  author = {Frey, Bruno S.},
  year = {2003},
  month = jul,
  journal = {Public Choice},
  volume = {116},
  number = {1},
  pages = {205--223},
  issn = {1573-7101},
  doi = {10.1023/A:1024208701874},
  abstract = {Survival in academia depends on publications in refereedjournals. Authors only get their papers accepted if theyintellectually prostitute themselves by slavishly followingthe demands made by anonymous referees who have no propertyrights to the journals they advise. Intellectual prostitutionis neither beneficial to suppliers nor consumers. But it isavoidable. The editor (with property rights to the journal)should make the basic decision of whether a paper is worthpublishing or not. The referees should only offer suggestionsfor improvement. The author may disregard this advice. Thisreduces intellectual prostitution and produces more originalpublications.},
  language = {en},
  keywords = {institutional}
}

@article{friese_phacking_2020,
  title = {P-{{Hacking}} and {{Publication Bias Interact}} to {{Distort Meta}}-{{Analytic Effect Size Estimates}}},
  author = {Friese, Malte and Frankenbach, Julius},
  year = {2020},
  month = aug,
  journal = {Psychological Methods},
  volume = {25},
  number = {4},
  pages = {456--471},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {1082-989X},
  doi = {10.1037/met0000246},
  abstract = {Science depends on trustworthy evidence. Thus, a biased scientific record is of questionable value because it impedes scientific progress, and the public receives advice on the basis of unreliable evidence that has the potential to have far-reaching detrimental consequences. Meta-analysis is a technique that can be used to summarize research evidence. However, meta-analytic effect size estimates may themselves be biased, threatening the validity and usefulness of meta-analyses to promote scientific progress. Here, we offer a large-scale simulation study to elucidate how p-hacking and publication bias distort meta-analytic effect size estimates under a broad array of circumstances that reflect the reality that exists across a variety of research areas. The results revealed that, first, very high levels of publication bias can severely distort the cumulative evidence. Second, p-hacking and publication bias interact: At relatively high and low levels of publication bias, p-hacking does comparatively little harm, but at medium levels of publication bias, p-hacking can considerably contribute to bias, especially when the true effects are very small or are approaching zero. Third, p-hacking can severely increase the rate of false positives. A key implication is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence. Translational Abstract In recent years, the trustworthiness of psychological science has been questioned. A major concern is that many research findings are less robust than the published evidence suggests. Several reasons may contribute to this state of affairs. Two prominently discussed reasons are that (a) researchers use questionable research practices (so called p-hacking) when they analyze the data of their empirical studies, and (b) studies that revealed results consistent with expectations are more likely published than studies that "failed" (publication bias). The present large-scale simulation study estimates the extent to which meta-analytic effect sizes are biased by different degrees of p-hacking and publication bias, considering several factors of influence that may impact on this bias (e.g., the true effect of the phenomenon of interest). Results show that both p-hacking and publication bias contribute to a potentially severely biased impression of the overall evidence. This is especially the case when the true effect that is investigated is very small or does not exist at all. Severe publication bias alone can exert considerable bias; p-hacking exerts considerable bias only when there is also publication bias. However, p-hacking can severely increase the rate of false positives, that is, findings that suggest that a study found a real effect when, in reality, no effect exists. A key implication of the present study is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence.},
  language = {English},
  keywords = {curve,decisions,incentives,meta-analysis,meta-regression,metascience,p-hacking,prevalence,psychological-research,publication bias,registered-reports,robust,science,tests},
  annotation = {WOS:000563801800004}
}

@article{gall_credibility_2017,
  title = {The Credibility Crisis in Research: {{Can}} Economics Tools Help?},
  shorttitle = {The Credibility Crisis in Research},
  author = {Gall, Thomas and Ioannidis, John P. A. and Maniadis, Zacharias},
  year = {2017},
  month = apr,
  journal = {PLOS Biology},
  volume = {15},
  number = {4},
  pages = {e2001846},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2001846},
  abstract = {The issue of nonreplicable evidence has attracted considerable attention across biomedical and other sciences. This concern is accompanied by an increasing interest in reforming research incentives and practices. How to optimally perform these reforms is a scientific problem in itself, and economics has several scientific methods that can help evaluate research reforms. Here, we review these methods and show their potential. Prominent among them are mathematical modeling and laboratory experiments that constitute affordable ways to approximate the effects of policies with wide-ranging implications.},
  language = {en},
  keywords = {crisis,Economic models,Economics,Experimental economics,Game theory,Health economics,Labor economics,Mathematical modeling,Randomized controlled trials}
}

@article{gerber_publication_2008,
  title = {Publication {{Bias}} in {{Empirical Sociological Research}}: {{Do Arbitrary Significance Levels Distort Published Results}}?},
  shorttitle = {Publication {{Bias}} in {{Empirical Sociological Research}}},
  author = {Gerber, Alan S. and Malhotra, Neil},
  year = {2008},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {37},
  number = {1},
  pages = {3--30},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124108318973},
  abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
  language = {en},
  keywords = {caliper test,hypothesis testing,meta-analysis,practices,publication bias}
}

@article{gerber_statistical_2008,
  title = {Do {{Statistical Reporting Standards Affect What Is Published}}? {{Publication Bias}} in {{Two Leading Political Science Journals}}},
  shorttitle = {Do {{Statistical Reporting Standards Affect What Is Published}}?},
  author = {Gerber, Alan and Malhotra, Neil},
  year = {2008},
  month = oct,
  journal = {Quarterly Journal of Political Science},
  volume = {3},
  number = {3},
  pages = {313--326},
  publisher = {{Now Publishers, Inc.}},
  issn = {1554-0626, 1554-0634},
  doi = {10.1561/100.00008024},
  abstract = {Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals},
  language = {English},
  keywords = {practices}
}

@article{geukes_ways_2016,
  title = {{Ways Out of the Crisis of Confidence: Individual Steps Toward a Reliable and Open Science}},
  shorttitle = {{Ways Out of the Crisis of Confidence}},
  author = {Geukes, Katharina and Schoenbrodt, Felix D. and Utesch, Till and Geukes, Sebastian and Back, Mitja D.},
  year = {2016},
  journal = {Zeitschrift Fur Sportpsychologie},
  volume = {23},
  number = {3},
  pages = {99--109},
  publisher = {{Hogrefe \& Huber Publishers}},
  address = {{Gottingen}},
  issn = {1612-5010},
  doi = {10.1026/1612-5010/a000167},
  abstract = {Psychology faces a so-called crisis of confidence as does sport psychology (see title of this special issue). While the debate on its causes and consequences is lively, the deduction of individual opportunities to collectively increase trust is missing. We propose ways out of this crisis and above all describe individual steps toward a reliable and open science. Reliable science refers to the publication of robust effects, as well as to direct and conceptual replications, and open science refers to transparency regarding the design (preregistration), the conducting (open material), and the analysis (open data, reproducible code) of scientific studies. The commitment to reliable and open science wilt change our behavior in the diverse roles within the scientific system (e.g., as researchers, reviewers, supervisors, editors, or members of commissions). In this sense, we consider the current discussion as a chance to enhance the trustworthiness of our findings and to ultimately create justified confidence.},
  language = {German},
  keywords = {crisis of confidence,exercise,incentives,open science,preregistration,psychological-research,recommendations,registered-reports,replicability,replication,research practices,special section,sport,truth},
  annotation = {WOS:000392884700005}
}

@article{gilbert_comment_2016,
  title = {Comment on "{{Estimating}} the Reproducibility of Psychological Science"},
  author = {Gilbert, D. T. and King, G. and Pettigrew, S. and Wilson, T. D.},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aad7243},
  language = {en}
}

@article{goodman_what_2016,
  title = {What Does Research Reproducibility Mean?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  journal = {Science Translational Medicine},
  volume = {8},
  number = {341},
  pages = {341ps12-341ps12},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {{$<$}p{$>$}The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''{$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  language = {en},
  pmid = {27252173}
}

@article{gorman_systems_2019,
  title = {A {{Systems Approach}} to {{Understanding}} and {{Improving Research Integrity}}},
  author = {Gorman, Dennis M. and Elkins, Amber D. and Lawley, Mark},
  year = {2019},
  month = feb,
  journal = {Science and Engineering Ethics},
  volume = {25},
  number = {1},
  pages = {211--229},
  publisher = {{Springer}},
  address = {{Dordrecht}},
  issn = {1353-3452},
  doi = {10.1007/s11948-017-9986-z},
  abstract = {Concern about the integrity of empirical research has arisen in recent years in the light of studies showing the vast majority of publications in academic journals report positive results, many of these results are false and cannot be replicated, and many positive results are the product of data dredging and the application of flexible data analysis practices coupled with selective reporting. While a number of potential solutions have been proposed, the effects of these are poorly understood and empirical evaluation of each would take many years. We propose that methods from the systems sciences be used to assess the effects, both positive and negative, of proposed solutions to the problem of declining research integrity such as study registration, Registered Reports, and open access to methods and data. In order to illustrate the potential application of systems science methods to the study of research integrity, we describe three broad types of models: one built on the characteristics of specific academic disciplines; one a diffusion of research norms model conceptualizing researchers as susceptible, infected and recovered; and one conceptualizing publications as a product produced by an industry comprised of academics who respond to incentives and disincentives.},
  language = {English},
  keywords = {aversion,bias,diffusion,dynamics,impact,medicine,Open data,programs,publication,Publish or perish,registered reports,Registered reports,Research ethics,science,System dynamics,Systems thinking},
  annotation = {WOS:000461310400012}
}

@article{greiff_introducing_2020,
  title = {Introducing {{New Open Science Practices}} at {{EJPA}}},
  author = {Greiff, Samuel and {van der Westhuizen}, Lindie and Mund, Marcus and Rauthmann, John F. and Wetzel, Eunike},
  year = {2020},
  month = sep,
  journal = {European Journal of Psychological Assessment},
  volume = {36},
  number = {5},
  pages = {717--720},
  publisher = {{Hogrefe Publishing Corp}},
  address = {{Boston}},
  issn = {1015-5759},
  doi = {10.1027/1015-5759/a000628},
  language = {English},
  keywords = {registered reports},
  annotation = {WOS:000595143100001}
}

@article{gundersen_fundamental_2021,
  title = {The Fundamental Principles of Reproducibility},
  author = {Gundersen, Odd Erik},
  year = {2021},
  month = may,
  journal = {Philosophical Transactions of the Royal Society a-Mathematical Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {20200210},
  publisher = {{Royal Soc}},
  address = {{London}},
  issn = {1364-503X},
  doi = {10.1098/rsta.2020.0210},
  abstract = {Reproducibility is a confused terminology. In this paper, I take a fundamental view on reproducibility rooted in the scientific method. The scientific method is analysed and characterized in order to develop the terminology required to define reproducibility. Furthermore, the literature on reproducibility and replication is surveyed, and experiments are modelled as tasks and problem solving methods. Machine learning is used to exemplify the described approach. Based on the analysis, reproducibility is defined and three different degrees of reproducibility as well as four types of reproducibility are specified. This article is part of the theme issue 'Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico'.},
  language = {English},
  keywords = {go,level,machine learning,problem solving methods,registered-reports,scientific method},
  annotation = {WOS:000636259600012}
}

@article{guttinger_limits_2020,
  title = {The Limits of Replicability},
  author = {Guttinger, Stephan},
  year = {2020},
  month = jan,
  journal = {European Journal for Philosophy of Science},
  volume = {10},
  number = {2},
  pages = {10},
  issn = {1879-4920},
  doi = {10.1007/s13194-019-0269-1},
  abstract = {Discussions about a replicability crisis in science have been driven by the normative claim that all of science should be replicable and the empirical claim that most of it isn't. Recently, such crisis talk has been challenged by a new localism, which argues a) that serious problems with replicability are not a general occurrence in science and b) that replicability itself should not be treated as a universal standard. The goal of this article is to introduce this emerging strand of the debate and to discuss some of its implications and limitations. I will in particular highlight the issue of demarcation that localist accounts have to address, i.e. the question of how we can distinguish replicable science from disciplines where replicability does not apply.},
  language = {en}
}

@article{haven_preregistering_2019,
  title = {Preregistering Qualitative Research},
  author = {Haven, Tamarinde L. and Grootel, Dr Leonie Van},
  year = {2019},
  month = apr,
  journal = {Accountability in Research},
  volume = {26},
  number = {3},
  pages = {229--244},
  publisher = {{Taylor \& Francis}},
  issn = {0898-9621},
  doi = {10.1080/08989621.2019.1580147},
  abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
  pmid = {30741570},
  keywords = {Preregistration,qualitative research,reports,transparency},
  annotation = {\_eprint: https://doi.org/10.1080/08989621.2019.1580147}
}

@article{head_extent_2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  journal = {PLOS Biology},
  volume = {13},
  number = {3},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  language = {en},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,practices,Publication ethics,Reproducibility,Statistical data,Test statistics}
}

@article{hernandez_open_2007,
  title = {{{OPEN ACCESS}}: {{EL PAPEL DE LAS BIBLIOTECAS EN LOS REPOSITORIOS INSTITUCIONALES DE ACCESO ABIERTO}}},
  author = {Hern{\'a}ndez, Tony and Rodr{\'i}guez, David and {Bueno De la Fuente}, Gema},
  year = {2007},
  volume = {10},
  pages = {185--204},
  abstract = {En Espa\~na, como en muchos otros pa\'ises, el n\'umero de repositorios insti-tucionales  ha  ido  creciendo  paulatinamente  en  los  \'ultimos  tres  a\~nos.  En  febrero  de  2007 estos repositorios contienen ya m\'as de 30000 documentos en acceso abierto, es decir, disponibles a texto completo de forma gratuita, y con posibilidad de descarga, impresi\'on o copia sin coste a\~nadido. La pr\'actica totalidad de estos repositorios est\'an siendo  gestionados  por  los  servicios  de  biblioteca  de  las  distintas  instituciones  que  los albergan. Este art\'iculo explica las razones de la crisis del modelo tradicional de comunicaci\'on cient\'ifica, iniciado en la era de lo impreso y que se ha trasladado a la era digital, la alternativa que representa el modelo basado en el acceso abierto, y el importante papel que las bibliotecas pueden jugar, un reto y una oportunidad que no deben perder, en la construcci\'on de colecciones digitales propias}
}

@article{hernandez_por_2016,
  title = {\textquestiondown{{Por}} Qu\'e Open Access?},
  author = {Hern{\'a}ndez, Enrique},
  year = {2016},
  volume = {28},
  number = {1},
  issn = {2448-8771}
}

@article{hitzig_problem_2020,
  title = {The {{Problem}} of {{New Evidence}}: {{P}}-{{Hacking}} and {{Pre}}-{{Analysis Plans}}},
  shorttitle = {The {{Problem}} of {{New Evidence}}},
  author = {Hitzig, Zoe and Stegenga, Jacob},
  year = {2020},
  month = dec,
  journal = {Diametros},
  volume = {17},
  number = {66},
  pages = {10--33},
  publisher = {{Jagiellonian Univ, Inst Philosophy}},
  address = {{Krakow}},
  issn = {1733-5566},
  doi = {10.33392/diam.1587},
  abstract = {We provide a novel articulation of the epistemic peril of p-hacking using three resources from philosophy: predictivism, Bayesian confirmation theory, and model selection theory. We defend a nuanced position on p-hacking: p-hacking is sometimes, but not always, epistemically pernicious. Our argument requires a novel understanding of Bayesianism, since a standard criticism of Bayesian confirmation theory is that it cannot represent the influence of biased methods. We then turn to pre-analysis plans, a methodological device used to mitigate p-hacking. Some say that pre-analysis plans are epistemically meritorious while others deny this, and in practice pre-analysis plans are often violated. We resolve this debate with a modest defence of pre-analysis plans. Further, we argue that pre-analysis plans can be epistemically relevant even if the plan is not strictly followed-and suggest that allowing for flexible pre-analysis plans may be the best available policy option.},
  language = {English},
  keywords = {Bayesian confirmation theory,credibility,old evidence,p-hacking,pre-analysis plans,prediction,predictivism,registered-reports,replication crisis,science,transparency},
  annotation = {WOS:000600025300002}
}

@article{hollenbeck_harking_2017,
  title = {Harking, {{Sharking}}, and {{Tharking}}: {{Making}} the {{Case}} for {{Post Hoc Analysis}} of {{Scientific Data}}},
  shorttitle = {Harking, {{Sharking}}, and {{Tharking}}},
  author = {Hollenbeck, John R. and Wright, Patrick M.},
  year = {2017},
  month = jan,
  journal = {Journal of Management},
  volume = {43},
  number = {1},
  pages = {5--18},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206316679487},
  abstract = {In this editorial we discuss the problems associated with HARKing (Hypothesizing After Results Are Known) and draw a distinction between Sharking (Secretly HARKing in the Introduction section) and Tharking (Transparently HARKing in the Discussion section). Although there is never any justification for the process of Sharking, we argue that Tharking can promote the effectiveness and efficiency of both scientific inquiry and cumulative knowledge creation. We argue that the discussion sections of all empirical papers should include a subsection that reports post hoc exploratory data analysis. We explain how authors, reviewers, and editors can best leverage post hoc analyses in the spirit of scientific discovery in a way that does not bias parameter estimates and recognizes the lack of definitiveness associated with any single study or any single replication. We also discuss why the failure to Thark in high-stakes contexts where data is scarce and costly may also be unethical.},
  language = {en},
  keywords = {macro topics,micro topics,philosophy of science,practices,research design,research methods,statistical methods}
}

@article{horbach_changing_2018,
  title = {The Changing Forms and Expectations of Peer Review},
  author = {Horbach, S. P. J. M. and Halffman, W.},
  year = {2018},
  month = sep,
  journal = {Research Integrity and Peer Review},
  volume = {3},
  number = {1},
  pages = {8},
  publisher = {{Bmc}},
  address = {{London}},
  doi = {10.1186/s41073-018-0051-5},
  abstract = {The quality and integrity of the scientific literature have recently become the subject of heated debate. Due to an apparent increase in cases of scientific fraud and irreproducible research, some have claimed science to be in a state of crisis. A key concern in this debate has been the extent to which science is capable of self-regulation. Among various mechanisms, the peer review system in particular is considered an essential gatekeeper of both quality and sometimes even integrity in science.However, the allocation of responsibility for integrity to the peer review system is fairly recent and remains controversial. In addition, peer review currently comes in a wide variety of forms, developed in the expectation they can address specific problems and concerns in science publishing. At present, there is a clear need for a systematic analysis of peer review forms and the concerns underpinning them, especially considering a wave of experimentation fuelled by internet technologies and their promise to improve research integrity and reporting.We describe the emergence of current peer review forms by reviewing the scientific literature on peer review and by adding recent developments based on information from editors and publishers. We analyse the rationale for developing new review forms and discuss how they have been implemented in the current system. Finally, we give a systematisation of the range of discussed peer review forms. We pay detailed attention to the emergence of the expectation that peer review can maintain 'the integrity of science's published record', demonstrating that this leads to tensions in the academic debate about the responsibilities and abilities of the peer review system.},
  language = {English},
  keywords = {credibility,exchange,fraud,Innovation,journals,Peer review,plagiarism,publication bias,registered-reports,science,Scientific integrity,scientific misconduct,Scientific misconduct,Scientific publishing},
  annotation = {WOS:000561282300001}
}

@misc{horgan_psychology_,
  title = {Psychology's {{Credibility Crisis}}: The {{Bad}}, the {{Good}} and the {{Ugly}}},
  shorttitle = {Psychology's {{Credibility Crisis}}},
  author = {Horgan, John},
  journal = {Scientific American},
  doi = {10.1038/scientificamericanmind0716-18},
  abstract = {As more studies are called into question and researchers bicker over methodology, the field is showing a healthy willingness to face its problems\&nbsp;},
  howpublished = {https://www.scientificamerican.com/article/psychology-s-credibility-crisis-the-bad-the-good-and-the-ugly/},
  language = {en},
  keywords = {crisis}
}

@article{ioannidis_power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  issn = {1468-0297},
  doi = {10.1111/ecoj.12461},
  abstract = {We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical economics literatures that draw upon 64,076 estimates of economic parameters reported in more than 6,700 empirical studies. Half of the research areas have nearly 90\% of their results under-powered. The median statistical power is 18\%, or less. A simple weighted average of those reported results that are adequately powered (power {$\geq$} 80\%) reveals that nearly 80\% of the reported effects in these empirical economics literatures are exaggerated; typically, by a factor of two and with one-third inflated by a factor of four or more.},
  copyright = {\textcopyright{} 2017 Royal Economic Society},
  language = {en},
  keywords = {bias,credibility,empirical economics,practices,publication bias,statistical power},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecoj.12461}
}

@article{ioannidis_why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  keywords = {Cancer risk factors,crisis,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,practices,Randomized controlled trials,Research design,Schizophrenia}
}

@article{jara_tracing_2018,
  title = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}: {{Official}} and {{Grassroots Initiatives}}},
  shorttitle = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}},
  author = {Jara, Daniela and Badilla, Manuela and Figueiredo, Ana and Cornejo, Marcela and Riveros, Victoria},
  year = {2018},
  month = nov,
  journal = {International Journal of Transitional Justice},
  volume = {12},
  number = {3},
  pages = {479--498},
  issn = {1752-7716},
  doi = {10.1093/ijtj/ijy025},
  abstract = {This article critically examines the official misrecognition of Mapuche experiences of violence during Augusto Pinochet's dictatorship (1973\textendash 1990) in state-sponsored truth commissions in Chile. We examine official post-dictatorial truth commission politics, narratives and procedures, analyzing how they envisioned the Mapuche as a political (absent) subject and how specific and homogenizing notions of victimhood were produced. We draw attention to three forms of cultural response by the Mapuche toward the official practices of the truth commissions from a bottom-up perspective: indifference, ambivalence and cultural resistance. We then draw attention to unofficial initiatives by nongovernmental organizations (NGOs) and grassroots groups that have aimed to tackle this gap in the transitional justice mechanisms by creating oppositional knowledge. We see in these counter initiatives a valuable knowledge that could allow the creation of bridges between Mapuche communities, mechanisms of transitional justice, grassroots and NGO activism and the Chilean state.}
}

@article{jerolmack_ethical_2019,
  title = {The {{Ethical Dilemmas}} and {{Social Scientific Trade}}-Offs of {{Masking}} in {{Ethnography}}},
  author = {Jerolmack, Colin and Murphy, Alexandra K.},
  year = {2019},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {48},
  number = {4},
  pages = {801--827},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117701483},
  abstract = {Masking, the practice of hiding or distorting identifying information about people, places, and organizations, is usually considered a requisite feature of ethnographic research and writing. This is justified both as an ethical obligation to one's subjects and as a scientifically neutral position (as readers are enjoined to treat a case's idiosyncrasies as sociologically insignificant). We question both justifications, highlighting potential ethical dilemmas and obstacles to constructing cumulative social science that can arise through masking. Regarding ethics, we show, on the one hand, how masking may give subjects a false sense of security because it implies a promise of confidentiality that it often cannot guarantee and, on the other hand, how naming may sometimes be what subjects want and expect. Regarding scientific tradeoffs, we argue that masking can reify ethnographic authority, exaggerate the universality of the case (e.g., ``Middletown''), and inhibit replicability (or ``revisits'') and sociological comparison. While some degree of masking is ethically and practically warranted in many cases and the value of disclosure varies across ethnographies, we conclude that masking should no longer be the default option that ethnographers unquestioningly choose.},
  language = {en},
  keywords = {disclosure,ethics,ethnography,generalizability,masking,pseudonyms}
}

@article{jl_it_2017,
  title = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}: {{Thoughts}} for and {{From Clinical Psychological Science}}},
  shorttitle = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}},
  author = {Jl, Tackett and So, Lilienfeld and Cj, Patrick and Sl, Johnson and Rf, Krueger and Jd, Miller and Tf, Oltmanns and Pe, Shrout},
  year = {2017},
  month = sep,
  journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  volume = {12},
  number = {5},
  publisher = {{Perspect Psychol Sci}},
  issn = {1745-6924},
  doi = {10.1177/1745691617690042},
  abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. N \ldots},
  language = {en},
  pmid = {28972844}
}

@article{john_measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  language = {en},
  keywords = {disclosure,judgment,methodology,practices,professional standards}
}

@article{kapiszewski_openness_2019,
  title = {Openness in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2019},
  month = jun,
  doi = {10.33774/apsa-2019-if2he},
  abstract = {The discipline of political science has been engaged in discussion about when, why, and how to make scholarship more open for at least three decades.This piece argues that the best way to resolve our differences and develop appropriate norms and guidelines for making different types of qualitative research more open is to move from ``if'' to ``how'' \textendash{} for individual political scientists to make their work more open \textendash{} generating examples from which we can learn and on which we can build. We begin by articulating a series of principles that underlie our views on openness. We then consider the ``state of the debate,'' briefly outlining the contours of the scholarship on openness in political and other social sciences, highlighting the fractured nature of the discussion. The heart of the piece considers various strategies, illustrated by exemplary applications, for making qualitative research more open.},
  language = {en}
}

@article{kapiszewski_transparency_2021,
  title = {Transparency in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2021},
  month = apr,
  journal = {PS: Political Science \& Politics},
  volume = {54},
  number = {2},
  pages = {285--291},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000955},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000955/resource/name/firstPage-S1049096520000955a.jpg},
  language = {en},
  keywords = {transparency}
}

@article{klein_practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jun,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  keywords = {herramienta}
}

@techreport{lareferencia.consejodirectivo_comunicacion_2019,
  title = {{Comunicaci\'on Acad\'emica y Acceso Abierto: Acciones para un Pol\'itica P\'ublica en Am\'erica Latina}},
  shorttitle = {{Comunicaci\'on Acad\'emica y Acceso Abierto}},
  author = {LA Referencia. Consejo Directivo},
  year = {2019},
  month = may,
  institution = {{Zenodo}},
  doi = {10.5281/ZENODO.3229410},
  abstract = {Documento redactado como insumo  para las autoridades regionales que asistieron a la reuni\'on anual del Global Research Council con acuerdo del Consejo Directivo.   La publicaci\'on y difusi\'on del mismo se realiza con el fin de favorecer el di\'alogo y la construcci\'on de una visi\'on conjunta sobre la cual se debe profundizar y actualizar a la luz de los desaf\'ios del Acceso Abierto en la regi\'on en el corto y mediano plazo. La comunicaci\'on cient\'ifica y el cambio del modelo; la situaci\'on de Am\'erica Latina; el sistema de comunicaci\'on acad\'emica de la regi\'on, principios y acciones y recomendaciones para repositorios, consorcios y revistas son los ejes tem\'aticos que se abordan a lo largo de sus p\'aginas. El art\'iculo  refuerza la premisa de que se deben tomar acciones decididas para que los resultados financiados total o parcialmente con fondos p\'ublicos est\'en en Acceso Abierto y reafirma el rol central de los organismos de CyT para lograrlo. Basado en la realidad regional, propone principios generales y acciones para los repositorios de Acceso Abierto, consorcios y revistas con una mirada m\'as sist\'emica desde las pol\'iticas p\'ublicas. Concluye con la necesidad de un di\'alogo con iniciativas como el ``Plan S'' se\~nalando los puntos de acuerdo, as\'i como diferencias, dado el contexto regional, en temas como el APC o una valorizaci\'on del rol de los repositorios. Presentado en la reuni\'on  de   COAR. 2019.  Technical and Strategic Meeting of Repository Networks. Mayo 21, 2019 - Lyon, France. Alberto Cabezas Bullemore, Secretario Ejecutivo,   LA Referencia.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es},
  keywords = {Acceso Abierto,Ciencia Abierta,Financiadores,ONCYTs,Plan S,Repositorios,revisado}
}

@article{lewandowsky_research_2016,
  title = {Research Integrity: {{Don}}'t Let Transparency Damage Science},
  shorttitle = {Research Integrity},
  author = {Lewandowsky, Stephan and Bishop, Dorothy},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {459--461},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/529459a},
  abstract = {Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.},
  copyright = {2016 Nature Publishing Group},
  language = {en},
  keywords = {transparency}
}

@article{linder_unfolding_2020,
  title = {Unfolding the {{Black Box}} of {{Questionable Research Practices}}: {{Where Is}} the {{Line Between Acceptable}} and {{Unacceptable Practices}}?},
  shorttitle = {Unfolding the {{Black Box}} of {{Questionable Research Practices}}},
  author = {Linder, Christian and Farahbakhsh, Siavash},
  year = {2020},
  month = jul,
  journal = {Business Ethics Quarterly},
  volume = {30},
  number = {3},
  pages = {335--360},
  publisher = {{Cambridge Univ Press}},
  address = {{New York}},
  issn = {1052-150X},
  doi = {10.1017/beq.2019.52},
  abstract = {Despite the extensive literature on what questionable research practices (QRPs) are and how to measure them, the normative underpinnings of such practices have remained less explored. QRPs often fall into a grey area of justifiable and unjustifiable practices. Where to precisely draw the line between such practices challenges individual scholars and this harms science. We investigate QRPs from a normative perspective using the theory of communicative action. We highlight the role of the collective in assessing individual behaviours. Our contribution is a framework that allows identification of when particular actions cross over from acceptable to unacceptable practice. Thus, this article provides grounds for developing scientific standards to raise the quality of scientific research.},
  language = {English},
  keywords = {academic standards,academic-freedom,business,deontology,discourse ethics,ethics,file-drawer problem,management,metaanalysis,norms,p-hacking,publication bias,questionable research practice,science,tests},
  annotation = {WOS:000549379800003}
}

@article{lindsay_seven_2020,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science.},
  author = {Lindsay, D. Stephen},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology/Psychologie canadienne},
  volume = {61},
  number = {4},
  pages = {310--317},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000222},
  language = {en},
  keywords = {forrt,transparency}
}

@article{linton_publish_2011,
  title = {Publish or {{Perish}}: {{How Are Research}} and {{Reputation Related}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {Linton, Jonathan D. and Tierney, Robert and Walsh, Steven T.},
  year = {2011},
  month = dec,
  journal = {Serials Review},
  volume = {37},
  number = {4},
  pages = {244--257},
  publisher = {{Routledge}},
  issn = {0098-7913},
  doi = {10.1080/00987913.2011.10765398},
  abstract = {A study of twenty-seven fields in 350 highly ranked universities examines the relationship between reputation and rank. We find that many metrics associated with research prowess significantly correlate to university reputation. However, the next logical step\textendash{} looking at the relationship that links different academic fields with the reputation of the university\textendash did not always offer the expected results. The phrase ``publish or perish'' clearly has very different meanings in different fields.},
  keywords = {Academic reputation,institutional,Interdisciplinary studies,Publish or perish,University research reputation},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00987913.2011.10765398}
}

@misc{loredo_derecho_2012,
  title = {Derecho {{Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  author = {Loredo, Alejandro},
  year = {2012},
  journal = {Portal de gobierno electr\'onico, inclusi\'on digital y sociedad del conocimiento}
}

@article{loredo_mexico_2006,
  title = {M\'exico: {{Derecho Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  shorttitle = {M\'exico},
  author = {Loredo, Alejandro},
  year = {2006},
  journal = {AR: Revista de Derecho Inform\'atico},
  number = {91},
  pages = {2},
  publisher = {{Alfa-Redi}}
}

@article{lucas_introduction_2021,
  title = {Introduction to the {{Special Issue}}: {{Preregistered Studies}} of {{Personality Development}} and {{Aging Using Existing Data}}},
  shorttitle = {Introduction to the {{Special Issue}}},
  author = {Lucas, Richard E. and Donnellan, M. Brent},
  year = {2021},
  month = jan,
  journal = {Journals of Gerontology Series B-Psychological Sciences and Social Sciences},
  volume = {76},
  number = {1},
  pages = {1--5},
  publisher = {{Oxford Univ Press Inc}},
  address = {{Cary}},
  issn = {1079-5014},
  doi = {10.1093/geronb/gbaa192},
  language = {English},
  keywords = {registered-reports},
  annotation = {WOS:000649390900001}
}

@techreport{luke_epistemological_2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Epistemological and {{Ontological Priors}}: {{Explicating}} the {{Perils}} of {{Transparency}}},
  shorttitle = {Epistemological and {{Ontological Priors}}},
  author = {Luke, Timothy W. and {V{\'a}zquez-Arroyo}, Antonio and Hawkesworth, Mary},
  year = {2019},
  month = feb,
  number = {ID 3332878},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3332878},
  abstract = {The discipline of political science encompasses multiple research communities, which have grown out of and rely upon different epistemological and ontological presuppositions.  Recent debates about transparency raise important questions about which of these research communities will be accredited within the discipline, whose values, norms, and methods of knowledge production will gain ascendency and whose will be marginalized.  Although the language of "transparency" makes it appear that these debates are apolitical, simply elaborating standards that all political scientists share, the intensity and content of recent contestations about DA-RT, JETS, and QTD attest to the profoundly political nature of these methodological discussions. This report traces the epistemological and ontological assumptions that have shaped diverse research communities within the discipline, situating "transparency" in relation to classical (Aristotelian), modern (Baconian) and twentieth-century (positivist, critical rationalist, and postpositivist) versions of empiricism.  It shows how recent discussions of transparency accredit certain empirical approaches by collapsing the scope of empirical investigation and the parameters of the knowable.  And it argues that "transparency" is inappropriate as a regulative ideal for political science because it misconstrues the roles of theory, social values, and critique in scholarly investigation.},
  language = {en},
  keywords = {epistemology,ontology,philosophy of science,qualitative methods,Qualitative Transparency Deliberations,research transparency}
}

@article{mardones_usos_2018,
  title = {{Usos del dise\~no metodol\'ogico cualitativo en art\'iculos de acceso abierto de alto impacto en ciencias sociales}},
  shorttitle = {{Dise\~nos metodol\'ogicos en art\'iculos de acceso abierto}},
  author = {Mardones, Rodolfo and Ulloa, Jorge and Salas, Gonz{\'a}lo},
  year = {2018},
  journal = {Forum: Qualitative Social Research},
  volume = {19, n\textdegree 1},
  pages = {1--18},
  issn = {1438-5627},
  abstract = {Las definiciones de dise\~no metodol\'ogico en la perspectiva cualitativa son variadas y sus diversos usos dejan entrever una multiplicidad de perspectivas. El presente trabajo tiene por objeto describir los usos del dise\~no metodol\'ogico cualitativo en art\'iculos de ciencias sociales de alto impacto. Se realiza una revisi\'on de 186 art\'iculos de resultados de investigaci\'on y propuestas metodol\'ogicas, publicados en revistas open access indexadas en Scopus en el periodo 2013-2015. Los resultados muestran que el 75\% de los art\'iculos declara su dise\~no metodol\'ogico. El uso de este se clasific\'o en tres categor\'ias: organizaci\'on de la investigaci\'on, elecci\'on paradigm\'atica y tipo de estudio. El 51\% de los art\'iculos utilizan dise\~no cualitativo basado en el tipo de estudio. Mientras tanto, la organizaci\'on de la investigaci\'on (37\%) y la elecci\'on paradigm\'atica (11\%) quedan en segundo lugar. Se concluye que los usos del dise\~no metodol\'ogico cualitativo se basa en supuestos te\'oricos y pr\'acticas propias de la categor\'ia definida como tipo de estudio. Esto puede facilitar la elecci\'on de un dise\~no cualitativo, sensible y flexible al contexto estudiado.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es}
}

@article{martinson_scientists_2005,
  title = {Scientists Behaving Badly},
  author = {Martinson, Brian C. and Anderson, Melissa S. and {de Vries}, Raymond},
  year = {2005},
  month = jun,
  journal = {Nature},
  volume = {435},
  number = {7043},
  pages = {737--738},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/435737a},
  abstract = {To protect the integrity of science, we must look beyond falsification, fabrication and plagiarism, to a wider range of questionable research practices, argue Brian C. Martinson, Melissa S. Anderson and Raymond de Vries.},
  copyright = {2005 Nature Publishing Group},
  language = {en},
  keywords = {practices},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion}
}

@article{martiny_degrees_2017,
  title = {Degrees of {{Openness}}, {{Embodiment}}, {{Circularity}}, and {{Invariance Response}}},
  author = {Martiny, Kristian Moltke},
  year = {2017},
  month = nov,
  journal = {Constructivist Foundations},
  volume = {13},
  number = {1},
  pages = {83--90},
  publisher = {{Alexander Riegler}},
  address = {{Brussels}},
  issn = {1782-348X},
  language = {English},
  keywords = {consciousness,dynamics,enaction,experience,incentives,neurophenomenology,open science,phenomenology,registered-reports,synchrony},
  annotation = {WOS:000415144800023}
}

@article{mcgrail_publish_2006,
  title = {Publish or Perish: A Systematic Review of Interventions to Increase Academic Publication Rates},
  shorttitle = {Publish or Perish},
  author = {McGrail, Matthew R. and Rickard, Claire M. and Jones, Rebecca},
  year = {2006},
  month = feb,
  journal = {Higher Education Research \& Development},
  volume = {25},
  number = {1},
  pages = {19--35},
  publisher = {{Routledge}},
  issn = {0729-4360},
  doi = {10.1080/07294360500453053},
  abstract = {Academics are expected to publish. In Australia universities receive extra funding based on their academic publication rates and academic promotion is difficult without a good publication record. However, the reality is that only a small percentage of academics are actively publishing. To fix this problem, a number of international universities and other higher education institutions have implemented interventions with the main aim being to increase the number of publications. A comprehensive literature search identified 17 studies published between 1984 and 2004, which examined the effects of these interventions. Three key types of interventions were identified: writing courses, writing support groups and writing coaches. The resulting publication output varied, but all interventions led to an increase in average publication rates for the participants.},
  keywords = {institutional},
  annotation = {\_eprint: https://doi.org/10.1080/07294360500453053}
}

@article{mcintosh_three_2020,
  title = {The Three {{R}}'s of Scientific Integrity: {{Replicability}}, Reproducibility, and Robustness},
  shorttitle = {The Three {{R}}'s of Scientific Integrity},
  author = {McIntosh, Robert D. and Chambers, Christopher D.},
  year = {2020},
  month = aug,
  journal = {Cortex},
  volume = {129},
  pages = {A4-A7},
  publisher = {{Elsevier Masson, Corp Off}},
  address = {{Paris}},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2020.04.019},
  language = {English},
  keywords = {fear,reconsolidation,registered-reports,retrieval},
  annotation = {WOS:000552946000003}
}

@article{mckiernan_how_2016,
  title = {How Open Science Helps Researchers Succeed},
  author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
  editor = {Rodgers, Peter},
  year = {2016},
  month = jul,
  journal = {eLife},
  volume = {5},
  pages = {e16800},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.16800},
  abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
  keywords = {open access,open data,open science,open source,research}
}

@article{mcvay_transparency_2021,
  title = {Transparency and Openness in Behavioral Medicine Research},
  author = {McVay, Megan A and Conroy, David E},
  year = {2021},
  month = jan,
  journal = {Translational Behavioral Medicine},
  volume = {11},
  number = {1},
  pages = {287--290},
  issn = {1869-6716},
  doi = {10.1093/tbm/ibz154},
  abstract = {Behavioral medicine aims to improve the health of individuals and communities by addressing behavioral, psychosocial, and environmental contributors to health. Succeeding in this endeavor requires rigorous research and effective communication of this research to relevant stakeholders and the public at large [1]. Both research rigor and effective communication of research may benefit from adopting transparent and open research practices [2\textendash 4], sometimes called ``open science.'' Such practices include preregistering designs, hypotheses, and data analysis plans; making publically available study materials, data, and analytic code; sharing preprints (works-in-progress) of articles; and publishing open access [2]. In this commentary, we describe the evolving pressures to increase the transparency and openness of research, examine the status of open science practices in behavioral medicine, and recommend a path forward to find the right fit for these practices in behavioral medicine research.},
  keywords = {transparency}
}

@article{melero_revistas_2008,
  title = {Revistas {{Open Access}}: Caracter\'isticas, Modelos Econ\'omicos y Tendencias},
  author = {Melero, Remedios and Abad, Mar{\'i}a Francisca},
  year = {2008},
  volume = {20},
  issn = {1575-5886}
}

@article{mertens_preregistration_2019,
  title = {Preregistration of {{Analyses}} of {{Preexisting Data}}},
  author = {Mertens, Ga{\"e}tan and Krypotos, Angelos-Miltiadis},
  year = {2019},
  journal = {Psychologica Belgica},
  volume = {59},
  number = {1},
  pages = {338--352},
  issn = {0033-2879},
  doi = {10.5334/pb.493},
  abstract = {The preregistration of a study's hypotheses, methods, and data-analyses steps is becoming a popular psychological research practice. To date, most of the discussion on study preregistration has focused on the preregistration of studies that include the collection of original data. However, much of the research in psychology relies on the (re-)analysis of preexisting data. Importantly, this type of studies is different from original studies as researchers cannot change major aspects of the study (e.g., experimental manipulations, sample size). Here, we provide arguments as to why it is useful to preregister analyses of preexisting data, discuss practical considerations, consider potential concerns, and introduce a preregistration template tailored for studies focused on the analyses of preexisting data. We argue that the preregistration of hypotheses and data-analyses for analyses of preexisting data is an important step towards more transparent psychological research.},
  pmcid = {PMC6706998},
  pmid = {31497308}
}

@article{miguel_promoting_2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and der Laan, M. Van},
  year = {2014},
  month = jan,
  journal = {Science},
  volume = {343},
  number = {6166},
  pages = {30--31},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245317},
  abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests (1, 2). Social scientists should adopt higher transparency standards to improve the quality and credibility of research. Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  language = {en},
  pmid = {24385620},
  keywords = {transparency}
}

@article{monti_acceso_2020,
  title = {{Acceso a la literatura cient\'ifica desde Sci-Hub: An\'alisis y reflexi\'on de las descargas en Argentina}},
  author = {Monti, Carolina and Unzurrunzaga, Carolina},
  year = {2020},
  journal = {Revista Hipertextos},
  volume = {8 (14)},
  pages = {111--116},
  doi = {10.24215/23143924e022},
  abstract = {La comercializaci\'on de la informaci\'on cient\'ifica y las barreras de pago para su acceso se han convertido en una de las problem\'aticas m\'as debatidas en las \'ultimas d\'ecadas. El movimiento internacional de acceso abierto ha propuesto v\'ias para favorecer su acceso y, a su vez, distintos actores han impulsado otras estrategias como sitios piratas para la descarga. En este art\'iculo estudiamos el uso que se hace de Sci-Hub en Argentina a partir de un an\'alisis cuantitativo de las estad\'isticas de descargas disponibles hasta 2017. Detectamos patrones de uso generales, editoriales y tem\'aticos de los documentos e indagamos aspectos comunes y diferenciadores con respecto al uso mundial y otros pa\'ises de Am\'erica Latina. Asimismo, determinamos, a partir de distintas muestras, si los art\'iculos accedidos adem\'as est\'an disponibles en acceso abierto y/o su acceso es posible a trav\'es de las suscripciones pagadas por el estado nacional a trav\'es de la Biblioteca Electr\'onica de Ciencia y Tecnolog\'ia (BECyT). Encontramos que las descargas realizadas desde Argentina representan poco m\'as del 1\% de las registradas a nivel mundial, que existe una gran dispersi\'on de t\'itulos solicitados y que hay un aumento significativo en el uso respecto a los mismos per\'iodos de 2015 y 2016. Con las distintas muestras pudimos observar que se est\'an descargando mayormente art\'iculos de acceso restringido publicados por las empresas editoriales que manejan el sector como oligopolio, en revistas indexadas en la llamada ``corriente principal'' y que corresponden al \'area de la medicina (oncolog\'ia, pediatr\'ia y medicina cardiovascular). Tambi\'en, detectamos un n\'umero significativo de descargas de art\'iculos que ya estaban disponibles en acceso abierto, evidenciando un posible desconocimiento de estos recursos y calculamos un incremento importante respecto al uso de BECyT. Concluimos que las descargas desde Argentina tienen patrones similares al resto del mundo y que el aumento de su uso muestra que un cambio m\'as radical para garantizar el derecho de acceso a la informaci\'on es necesario. Mientras la informaci\'on cient\'ifica siga siendo una mercanc\'ia es primordial profundizar el estudio de los distintos proyectos que permiten reapropiarnos del conocimiento.},
  language = {es}
}

@article{moore_preregister_2016,
  title = {Preregister If You Want To},
  author = {Moore, Don A.},
  year = {2016},
  month = apr,
  journal = {The American Psychologist},
  volume = {71},
  number = {3},
  pages = {238--239},
  issn = {1935-990X},
  doi = {10.1037/a0040195},
  abstract = {Prespecification of confirmatory hypothesis tests is a useful tool that makes our statistical tests informative. On the other hand, selectively reporting studies, measures, or statistical tests renders the probability of false positives higher than the p values would imply. The bad news is that it is usually difficult to tell how much higher the probability is. Fortunately, there are enormous opportunities to improve the quality of our science by preregistering our research plans. Preregistration is a highly distinctive strength that should increase our faith in the veracity and replicability of a research result.},
  language = {eng},
  pmid = {27042885},
  keywords = {Clinical Trials as Topic,Humans,Information Dissemination,Reproducibility of Results,Research Design,Science}
}

@article{motta_dynamics_2018,
  title = {The {{Dynamics}} and {{Political Implications}} of {{Anti}}-{{Intellectualism}} in the {{United States}}},
  author = {Motta, Matthew},
  year = {2018},
  month = may,
  journal = {American Politics Research},
  volume = {46},
  number = {3},
  pages = {465--498},
  publisher = {{SAGE Publications Inc}},
  issn = {1532-673X},
  doi = {10.1177/1532673X17719507},
  abstract = {Recently, Americans have become increasingly likely to hold anti-intellectual attitudes (i.e., negative affect toward scientists and other experts). However, few have investigated the political implications of anti-intellectualism, and much empirical uncertainty surrounds whether or not these attitudes can be mitigated. Drawing on cross-sectional General Social Survey (GSS) data and a national election panel in 2016, I find that anti-intellectualism is associated with not only the rejection of policy-relevant matters of scientific consensus but support for political movements (e.g., ``Brexit'') and politicians (e.g., George Wallace, Donald Trump) who are skeptical of experts. Critically, though, I show that these effects can be mitigated. Verbal intelligence plays a strong role in mitigating anti-intellectual sympathies, compared with previously studied potential mitigators. I conclude by discussing how scholars might build on this research to study the political consequences of anti-intellectualism in the future.},
  language = {en},
  keywords = {anti-intellectualism,antiscience attitudes,political psychology,public opinion,verbal intelligence}
}

@article{motyl_state_2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  month = jul,
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (PsycINFO Database Record},
  language = {eng},
  pmid = {28447837},
  keywords = {Attitude of Health Personnel,Ethics; Research,Female,Humans,Male,Personality,Psychology,Psychology; Social,Research,Research Design,Surveys and Questionnaires}
}

@article{munafo_manifesto_2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  copyright = {2017 Macmillan Publishers Limited},
  language = {en},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Social sciences Subject\_term\_id: social-sciences}
}

@misc{nassi-calo_open_2013,
  title = {Open {{Access}} and a Call to Prevent the Looming Crisis in Science | {{SciELO}} in {{Perspective}}},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2013},
  month = jul,
  abstract = {The number of retracted articles has recently been on the rise. Bj\"orn Brembs identifies this tendency as a reflection of an imminent crisis in science whose},
  language = {en-US},
  keywords = {crisis}
}

@misc{nassi-calo_reproducibilidad_2014,
  title = {La Reproducibilidad En Los Resultados de Investigaci\'on: La Mirada Subjetiva | {{SciELO}} En {{Perspectiva}}},
  shorttitle = {La Reproducibilidad En Los Resultados de Investigaci\'on},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2014},
  month = feb,
  abstract = {En una \'epoca en que las discusiones sobre \'etica en la experimentaci\'on y la publicaci\'on cient\'ifica traspasan los laboratorios y ambientes acad\'emicos para},
  language = {en-US}
}

@article{naturehumanbehaviour_tell_2020,
  title = {Tell It like It Is},
  author = {{Nature human behaviour}},
  year = {2020},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {1},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0818-9},
  abstract = {Every research paper tells a story, but the pressure to provide `clean' narratives is harmful for the scientific endeavour.},
  copyright = {2020 Springer Nature Limited},
  language = {en},
  keywords = {forrt,transparency}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  language = {en},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration,reports,revisado}
}

@article{nosek_preregistration_2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and {van 't Veer}, Anna E. and Vazire, Simine},
  year = {2019},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.07.009},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  language = {en},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency}
}

@article{nosek_promoting_2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  journal = {Science},
  volume = {348},
  number = {6242},
  pages = {1422--1425},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  language = {en},
  pmid = {26113702},
  keywords = {revisado}
}

@article{nosek_registered_2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  month = may,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  language = {en},
  keywords = {forrt,reports}
}

@article{nosek_transparency_2014,
  title = {Transparency and {{Openness Promotion}} ({{TOP}}) {{Guidelines}}},
  author = {Nosek, Brian A. and Alter, George and Banks, George Christopher and Borsboom, Denny and Bowman, Sara and Breckler, Steven and Buck, Stuart and Chambers, Chris and Chin, Gilbert and Christensen, Garret},
  year = {2014},
  month = aug,
  publisher = {{OSF}},
  abstract = {The Transparency and Openness Promotion (TOP) Committee met in November 2014 to address one important element of the incentive systems - journals' procedures and  policies for publication. The outcome of the effort is the TOP Guidelines. There are eight standards in the TOP guidelines; each move scientific communication toward greater openness.  These standards are modular, facilitating adoption in whole or in part. However, they also complement each other, in that commitment to one standard may facilitate adoption of others. Moreover, the guidelines are sensitive to barriers to openness by articulating, for example, a process for exceptions to sharing because of ethical issues, intellectual property concerns, or availability of necessary resources.      Hosted on the Open Science Framework},
  language = {en},
  keywords = {(,transparency}
}

@misc{nw_trust_2019,
  title = {Trust and {{Mistrust}} in {{Americans}}' {{Views}} of {{Scientific Experts}}},
  author = {NW, 1615 L. St and Suite 800Washington and Inquiries, DC 20036USA202-419-4300 | Main202-857-8562 | Fax202-419-4372 | Media},
  year = {2019},
  month = aug,
  journal = {Pew Research Center Science \& Society},
  abstract = {Public confidence in scientists is on the upswing, and six-in-ten Americans say scientists should play an active role in policy debates about scientific issues, according to a new Pew Research Center survey.},
  language = {en-US}
}

@article{nyhan_increasing_2015,
  title = {Increasing the {{Credibility}} of {{Political Science Research}}: {{A Proposal}} for {{Journal Reforms}}},
  shorttitle = {Increasing the {{Credibility}} of {{Political Science Research}}},
  author = {Nyhan, Brendan},
  year = {2015},
  month = sep,
  journal = {Ps-Political Science \& Politics},
  volume = {48},
  pages = {78--83},
  publisher = {{Cambridge Univ Press}},
  address = {{New York}},
  issn = {1049-0965},
  doi = {10.1017/S1049096515000463},
  language = {English},
  keywords = {gender,medicaid,publication bias,registered-reports,replication,transparency},
  annotation = {WOS:000359291900014}
}

@article{oboyle_chrysalis_2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  journal = {Journal of Management},
  volume = {43},
  number = {2},
  pages = {376--399},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  language = {en},
  keywords = {ethics,morality and moral behavior,philosophy of science,revisado,statistical methods,transparency}
}

@misc{omatos_aspectos_2013,
  title = {Aspectos {{Legales}} En La {{Educaci\'on}}},
  author = {Omatos, Antonio},
  year = {2013}
}

@article{opensciencecollaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  language = {en}
}

@article{ospina_problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Ospina, Adriana Mar{\'i}a Restrepo},
  year = {2014},
  journal = {Estudios de Derecho},
  volume = {71},
  number = {158},
  pages = {69--96}
}

@article{patil_visual_2019,
  title = {A Visual Tool for Defining Reproducibility and Replicability},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  year = {2019},
  month = jul,
  journal = {Nature Human Behaviour},
  volume = {3},
  number = {7},
  pages = {650--652},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0629-z},
  abstract = {Reproducibility and replicability are fundamental requirements of scientific studies. Disagreements over universal definitions for these terms have affected the interpretation of large-scale replication attempts. We provide a visual tool for representing definitions and use it to re-examine these attempts.},
  copyright = {2019 Springer Nature Limited},
  language = {en},
  keywords = {herramienta},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Software;Statistics Subject\_term\_id: software;statistics}
}

@misc{pena_declaracion_2003,
  title = {{Declaraci\'on de Bethesda sobre Publicaci\'on de Acceso Abierto}},
  author = {Pe{\~n}a, Ismaes},
  year = {20 de Junio, 2003},
  language = {Traducido}
}

@article{penders_rinse_2019,
  title = {Rinse and {{Repeat}}: {{Understanding}} the {{Value}} of {{Replication}} across {{Different Ways}} of {{Knowing}}},
  shorttitle = {Rinse and {{Repeat}}},
  author = {Penders, Bart and Holbrook, J. Britt and {de Rijcke}, Sarah},
  year = {2019},
  month = sep,
  journal = {Publications},
  volume = {7},
  number = {3},
  pages = {52},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7030052},
  abstract = {The increasing pursuit of replicable research and actual replication of research is a political project that articulates a very specific technology of accountability for science. This project was initiated in response to concerns about the openness and trustworthiness of science. Though applicable and valuable in many fields, here we argue that this value cannot be extended everywhere, since the epistemic content of fields, as well as their accountability infrastructures, differ. Furthermore, we argue that there are limits to replicability across all fields; but in some fields, including parts of the humanities, these limits severely undermine the value of replication to account for the value of research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {accountability,epistemic pluralism,humanities,Replicability,replication,reproducibility,reproduction}
}

@article{peng_reproducibility_2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  journal = {Significance},
  volume = {12},
  number = {3},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  language = {en},
  keywords = {crisis},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x}
}

@article{peng_reproducible_2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, R. D.},
  year = {2011},
  month = dec,
  journal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  language = {en}
}

@article{petousi_contextualising_2020,
  title = {Contextualising Harm in the Framework of Research Misconduct. {{Findings}} from Discourse Analysis of Scientific Publications},
  author = {Petousi, Vasiliki and Sifaki, Eirini},
  year = {2020},
  month = jan,
  journal = {International Journal of Sustainable Development},
  volume = {23},
  number = {3-4},
  pages = {149--174},
  publisher = {{Inderscience Publishers}},
  issn = {0960-1406},
  doi = {10.1504/IJSD.2020.115206},
  abstract = {This article reports on research, which deals with dimensions of harm resulting from research misconduct, in articles published in scientific journals. An appropriate sample of publications retrieved from Pubmed, Scopus and WOS was selected across various disciplines and topics. Implementing discourse analysis, articles were classified according to the narratives of 'individual impurity', 'institutional failure' and 'structural crisis'. Most of the articles analysed fall within the narrative of structural crisis. The main argument advanced is that research misconduct harms the scientific enterprise as a whole. Harm is narrated in the context of institutional characteristics, policies, procedures, guidelines, and work environment. Mainly, however, harm is narrated in the context of structural characteristics of contemporary scientific practices, which result in normative dissonance for scientists and loss of trust in science in the relation between science and society and within the scientific enterprise itself. We conclude that new grounds for building trust and confidence in science are needed.},
  keywords = {practices}
}

@article{piwowar_future_2019,
  title = {The {{Future}} of {{OA}}: {{A}} Large-Scale Analysis Projecting {{Open Access}} Publication and Readership},
  shorttitle = {The {{Future}} of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Orr, Richard},
  year = {2019},
  journal = {BioRxiv},
  pages = {795310},
  publisher = {{Cold Spring Harbor Laboratory}}
}

@article{piwowar_state_2018,
  title = {The State of {{OA}}: A Large-Scale Analysis of the Prevalence and Impact of {{Open Access}} Articles},
  shorttitle = {The State of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Larivi{\`e}re, Vincent and Alperin, Juan Pablo and Matthias, Lisa and Norlander, Bree and Farley, Ashley and West, Jevin and Haustein, Stefanie},
  year = {2018},
  journal = {PeerJ},
  volume = {6},
  pages = {e4375},
  publisher = {{PeerJ Inc.}}
}

@incollection{poumadere_credibility_1991,
  title = {The {{Credibility Crisis}}},
  booktitle = {Chernobyl: {{A Policy Response Study}}},
  author = {Poumad{\`e}re, Marc},
  editor = {Segerst{\aa}hl, Boris},
  year = {1991},
  series = {Springer {{Series}} on {{Environmental Management}}},
  pages = {149--171},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-84367-9_8},
  abstract = {Those who said that the twentieth century is the century of the atom didn't know how right they were. Certainly, the scientific discovery of a particularly powerful new energy source had produced hopes and applications in both the civil and military sectors. In the final equation, though, it is the major accident at Chernobyl that dramatically and suddenly brought the reality of the atom's presence home to many persons and groups around the world.},
  isbn = {978-3-642-84367-9},
  language = {en},
  keywords = {Chernobyl Accident,crisis,Nuclear Energy,Nuclear Power Plant,Social Defense,Social Distance}
}

@article{price_problem_2020,
  title = {Problem with p Values: Why p Values Do Not Tell You If Your Treatment Is Likely to Work},
  shorttitle = {Problem with p Values},
  author = {Price, Robert and Bethune, Rob and Massey, Lisa},
  year = {2020},
  month = jan,
  journal = {Postgraduate Medical Journal},
  volume = {96},
  number = {1131},
  pages = {1--3},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/postgradmedj-2019-137079},
  language = {en},
  keywords = {forrt,practices}
}

@misc{rapp_what_2019,
  title = {What {{bioRxiv}}'s First 30,000 Preprints Reveal about Biologists},
  author = {Rapp, Joshua},
  year = {2019},
  journal = {nature},
  abstract = {More than 1 million studies are now downloaded from the site every month, mostly in neuroscience, bioinformatics and genomics.},
  language = {English}
}

@article{redish_opinion_2018,
  title = {Opinion: {{Reproducibility}} Failures Are Essential to Scientific Inquiry},
  shorttitle = {Opinion},
  author = {Redish, A. David and Kummerfeld, Erich and Morris, Rebecca Lea and Love, Alan C.},
  year = {2018},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {20},
  pages = {5042--5046}
}

@article{restrepo_problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Restrepo, Adriana Mar{\'i}a},
  year = {30 de Junio, 2014},
  volume = {71},
  number = {158},
  pages = {69--96}
}

@misc{rinke_probabilistic_2018,
  title = {Probabilistic {{Misconceptions Are Pervasive Among Communication Researchers}}},
  author = {Rinke, Eike Mark and Schneider, Frank M.},
  year = {2018},
  month = sep,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/h8zbe},
  abstract = {Across all areas of communication research, the most popular approach to generating insights about communication is the classical significance test (also called null hypothesis significance testing, NHST). The predominance of NHST in communication research is in spite of serious concerns about the ability of researchers to properly interpret its results. We draw on data from a survey of the ICA membership to assess the evidential basis of these concerns. The vast majority of communication researchers misinterpreted NHST (91\%) and the most prominent alternative, confidence intervals (96\%), while overestimating their competence. Academic seniority and statistical experience did not predict better interpretation outcomes. These findings indicate major problems regarding the generation of knowledge in the field of communication research.},
  keywords = {Communication,confidence intervals,data analysis,misconceptions,practices,significance testing,Social and Behavioral Sciences,statistical inference,statistics}
}

@article{rodriguez-sanchez_ciencia_2016,
  title = {{Ciencia reproducible: qu\'e, por qu\'e, c\'omo}},
  shorttitle = {{Ciencia reproducible}},
  author = {{Rodriguez-Sanchez}, Francisco and {P{\'e}rez-Luque}, Antonio Jes{\'u}s and Bartomeus, Ignasi and Varela, Sara},
  year = {2016},
  month = jul,
  journal = {Ecosistemas},
  volume = {25},
  number = {2},
  pages = {83--92},
  issn = {1697-2473},
  doi = {10.7818/ECOS.2016.25-2.11},
  copyright = {Derechos de autor},
  language = {es},
  keywords = {reproducibilidad,revisado}
}

@article{rosenthal_file_1979,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.86.3.638},
  language = {en}
}

@misc{rowe_preview_2018,
  title = {Preview My New Book: {{Introduction}} to {{Reproducible Science}} in {{R}} | {{R}}-Bloggers},
  shorttitle = {Preview My New Book},
  author = {Rowe, Brian Lee Yung},
  year = {2018},
  month = nov,
  abstract = {I'm pleased to share Part I of my new book ``Introduction to Reproducible Science in R``. The purpose of this \ldots Continue reading \textrightarrow},
  language = {en-US}
}

@article{rubin_evaluation_2017,
  title = {An {{Evaluation}} of {{Four Solutions}} to the {{Forking Paths Problem}}: {{Adjusted Alpha}}, {{Preregistration}}, {{Sensitivity Analyses}}, and {{Abandoning}} the {{Neyman}}-{{Pearson Approach}}},
  shorttitle = {An {{Evaluation}} of {{Four Solutions}} to the {{Forking Paths Problem}}},
  author = {Rubin, Mark},
  year = {2017},
  month = dec,
  journal = {Review of General Psychology},
  volume = {21},
  number = {4},
  pages = {321--329},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {1089-2680},
  doi = {10.1037/gpr0000135},
  abstract = {Gelman and Loken (2013, 2014) proposed that when researchers base their statistical analyses on the idiosyncratic characteristics of a specific sample (e.g., a nonlinear transformation of a variable because it is skewed), they open up alternative analysis paths in potential replications of their study that are based on different samples (i.e., no transformation of the variable because it is not skewed). These alternative analysis paths count as additional (multiple) tests and, consequently, they increase the probability of making a Type I error during hypothesis testing. The present article considers this forking paths problem and evaluates four potential solutions that might be used in psychology and other fields: (a) adjusting the prespecified alpha level, (b) preregistration, (c) sensitivity analyses, and (d) abandoning the Neyman-Pearson approach. It is concluded that although preregistration and sensitivity analyses are effective solutions to p-hacking, they are ineffective against result-neutral forking paths, such as those caused by transforming data. Conversely, although adjusting the alpha level cannot address p-hacking, it can be effective for result-neutral forking paths. Finally, abandoning the Neyman-Pearson approach represents a further solution to the forking paths problem.},
  language = {English},
  keywords = {confusion,familywise error,forking paths,null hypothesis significance testing,okeefes,p-values,preregistration,registered reports,replication crisis,sensitivity analyses,statistical-methods},
  annotation = {WOS:000417900400004}
}

@article{sadaba_acceso_2014,
  title = {{{EL ACCESO ABIERTO EN CIENCIAS SOCIALES}}: {{NOTAS SOCIOL\'OGICAS SOBRE PUBLICACIONES}}, {{COMUNIDADES Y CAMPOS}}},
  author = {Sadaba, Igor},
  year = {2014},
  volume = {17},
  pages = {93--113},
  issn = {1139-3327},
  abstract = {En el presente art\'iculo proponemos evitar las caracterizaciones abstractas y pol\'iticas del Open Access para pasar a evaluar emp\'iricamente su funcionamiento. Solo apart\'andonos de los manifiestos program\'aticos y los listados de beneficios te\'oricos de dichas pr\'acticas podremos valorar en su justa medida las resistencias existentes y aprovechar sus potencialidades reales. En concreto, se propone estudiar el Open Access en las Ciencias Sociales (en comparaci\'on con las Ciencias Naturales) y entender que todav\'ia estamos ante un proceso desigual de difusi\'on del conocimiento acad\'emico debido, en parte, a dos nociones sociol\'ogicas centrales (de dos autores tambi\'en centrales en las propias Ciencias Sociales): i) la arquitectura diferencial de sus ``comunidades cient\'ificas'' (Merton) y ii) las diferentes reglas de ``campo acad\'emico'' (Bourdieu) configuradas a partir del dominio de los \'indices de impacto en las ciencias contempor\'aneas.}
}

@article{schnell_reproducible_2018,
  title = {``{{Reproducible}}'' {{Research}} in {{Mathematical Sciences Requires Changes}} in Our {{Peer Review Culture}} and {{Modernization}} of Our {{Current Publication Approach}}},
  author = {Schnell, Santiago},
  year = {2018},
  month = dec,
  journal = {Bulletin of Mathematical Biology},
  volume = {80},
  number = {12},
  pages = {3095--3105},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0500-9},
  abstract = {The nature of scientific research in mathematical and computational biology allows editors and reviewers to evaluate the findings of a scientific paper. Replication of a research study should be the minimum standard for judging its scientific claims and considering it for publication. This requires changes in the current peer review practice and a strict adoption of a replication policy similar to those adopted in experimental fields such as organic synthesis. In the future, the culture of replication can be easily adopted by publishing papers through dynamic computational notebooks combining formatted text, equations, computer algebra and computer code.},
  language = {en}
}

@article{schoenbrodt_fostering_2018,
  title = {{Fostering Research Transparency as a Key Property of Science: Concrete Actions for Psychological Departments}},
  shorttitle = {{Fostering Research Transparency as a Key Property of Science}},
  author = {Schoenbrodt, Felix D. and Maier, Markus and Heene, Moritz and Buehner, Markus},
  year = {2018},
  month = jan,
  journal = {Psychologische Rundschau},
  volume = {69},
  number = {1},
  pages = {37--44},
  publisher = {{Hogrefe \& Huber Publishers}},
  address = {{Gottingen}},
  issn = {0033-3042},
  doi = {10.1026/0033-3042/a000386},
  abstract = {Recent large-scale replication projects suggest an amount of nonreplicable results in the scientific literature, in psychology but also in other sciences, which is concerning from our point of view. We analyze some causes for this situation, and argue that the change toward more research transparency ("open science") must be one consequence that should be drawn from the credibility crisis. We call for feasible changes in the local research units and departments and show as an example the steps that have been taken at the Department of Psychology of the Ludwig-Maximilians-Universitat Munchen. These changes concern incentive structures, research culture, teaching, and a close integration with the local ethics committee. The goal is to foster a more credible and more reproducible research output without generating unnecessary bureaucratic overhead.},
  language = {German},
  keywords = {credibility crisis,game,incentives,registered reports,replicability,replication crisis,research quality,rules,standards,truth},
  annotation = {WOS:000419334000004}
}

@misc{serevicionacionaldelpatrimoniocultural_tratados_,
  title = {Tratados {{Inernacionales}}},
  author = {{Serevicio Nacional del Patrimonio Cultural}},
  journal = {Departamento de Derechos Intelectuales}
}

@article{serra-garcia_nonreplicable_2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  journal = {Science Advances},
  volume = {7},
  number = {21},
  pages = {eabd1705},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  abstract = {Published papers that fail to replicate are cited more than those that replicate, even after the failure is published., We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility.},
  pmcid = {PMC8139580},
  pmid = {34020944}
}

@article{simmons_falsepositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  language = {en},
  keywords = {practices}
}

@article{simonsohn_pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534--547},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves\textemdash containing more low (.01s) than high (.04s) significant p values\textemdash only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {herramienta,Hypothesis Testing,practices,Psychology,Scientific Communication,Statistics}
}

@misc{socha_cuanto_2018,
  title = {{\textquestiondown Cu\'anto cobran los principales editores comerciales por tener un art\'iculo en acceso abierto?}},
  author = {Socha, Beata},
  year = {2018},
  journal = {Universo Abierto},
  abstract = {Los cuatro grandes actores de la industria editorial, Elsevier, Springer, Wiley y Taylor \& Francis, han adoptado el acceso abierto (Open Access, OA), a trav\'es de la modalidad ``El autor paga'' aunque en distintos grados. Tambi\'en han empleado estrategias muy diferentes en cuanto a cu\'anto cobran a sus autores. Para cualquier autor que desee publicar su investigaci\'on en Acceso Abierto en alguna de estas revistas probablemente necesita conocer lo que el mercado editorial tiene para ofrecer y qu\'e gama de precios existe. Los datos primarios proceden de las listas de precios oficiales de los editores disponibles en sus sitios web.},
  language = {Traducido}
}

@article{sociedadmaxplanck_declaracion_2003,
  title = {La {{Declaraci\'on}} de {{Berl\'in}} Sobre Acceso Abierto.},
  author = {Sociedad Max Planck},
  year = {2003},
  volume = {1},
  number = {2},
  pages = {152--154}
}

@techreport{soderberg_initial_2020,
  type = {Preprint},
  title = {Initial {{Evidence}} of {{Research Quality}} of {{Registered Reports Compared}} to the {{Traditional Publishing Model}}},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia G. and Singleton Thorn, Felix and Vazire, Simine and Esterling, Kevin and Nosek, Brian A.},
  year = {2020},
  month = nov,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/7x9vy},
  abstract = {In Registered Reports (RRs), initial peer review and in-principle acceptance occurs before knowing the research outcomes. This combats publication bias and distinguishes planned and unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs outperformed comparison papers on all 19 criteria (mean difference=0.46; Scale range -4 to +4) with effects ranging from little improvement in novelty (0.13, 95\% credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14, 0.58]) to larger improvements in rigor of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
  keywords = {reports,transparency}
}

@misc{spinak_revistas_2019,
  title = {{Revistas que han aumentado el valor del APC han recibido m\'as art\'iculos}},
  author = {Spinak, Ernesto},
  year = {2019},
  journal = {Scielo en Perspectiva},
  abstract = {El Acceso Abierto (AA) a las publicaciones cient\'ificas online surgi\'o hace dos d\'ecadas. Entre las expectativas de su amplia adopci\'on, se consideraba la superaci\'on de la crisis presupuestal que afrontaban las universidades y otras instituciones educativas y de investigaci\'on debido al aumento constante de los precios de suscripci\'on por encima de la inflaci\'on. Es en este contexto internacional que surge la Red SciELO hace ya 20 a\~nos, proyecto pionero en su momento y hoy d\'ia el m\'as importante en el \'ambito de los pa\'ises en v\'ias de desarrollo, con m\'as de 1.650 revistas y 812.000 art\'iculos publicados en texto completo al momento de escribir este post. SciELO adopt\'o el Acceso Abierto con el objetivo de maximizar la visibilidad de las revistas y de las investigaciones que comunican.},
  language = {es}
}

@article{stamkou_cultural_2019,
  title = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}: {{Effects}} on {{Power Perception}}, {{Moral Emotions}}, and {{Leader Support}}},
  shorttitle = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}},
  author = {Stamkou, Eftychia and {van Kleef}, Gerben A. and Homan, Astrid C. and Gelfand, Michele J. and {van de Vijver}, Fons J. R. and {van Egmond}, Marieke C. and Boer, Diana and Phiri, Natasha and Ayub, Nailah and Kinias, Zoe and Cantarero, Katarzyna and Efrat Treister, Dorit and Figueiredo, Ana and Hashimoto, Hirofumi and Hofmann, Eva B. and Lima, Renata P. and Lee, I-Ching},
  year = {2019},
  month = jun,
  journal = {Personality and Social Psychology Bulletin},
  volume = {45},
  number = {6},
  pages = {947--964},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-1672},
  doi = {10.1177/0146167218802832},
  abstract = {Responses to norm violators are poorly understood. On one hand, norm violators are perceived as powerful, which may help them to get ahead. On the other hand, norm violators evoke moral outrage, which may frustrate their upward social mobility. We addressed this paradox by considering the role of culture. Collectivistic cultures value group harmony and tight cultures value social order. We therefore hypothesized that collectivism and tightness moderate reactions to norm violators. We presented 2,369 participants in 19 countries with a norm violation or a norm adherence scenario. In individualistic cultures, norm violators were considered more powerful than norm abiders and evoked less moral outrage, whereas in collectivistic cultures, norm violators were considered less powerful and evoked more moral outrage. Moreover, respondents in tighter cultures expressed a stronger preference for norm followers as leaders. Cultural values thus influence responses to norm violators, which may have downstream consequences for violators' hierarchical positions.},
  language = {en},
  keywords = {collectivism,leadership,moral emotions,norm violation,tightness}
}

@article{steneck_fostering_2006,
  title = {Fostering Integrity in Research: {{Definitions}}, Current Knowledge, and Future Directions},
  shorttitle = {Fostering Integrity in Research},
  author = {Steneck, Nicholas H.},
  year = {2006},
  month = mar,
  journal = {Science and Engineering Ethics},
  volume = {12},
  number = {1},
  pages = {53--74},
  issn = {1471-5546},
  doi = {10.1007/PL00022268},
  abstract = {Over the last 25 years, a small but growing body of research on research behavior has slowly provided a more complete and critical understanding of research practices, particularly in the biomedical and behavioral sciences. The results of this research suggest that some earlier assumptions about irresponsible conduct are not reliable, leading to the conclusion that there is a need to change the way we think about and regulate research behavior. This paper begins with suggestions for more precise definitions of the terms ``responsible conduct of research,'' ``research ethics,'' and ``research integrity.'' It then summarizes the findings presented in some of the more important studies of research behavior, looking first at levels of occurrence and then impact. Based on this summary, the paper concludes with general observations about priorities and recommendations for steps to improve the effectiveness of efforts to respond to misconduct and foster higher standards for integrity in research.},
  language = {en},
  keywords = {practices}
}

@misc{stewart_preregistration_2020,
  title = {Pre-Registration and {{Registered Reports}}: A {{Primer}} from {{UKRN}}},
  shorttitle = {Pre-Registration and {{Registered Reports}}},
  author = {Stewart, Suzanne and Rinke, Eike Mark and McGarrigle, Ronan and Lynott, Dermot and Lunny, Carole and Lautarescu, Alexandra and Galizzi, Matteo M. and Farran, Emily K. and Crook, Zander},
  year = {2020},
  month = oct,
  institution = {{OSF Preprints}},
  doi = {10.31219/osf.io/8v2n7},
  abstract = {Help reduce questionable research practices, and prevent selective reporting.},
  keywords = {Architecture,Arts and Humanities,Business,Education,Engineering,Law,Life Sciences,Medicine and Health Sciences,Physical Sciences and Mathematics,pre-analysis plan,pre-registration,preregistration,primer,primers,prospective registration,registered reports,registration,reproducibility,Social and Behavioral Sciences,UK Reproducibility Network,UKRN}
}

@article{stodden_trust_2011,
  title = {Trust {{Your Science}}? {{Open Your Data}} and {{Code}}},
  shorttitle = {Trust {{Your Science}}?},
  author = {Stodden, Victoria C.},
  year = {2011},
  volume = {409},
  pages = {21--22},
  doi = {10.7916/D8CJ8Q0P},
  abstract = {This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.},
  language = {en},
  keywords = {forrt}
}

@book{swan_directrices_2013,
  title = {Directrices Para Pol\'iticasde Desarrollo y Promoci\'on Del Acceso Abierto},
  author = {Swan, Alma},
  year = {2013},
  publisher = {{UNESCO}},
  isbn = {978-959-18-0928- 5}
}

@article{szollosi_preregistration_2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {2},
  pages = {94--95},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.11.009},
  language = {en},
  keywords = {forrt,reports}
}

@article{tackett_bringing_2020,
  title = {Bringing the ({{Pre}}){{Registration Revolution}} to {{Graduate Training}}},
  author = {Tackett, Jennifer L. and Brandes, Cassandra M. and Dworak, Elizabeth M. and Shields, Allison N.},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology-Psychologie Canadienne},
  volume = {61},
  number = {4},
  pages = {299--309},
  publisher = {{Canadian Psychological Assoc}},
  address = {{Ottawa}},
  issn = {0708-5591},
  doi = {10.1037/cap0000221},
  abstract = {Preregistration, which involves documentation of hypotheses, methods, and plans for data analysis prior to data collection or analysis, has been lauded as 1 potential solution to the replication crisis in psychological science. Yet, many researchers have been slow to adopt preregistration, and the next generation of researchers is offered little formalized instruction in creating comprehensive preregistrations. In this article, we describe a collaborative workshop-based preregistration course designed and taught by Jennifer L. Tackett. We provide a brief overview of preregistration, including resources available, common concerns with preregistration, and responses to these concerns. We then describe the goals, structure, and evolution of our preregistration course and provide examples of enrolled students' final research products. We conclude with reflections on the strengths and opportunities for growth for the 1st iteration of this course and suggestions for others who are interested in implementing similar open science-focused courses in their training programs.},
  language = {English},
  keywords = {error,guide,incentives,open science,personality,preregistration,psychopathology,publication,registered-reports,registration,science,workshop-based learning},
  annotation = {WOS:000592853200005}
}

@article{taylor_altmetric_2020,
  title = {An Altmetric Attention Advantage for Open Access Books in the Humanities and Social Sciences},
  author = {Taylor, Michael},
  year = {2020},
  journal = {Scientometrics},
  volume = {125},
  pages = {2523--2543},
  doi = {10.1007/s11192-020-03735-8},
  abstract = {The last decade has seen two significant phenomena emerge in research communication: the rise of open access (OA) publishing, and the easy availability of evidence of online sharing in the form of altmetrics. There has been limited examination of the effect of OA on online sharing for journal articles, and little for books. This paper examines the altmetrics of a set of 32,222 books (of which 5\% are OA) and a set of 220,527 chapters (of which 7\% are OA) indexed by the scholarly database Dimensions in the Social Sciences and Humanities. Both OA books and chapters have significantly higher use on social networks, higher coverage in the mass media and blogs, and evidence of higher rates of social impact in policy documents. OA chapters have higher rates of coverage on Wikipedia than their non-OA equivalents, and are more likely to be shared on Mendeley. Even within the Humanities and Social Sciences, disciplinary differences in altmetric activity are evident. The effect is confirmed for chapters, although sampling issues prevent the strong conclusion that OA facilitates extra attention at the whole book level, the apparent OA altmetrics advantage suggests that the move towards OA is increasing social sharing and broader impact.},
  language = {English}
}

@article{tennant_ten_2019,
  title = {Ten {{Hot Topics}} around {{Scholarly Publishing}}},
  author = {Tennant, Jonathan P. and Crane, Harry and Crick, Tom and Davila, Jacinto and Enkhbayar, Asura and Havemann, Johanna and Kramer, Bianca and Martin, Ryan and Masuzzo, Paola and Nobes, Andy and Rice, Curt and {Rivera-L{\'o}pez}, B{\'a}rbara and {Ross-Hellauer}, Tony and Sattler, Susanne and Thacker, Paul D. and Vanholsbeeck, Marc},
  year = {2019},
  month = jun,
  journal = {Publications},
  volume = {7},
  number = {2},
  pages = {34},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7020034},
  abstract = {The changing world of scholarly communication and the emerging new wave of \&lsquo;Open Science\&rsquo; or \&lsquo;Open Research\&rsquo; has brought to light a number of controversial and hotly debated topics. Evidence-based rational debate is regularly drowned out by misinformed or exaggerated rhetoric, which does not benefit the evolving system of scholarly communication. This article aims to provide a baseline evidence framework for ten of the most contested topics, in order to help frame and move forward discussions, practices, and policies. We address issues around preprints and scooping, the practice of copyright transfer, the function of peer review, predatory publishers, and the legitimacy of \&lsquo;global\&rsquo; databases. These arguments and data will be a powerful tool against misinformation across wider academic research, policy and practice, and will inform changes within the rapidly evolving scholarly publishing system.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {copyright,impact factor,open access,open science,peer review,research evaluation,scholarly communication,Scopus,web of science}
}

@article{thibodeaux_production_2016,
  title = {Production as Social Change: {{Policy}} Sociology as a Public Good},
  shorttitle = {Production as Social Change},
  author = {Thibodeaux, Jarrett},
  year = {2016},
  month = may,
  journal = {Sociological Spectrum},
  volume = {36},
  number = {3},
  pages = {183--190},
  publisher = {{Routledge}},
  issn = {0273-2173},
  doi = {10.1080/02732173.2015.1102666},
  abstract = {Burawoy described two ways sociology can aid the public, through: (1) instrumental (policy) sociology and (2) reflexive (public) sociology. This article elaborates the different assumptions of how social change occurs according to policy and public sociology (and how sociology effects social change). Policy sociology assumes social change occurs through the scientific elaboration of the best means to achieve goals. However, policy sociology largely takes the public as an object of power rather than subjects who can utilize scientific knowledge. Public sociology assumes that social change occurs through the exposure of contradictions in goals, which elaborates better goals. However, the elaboration of contradictions assumes that there is a fundamental thesis/antithesis in society. If there are multiple goals/theses, public sociology fails in at least three ways. Policy sociology, when reflexively selecting its public, provides the best way sociology can aid the public.},
  annotation = {\_eprint: https://doi.org/10.1080/02732173.2015.1102666}
}

@article{tijdink_publication_2014,
  title = {Publication {{Pressure}} and {{Scientific Misconduct}} in {{Medical Scientists}}},
  author = {Tijdink, Joeri K. and Verbeke, Reinout and Smulders, Yvo M.},
  year = {2014},
  month = dec,
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {9},
  number = {5},
  pages = {64--71},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {1556-2646},
  doi = {10.1177/1556264614552421},
  abstract = {There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15\% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72\% rated publication pressure as too high. Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score.},
  language = {English},
  keywords = {ethics,ethics in publishing,fraud,impact,integrity,publication pressure,questionable   research practice,science,scientific misconduct},
  annotation = {WOS:000344691700008}
}

@article{ulloa_tendencias_2017,
  title = {Tendencias Paradigm\'aticas y T\'ecnicas Conversacionales En Investigaci\'on {{Cualitativa}} En {{Ciencias Sociales}}},
  author = {Ulloa, Jorge and Mardones, Rodolfo},
  year = {2017},
  journal = {Perspectivas de la Comunicaci\'on},
  series = {Universidad de La {{Frontera}}},
  volume = {10, n\textdegree 1},
  pages = {213--235},
  issn = {0718-4867},
  abstract = {El art\'iculo presenta los resultados de una investigaci\'on emp\'irica que busca describir el quehacer cient\'ifico cualitativo de alto impacto a trav\'es del an\'alisis de tres revistas indexadas en la base de datos Scopus entre los a\~nos 2013-2015. Para ello se construy\'o un corpus de 186 publicaciones reportando propuestas de investigaci\'on y/o resultados emp\'iricos de investigaci\'on. Se puso \'enfasis en el an\'alisis de la posici\'on paradigm\'atica de los autores, la explicitaci\'on y justificaci\'on de los objetivos de estudio y la utilizaci\'on de t\'ecnicas cualitativas conversacionales en su amplio rango, como las entrevistas en profundidad, historias de vida, grupos focales y grupos de discusi\'on. Los resultados dan cuenta que la mayor\'ia de los art\'iculos revisados no declaran abiertamente el paradigma desde el cual se posicionan, adem\'as se observa una multiplicidad de t\'ecnicas conversacionales, en las cuales destaca el uso de entrevistas. Se concluye que la adscripci\'on a alg\'un paradigma no es un asunto determinante a la hora de informar resultados, por cuanto estos forman parte de estudios m\'as amplios o, por su estructura son m\'as pragm\'aticos. Adem\'as, entre quienes declaran paradigmas de investigaci\'on, predomina una posici\'on constructivista. Por otro lado, el uso de t\'ecnicas conversacionales var\'ia en funci\'on de los objetivos de cada investigaci\'on, present\'andose en un amplio espectro, predomina el uso de entrevistas, y los grupos de discusi\'on y grupos focales son usados indistintamente.}
}

@inproceedings{unesco_declaracion_1999,
  title = {Declaraci\'on Sobre La Ciencia y El Uso Del Saber Cinet\'ifico},
  booktitle = {Conferencia Mundial Sobre La Ciencia},
  author = {UNESCO},
  year = {1999},
  address = {{Hungry - Budapest}}
}

@misc{universoabierto_rutas_2019,
  title = {Las 5 Rutas Para Llegar al Acceso Abierto: Verde, Dorada, Bronce, H\'ibrida y Diamante},
  author = {Universo Abierto},
  year = {2019},
  journal = {Blog de la biblioteca de Traducci\'on y Documentaci\'on de la Universidad de Salamanca}
}

@article{vanderzee_open_2018,
  title = {Open {{Education Science}}},
  author = {{van der Zee}, Tim and Reich, Justin},
  year = {2018},
  month = jul,
  journal = {Aera Open},
  volume = {4},
  number = {3},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  doi = {10.1177/2332858418787466},
  abstract = {Scientific progress is built on research that is reliable, accurate, and verifiable. The methods and evidentiary reasoning that underlie scientific claims must be available for scrutiny. Like other fields, the education sciences suffer from problems such as failure to replicate, validity and generalization issues, publication bias, and high costs of access to publications-all of which are symptoms of a nontransparent approach to research. Each aspect of the scientific cycle-research design, data collection, analysis, and publication-can and should be made more transparent and accessible. Open Education Science is a set of practices designed to increase the transparency of evidentiary reasoning and access to scientific research in a domain characterized by diverse disciplinary traditions and a commitment to impact in policy and practice. Transparency and accessibility are functional imperatives that come with many benefits for the individual researcher, scientific community, and society at large Open Education Science is the way forward.},
  language = {English},
  keywords = {articles,availability,bias,journals,open access,open science,preregistration,qualitative research,registered report,registered-reports,replication},
  annotation = {WOS:000509663300004}
}

@article{vantveer_preregistration_2016,
  title = {Pre-Registration in Social Psychology\textemdash{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied\textemdash reviewed and unreviewed pre-registration\textemdash and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  language = {en},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)}
}

@misc{velterop_suscripciones_2018,
  type = {{Scientific Blog}},
  title = {{De suscripciones y Tasas de Procesamiento de Art\'iculos}},
  author = {Velterop, Jan},
  year = {2018},
  journal = {Scielo en Perspectiva},
  abstract = {Estoy viendo m\'as y m\'as cr\'iticas a las Tasas de Procesamiento de Art\'iculos (Article Processing Charges \textendash{} APCs). Mientras que la mayor\'ia de ellas se preocupan por el monto de las tasas, tambi\'en hay algo de cr\'iticas al concepto mismo de las APC. Naturalmente, las tasas son a menudo altas. Demasiado altas, en particular en el caso de las revistas llamadas ``h\'ibridas''. M\'as sobre esto a continuaci\'on.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es}
}

@article{warren_how_2019,
  title = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}? {{Or}} to {{Get Tenure}}? {{Trends}} over a {{Generation}}},
  shorttitle = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}?},
  author = {Warren, John Robert},
  year = {2019},
  month = feb,
  journal = {Sociological Science},
  volume = {6},
  pages = {172--196},
  issn = {2330-6696},
  doi = {10.15195/v6.a7},
  abstract = {Many sociologists suspect that publication expectations have risen over time\textemdash that how much graduate students have published to get assistant professor jobs and how much assistant professors have published to be promoted have gone up. Using information about faculty in 21 top sociology departments from the American Sociological Association's Guide to Graduate Departments of Sociology, online curricula vitae, and other public records, I provide empirical evidence to support this suspicion. On the day they start their first jobs, new assistant professors in recent years have already published roughly twice as much as their counterparts did in the early 1990s. Trends for promotion to associate professor are not as dramatic but are still remarkable. I evaluate several potential explanations for these trends and conclude that they are driven mainly by changes over time in the fiscal and organizational realities of universities and departments.},
  language = {en-US}
}

@article{weston_recommendations_2019,
  title = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  year = {2019},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {3},
  pages = {214--227},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919848684},
  abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more true than now, when technological advances enable both sharing data across labs and continents and mining large sources of preexisting data. However, secondary data analysis is easily overlooked as a key domain for developing new open-science practices or improving analytic methods for robust data analysis. In this article, we provide researchers with the knowledge necessary to incorporate secondary data analysis into their methodological toolbox. We explain that secondary data analysis can be used for either exploratory or confirmatory work, and can be either correlational or experimental, and we highlight the advantages and disadvantages of this type of research. We describe how transparency-enhancing practices can improve and alter interpretations of results from secondary data analysis and discuss approaches that can be used to improve the robustness of reported results. We close by suggesting ways in which scientific subfields and institutions could address and improve the use of secondary data analysis.},
  language = {en},
  keywords = {bias,file drawer,p-hacking,panel design,preexisting data,preregistration,reproducibility,secondary analysis,transparency}
}

@article{wilson_replication_1973,
  title = {The {{Replication Problem}} in {{Sociology}}: {{A Report}} and a {{Suggestion}}*},
  shorttitle = {The {{Replication Problem}} in {{Sociology}}},
  author = {Wilson, Franklin D. and Smoke, Gale L. and Martin, J. David},
  year = {1973},
  journal = {Sociological Inquiry},
  volume = {43},
  number = {2},
  pages = {141--149},
  issn = {1475-682X},
  doi = {10.1111/j.1475-682X.1973.tb00711.x},
  abstract = {The deleterious effects of joint bias in favor of statistical inference and against replication are becoming well known. The acceptance of numerous Type I errors into the literature is by far the most serious of these. Data on the contents of three major journals support the contention that a joint bias for statistical significance tests, for rejections, and against replication exists in modern sociology. This finding replicates that of Sterling (1959) for psychology. A speculative analysis of the dynamics of publication decisions suggests that a compact format for reporting replications might make their publication more attractive to editors, and thus increase their frequency in the literature. A possible format for briefly reporting replication studies is suggested.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682X.1973.tb00711.x}
}

@article{wingen_no_2020,
  title = {No {{Replication}}, {{No Trust}}? {{How Low Replicability Influences Trust}} in {{Psychology}}},
  shorttitle = {No {{Replication}}, {{No Trust}}?},
  author = {Wingen, Tobias and Berkessel, Jana B. and Englich, Birte},
  year = {2020},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {11},
  number = {4},
  pages = {454--463},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550619877412},
  abstract = {In the current psychological debate, low replicability of psychological findings is a central topic. While the discussion about the replication crisis has a huge impact on psychological research, we know less about how it impacts public trust in psychology. In this article, we examine whether low replicability damages public trust and how this damage can be repaired. Studies 1\textendash 3 provide correlational and experimental evidence that low replicability reduces public trust in psychology. Additionally, Studies 3\textendash 5 evaluate the effectiveness of commonly used trust-repair strategies such as information about increased transparency (Study 3), explanations for low replicability (Study 4), or recovered replicability (Study 5). We found no evidence that these strategies significantly repair trust. However, it remains possible that they have small but potentially meaningful effects, which could be detected with larger samples. Overall, our studies highlight the importance of replicability for public trust in psychology.},
  language = {en},
  keywords = {crisis,open science,public trust,replicability,replication crisis}
}

@misc{wipo_frequently_2020,
  type = {Science {{Organization}}},
  title = {Frequently {{Asked Questions}}: {{IP Policies}} for {{Universities}} and {{Research Institutions}}},
  shorttitle = {{{FAQ}}},
  author = {WIPO},
  year = {2020},
  journal = {World Intelectual Property Organization},
  abstract = {WIPO is the global forum for intellectual property (IP) services, policy, information and cooperation. We are a self-funding agency of the United Nations, with 193 member states. Our mission is to lead the development of a balanced and effective international IP system that enables innovation and creativity for the benefit of all. Our mandate, governing bodies and procedures are set out in the WIPO Convention, which established WIPO in 1967},
  copyright = {Creative Commons 3.0},
  language = {English}
}

@book{worldintelectualpropietyorganization_what_2020,
  title = {What Is Intellectual Property?},
  shorttitle = {Whats Is {{IP}}?},
  author = {World Intelectual Propiety Organization, WIPO},
  year = {2020},
  edition = {WIPO},
  address = {{Switzerland}},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  isbn = {978-92-805-3176-3},
  language = {English}
}

@article{zenk-moltgen_factors_2018,
  title = {Factors Influencing the Data Sharing Behavior of Researchers in Sociology and Political Science},
  author = {{Zenk-M{\"o}ltgen}, Wolfgang and Akdeniz, Esra and Katsanidou, Alexia and Na{\ss}hoven, Verena and Balaban, Ebru},
  year = {2018},
  month = jan,
  journal = {Journal of Documentation},
  volume = {74},
  number = {5},
  pages = {1053--1073},
  publisher = {{Emerald Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/JD-09-2017-0126},
  abstract = {Purpose Open data and data sharing should improve transparency of research. The purpose of this paper is to investigate how different institutional and individual factors affect the data sharing behavior of authors of research articles in sociology and political science. Design/methodology/approach Desktop research analyzed attributes of sociology and political science journals (n=262) from their websites. A second data set of articles (n=1,011; published 2012-2014) was derived from ten of the main journals (five from each discipline) and stated data sharing was examined. A survey of the authors used the Theory of Planned Behavior to examine motivations, behavioral control, and perceived norms for sharing data. Statistical tests (Spearman's {$\rho$}, {$\chi$}2) examined correlations and associations. Findings Although many journals have a data policy for their authors (78 percent in sociology, 44 percent in political science), only around half of the empirical articles stated that the data were available, and for only 37 percent of the articles could the data be accessed. Journals with higher impact factors, those with a stated data policy, and younger journals were more likely to offer data availability. Of the authors surveyed, 446 responded (44 percent). Statistical analysis indicated that authors' attitudes, reported past behavior, social norms, and perceived behavioral control affected their intentions to share data. Research limitations/implications Less than 50 percent of the authors contacted provided responses to the survey. Results indicate that data sharing would improve if journals had explicit data sharing policies but authors also need support from other institutions (their universities, funding councils, and professional associations) to improve data management skills and infrastructures. Originality/value This paper builds on previous similar research in sociology and political science and explains some of the barriers to data sharing in social sciences by combining journal policies, published articles, and authors' responses to a survey.},
  keywords = {Data availability,Data policy,Data sharing,Political science,practices,Replication,Research data management,Research transparency,Sociology,Theory of Planned Behaviour}
}


